{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.layers import Input, Dense, BatchNormalization\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "import h5py\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from jarvis.db.figshare import data\n",
    "import csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(21263, 170)\n"
     ]
    }
   ],
   "source": [
    "df1 = pd.read_csv('data/train.csv')\n",
    "other = pd.read_csv('data/unique_m.csv')\n",
    "df = pd.concat([df1,other],axis=1)\n",
    "original_columns = len(df.columns)\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.T.drop_duplicates().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop('material', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number_of_elements\n",
      "mean_atomic_mass\n",
      "wtd_mean_atomic_mass\n",
      "gmean_atomic_mass\n",
      "wtd_gmean_atomic_mass\n",
      "entropy_atomic_mass\n",
      "wtd_entropy_atomic_mass\n",
      "range_atomic_mass\n",
      "wtd_range_atomic_mass\n",
      "std_atomic_mass\n",
      "wtd_std_atomic_mass\n",
      "mean_fie\n",
      "wtd_mean_fie\n",
      "gmean_fie\n",
      "wtd_gmean_fie\n",
      "entropy_fie\n",
      "wtd_entropy_fie\n",
      "range_fie\n",
      "wtd_range_fie\n",
      "std_fie\n",
      "wtd_std_fie\n",
      "mean_atomic_radius\n",
      "wtd_mean_atomic_radius\n",
      "gmean_atomic_radius\n",
      "wtd_gmean_atomic_radius\n",
      "entropy_atomic_radius\n",
      "wtd_entropy_atomic_radius\n",
      "range_atomic_radius\n",
      "wtd_range_atomic_radius\n",
      "std_atomic_radius\n",
      "wtd_std_atomic_radius\n",
      "mean_Density\n",
      "wtd_mean_Density\n",
      "gmean_Density\n",
      "wtd_gmean_Density\n",
      "entropy_Density\n",
      "wtd_entropy_Density\n",
      "range_Density\n",
      "wtd_range_Density\n",
      "std_Density\n",
      "wtd_std_Density\n",
      "mean_ElectronAffinity\n",
      "wtd_mean_ElectronAffinity\n",
      "gmean_ElectronAffinity\n",
      "wtd_gmean_ElectronAffinity\n",
      "entropy_ElectronAffinity\n",
      "wtd_entropy_ElectronAffinity\n",
      "range_ElectronAffinity\n",
      "wtd_range_ElectronAffinity\n",
      "std_ElectronAffinity\n",
      "wtd_std_ElectronAffinity\n",
      "mean_FusionHeat\n",
      "wtd_mean_FusionHeat\n",
      "gmean_FusionHeat\n",
      "wtd_gmean_FusionHeat\n",
      "entropy_FusionHeat\n",
      "wtd_entropy_FusionHeat\n",
      "range_FusionHeat\n",
      "wtd_range_FusionHeat\n",
      "std_FusionHeat\n",
      "wtd_std_FusionHeat\n",
      "mean_ThermalConductivity\n",
      "wtd_mean_ThermalConductivity\n",
      "gmean_ThermalConductivity\n",
      "wtd_gmean_ThermalConductivity\n",
      "entropy_ThermalConductivity\n",
      "wtd_entropy_ThermalConductivity\n",
      "range_ThermalConductivity\n",
      "wtd_range_ThermalConductivity\n",
      "std_ThermalConductivity\n",
      "wtd_std_ThermalConductivity\n",
      "mean_Valence\n",
      "wtd_mean_Valence\n",
      "gmean_Valence\n",
      "wtd_gmean_Valence\n",
      "entropy_Valence\n",
      "wtd_entropy_Valence\n",
      "range_Valence\n",
      "wtd_range_Valence\n",
      "std_Valence\n",
      "wtd_std_Valence\n",
      "critical_temp\n",
      "H\n",
      "He\n",
      "Li\n",
      "Be\n",
      "B\n",
      "C\n",
      "N\n",
      "O\n",
      "F\n",
      "Na\n",
      "Mg\n",
      "Al\n",
      "Si\n",
      "P\n",
      "S\n",
      "Cl\n",
      "K\n",
      "Ca\n",
      "Sc\n",
      "Ti\n",
      "V\n",
      "Cr\n",
      "Mn\n",
      "Fe\n",
      "Co\n",
      "Ni\n",
      "Cu\n",
      "Zn\n",
      "Ga\n",
      "Ge\n",
      "As\n",
      "Se\n",
      "Br\n",
      "Rb\n",
      "Sr\n",
      "Y\n",
      "Zr\n",
      "Nb\n",
      "Mo\n",
      "Tc\n",
      "Ru\n",
      "Rh\n",
      "Pd\n",
      "Ag\n",
      "Cd\n",
      "In\n",
      "Sn\n",
      "Sb\n",
      "Te\n",
      "I\n",
      "Cs\n",
      "Ba\n",
      "La\n",
      "Ce\n",
      "Pr\n",
      "Nd\n",
      "Sm\n",
      "Eu\n",
      "Gd\n",
      "Tb\n",
      "Dy\n",
      "Ho\n",
      "Er\n",
      "Tm\n",
      "Yb\n",
      "Lu\n",
      "Hf\n",
      "Ta\n",
      "W\n",
      "Re\n",
      "Os\n",
      "Ir\n",
      "Pt\n",
      "Au\n",
      "Hg\n",
      "Tl\n",
      "Pb\n",
      "Bi\n"
     ]
    }
   ],
   "source": [
    "for x in df.columns:\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(df.columns[[0,1,2,3,4,5,6,7,8,9,10,11]], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = df[df.critical_temp >25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = df[df.critical_temp != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df.critical_temp >30 ] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.asarray(df['critical_temp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = np.exp(0.1515*y.astype('float32'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([79.36884   , 49.812916  , 16.23327   , ..., -0.2044816 ,\n",
       "       -0.23280978,  5.3988867 ], dtype=float32)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g-1.5543"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "plt.hist(g-1.5543, bins = 20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function matplotlib.pyplot.show(*, block=None)>"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df.drop('critical_temp', axis=1), df['critical_temp'], test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21263"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(g-1.543)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_4 (InputLayer)        [(None, 159)]             0         \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  (None, 159)              636       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " fc1_relu (Dense)            (None, 256)               40960     \n",
      "                                                                 \n",
      " fc2 (Dense)                 (None, 128)               32896     \n",
      "                                                                 \n",
      " fc3 (Dense)                 (None, 64)                8256      \n",
      "                                                                 \n",
      " fc4 (Dense)                 (None, 32)                2080      \n",
      "                                                                 \n",
      " relu (Dense)                (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 84,861\n",
      "Trainable params: 84,543\n",
      "Non-trainable params: 318\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "Inputs = Input(shape=(159))\n",
    "x = BatchNormalization()(Inputs)\n",
    "\n",
    "x = Dense(256, activation='relu', kernel_initializer='lecun_uniform', name='fc1_relu')(x)\n",
    "x = Dense(128, activation='relu', kernel_initializer='lecun_uniform', name = 'fc2')(x)\n",
    "\n",
    "x = Dense(64, activation='relu', kernel_initializer='lecun_uniform', name = 'fc3')(x)\n",
    "x = Dense(32, activation='relu', kernel_initializer='lecun_uniform', name='fc4')(x)\n",
    "predictions = Dense(1, activation='relu', kernel_initializer='lecun_uniform', name = 'relu')(x)\n",
    "model = Model(inputs=Inputs, outputs=predictions)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anrunw/miniconda3/envs/tensorflow/lib/python3.10/site-packages/keras/optimizers/optimizer_v2/adam.py:117: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "adam = Adam(lr=0.0001)\n",
    "model.compile(optimizer=adam, loss='MSE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([80.92314  , 51.367214 , 17.78757  , ...,  1.3498183,  1.3214902,\n",
       "        6.9531865], dtype=float32)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g-1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      " 1/13 [=>............................] - ETA: 3s - loss: 1835440181084160.0000"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-24 15:30:21.444867: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13/13 [==============================] - 1s 23ms/step - loss: 173234917908666122240.0000 - val_loss: 2168220958064640.0000\n",
      "Epoch 2/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173234935500852166656.0000 - val_loss: 2168220958064640.0000\n",
      "Epoch 3/1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-24 15:30:21.796645: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13/13 [==============================] - 0s 9ms/step - loss: 173234935500852166656.0000 - val_loss: 2168220958064640.0000\n",
      "Epoch 4/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173234917908666122240.0000 - val_loss: 2168220958064640.0000\n",
      "Epoch 5/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173234935500852166656.0000 - val_loss: 2168220958064640.0000\n",
      "Epoch 6/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173234935500852166656.0000 - val_loss: 2168220958064640.0000\n",
      "Epoch 7/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173234935500852166656.0000 - val_loss: 2168220958064640.0000\n",
      "Epoch 8/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173234935500852166656.0000 - val_loss: 2168220958064640.0000\n",
      "Epoch 9/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173234935500852166656.0000 - val_loss: 2168220958064640.0000\n",
      "Epoch 10/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173234935500852166656.0000 - val_loss: 2168220823846912.0000\n",
      "Epoch 11/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173234917908666122240.0000 - val_loss: 2168220823846912.0000\n",
      "Epoch 12/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173234953093038211072.0000 - val_loss: 2168220823846912.0000\n",
      "Epoch 13/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173234935500852166656.0000 - val_loss: 2168220823846912.0000\n",
      "Epoch 14/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173234935500852166656.0000 - val_loss: 2168220555411456.0000\n",
      "Epoch 15/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173234935500852166656.0000 - val_loss: 2168220555411456.0000\n",
      "Epoch 16/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173234935500852166656.0000 - val_loss: 2168220286976000.0000\n",
      "Epoch 17/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173234935500852166656.0000 - val_loss: 2168220286976000.0000\n",
      "Epoch 18/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 173234953093038211072.0000 - val_loss: 2168220286976000.0000\n",
      "Epoch 19/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173234917908666122240.0000 - val_loss: 2168220286976000.0000\n",
      "Epoch 20/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173234917908666122240.0000 - val_loss: 2168220018540544.0000\n",
      "Epoch 21/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 173234935500852166656.0000 - val_loss: 2168220018540544.0000\n",
      "Epoch 22/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 173234900316480077824.0000 - val_loss: 2168219615887360.0000\n",
      "Epoch 23/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 173234970685224255488.0000 - val_loss: 2168219213234176.0000\n",
      "Epoch 24/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 173234935500852166656.0000 - val_loss: 2168219079016448.0000\n",
      "Epoch 25/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173234935500852166656.0000 - val_loss: 2168218542145536.0000\n",
      "Epoch 26/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 173234935500852166656.0000 - val_loss: 2168218407927808.0000\n",
      "Epoch 27/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173234935500852166656.0000 - val_loss: 2168217871056896.0000\n",
      "Epoch 28/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173234935500852166656.0000 - val_loss: 2168217468403712.0000\n",
      "Epoch 29/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173234935500852166656.0000 - val_loss: 2168217199968256.0000\n",
      "Epoch 30/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173234917908666122240.0000 - val_loss: 2168216528879616.0000\n",
      "Epoch 31/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173234935500852166656.0000 - val_loss: 2168215857790976.0000\n",
      "Epoch 32/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173234917908666122240.0000 - val_loss: 2168215320920064.0000\n",
      "Epoch 33/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173234917908666122240.0000 - val_loss: 2168214247178240.0000\n",
      "Epoch 34/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173234935500852166656.0000 - val_loss: 2168213576089600.0000\n",
      "Epoch 35/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173234935500852166656.0000 - val_loss: 2168212770783232.0000\n",
      "Epoch 36/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173234917908666122240.0000 - val_loss: 2168211294388224.0000\n",
      "Epoch 37/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173234917908666122240.0000 - val_loss: 2168210354864128.0000\n",
      "Epoch 38/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173234917908666122240.0000 - val_loss: 2168209146904576.0000\n",
      "Epoch 39/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 173234935500852166656.0000 - val_loss: 2168207670509568.0000\n",
      "Epoch 40/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 173234935500852166656.0000 - val_loss: 2168206194114560.0000\n",
      "Epoch 41/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173234935500852166656.0000 - val_loss: 2168204851937280.0000\n",
      "Epoch 42/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173234917908666122240.0000 - val_loss: 2168203107106816.0000\n",
      "Epoch 43/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 173234935500852166656.0000 - val_loss: 2168201093840896.0000\n",
      "Epoch 44/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173234935500852166656.0000 - val_loss: 2168199080574976.0000\n",
      "Epoch 45/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173234953093038211072.0000 - val_loss: 2168196664655872.0000\n",
      "Epoch 46/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173234917908666122240.0000 - val_loss: 2168194517172224.0000\n",
      "Epoch 47/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173234935500852166656.0000 - val_loss: 2168192101253120.0000\n",
      "Epoch 48/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173234935500852166656.0000 - val_loss: 2168189685334016.0000\n",
      "Epoch 49/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173234935500852166656.0000 - val_loss: 2168186464108544.0000\n",
      "Epoch 50/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173234935500852166656.0000 - val_loss: 2168183108665344.0000\n",
      "Epoch 51/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 173234953093038211072.0000 - val_loss: 2168179753222144.0000\n",
      "Epoch 52/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173234935500852166656.0000 - val_loss: 2168176129343488.0000\n",
      "Epoch 53/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173234917908666122240.0000 - val_loss: 2168172102811648.0000\n",
      "Epoch 54/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173234935500852166656.0000 - val_loss: 2168168344715264.0000\n",
      "Epoch 55/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173234935500852166656.0000 - val_loss: 2168163647094784.0000\n",
      "Epoch 56/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173234935500852166656.0000 - val_loss: 2168158949474304.0000\n",
      "Epoch 57/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173234935500852166656.0000 - val_loss: 2168153983418368.0000\n",
      "Epoch 58/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173234935500852166656.0000 - val_loss: 2168149017362432.0000\n",
      "Epoch 59/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173234917908666122240.0000 - val_loss: 2168143917088768.0000\n",
      "Epoch 60/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173234935500852166656.0000 - val_loss: 2168138011508736.0000\n",
      "Epoch 61/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173234935500852166656.0000 - val_loss: 2168131569057792.0000\n",
      "Epoch 62/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 173234935500852166656.0000 - val_loss: 2168124992389120.0000\n",
      "Epoch 63/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13/13 [==============================] - 0s 8ms/step - loss: 173234935500852166656.0000 - val_loss: 2168118013067264.0000\n",
      "Epoch 64/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173234935500852166656.0000 - val_loss: 2168109960003584.0000\n",
      "Epoch 65/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173234935500852166656.0000 - val_loss: 2168102846464000.0000\n",
      "Epoch 66/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173234935500852166656.0000 - val_loss: 2168094524964864.0000\n",
      "Epoch 67/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173234917908666122240.0000 - val_loss: 2168085398159360.0000\n",
      "Epoch 68/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173234935500852166656.0000 - val_loss: 2168077076660224.0000\n",
      "Epoch 69/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173234935500852166656.0000 - val_loss: 2168066205024256.0000\n",
      "Epoch 70/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 173234953093038211072.0000 - val_loss: 2168055467606016.0000\n",
      "Epoch 71/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 173234935500852166656.0000 - val_loss: 2168045669711872.0000\n",
      "Epoch 72/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173234953093038211072.0000 - val_loss: 2168035066511360.0000\n",
      "Epoch 73/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173234935500852166656.0000 - val_loss: 2168022584262656.0000\n",
      "Epoch 74/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173234935500852166656.0000 - val_loss: 2168009028272128.0000\n",
      "Epoch 75/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173234935500852166656.0000 - val_loss: 2167996143370240.0000\n",
      "Epoch 76/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173234935500852166656.0000 - val_loss: 2167983124250624.0000\n",
      "Epoch 77/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173234935500852166656.0000 - val_loss: 2167970776219648.0000\n",
      "Epoch 78/1000\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 173234935500852166656.0000 - val_loss: 2167957354446848.0000\n",
      "Epoch 79/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173234953093038211072.0000 - val_loss: 2167941919408128.0000\n",
      "Epoch 80/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173234935500852166656.0000 - val_loss: 2167924202668032.0000\n",
      "Epoch 81/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173234935500852166656.0000 - val_loss: 2167909170282496.0000\n",
      "Epoch 82/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173234935500852166656.0000 - val_loss: 2167889037623296.0000\n",
      "Epoch 83/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173234935500852166656.0000 - val_loss: 2167871186665472.0000\n",
      "Epoch 84/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173234935500852166656.0000 - val_loss: 2167853469925376.0000\n",
      "Epoch 85/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173234935500852166656.0000 - val_loss: 2167833068830720.0000\n",
      "Epoch 86/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173234917908666122240.0000 - val_loss: 2167813070389248.0000\n",
      "Epoch 87/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173234935500852166656.0000 - val_loss: 2167790656028672.0000\n",
      "Epoch 88/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173234935500852166656.0000 - val_loss: 2167768241668096.0000\n",
      "Epoch 89/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173234935500852166656.0000 - val_loss: 2167745827307520.0000\n",
      "Epoch 90/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173234935500852166656.0000 - val_loss: 2167722339205120.0000\n",
      "Epoch 91/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173234935500852166656.0000 - val_loss: 2167697106272256.0000\n",
      "Epoch 92/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173234935500852166656.0000 - val_loss: 2167673215516672.0000\n",
      "Epoch 93/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173234917908666122240.0000 - val_loss: 2167646908841984.0000\n",
      "Epoch 94/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173234917908666122240.0000 - val_loss: 2167623689175040.0000\n",
      "Epoch 95/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173234935500852166656.0000 - val_loss: 2167591476920320.0000\n",
      "Epoch 96/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173234935500852166656.0000 - val_loss: 2167563425415168.0000\n",
      "Epoch 97/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173234935500852166656.0000 - val_loss: 2167531078942720.0000\n",
      "Epoch 98/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173234935500852166656.0000 - val_loss: 2167502356348928.0000\n",
      "Epoch 99/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173234917908666122240.0000 - val_loss: 2167465446473728.0000\n",
      "Epoch 100/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173234917908666122240.0000 - val_loss: 2167430952517632.0000\n",
      "Epoch 101/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173234935500852166656.0000 - val_loss: 2167404108972032.0000\n",
      "Epoch 102/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173234935500852166656.0000 - val_loss: 2167363038347264.0000\n",
      "Epoch 103/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173234935500852166656.0000 - val_loss: 2167328275955712.0000\n",
      "Epoch 104/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173234935500852166656.0000 - val_loss: 2167289084379136.0000\n",
      "Epoch 105/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173234935500852166656.0000 - val_loss: 2167246403141632.0000\n",
      "Epoch 106/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 173234935500852166656.0000 - val_loss: 2167204661428224.0000\n",
      "Epoch 107/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173234953093038211072.0000 - val_loss: 2167159564271616.0000\n",
      "Epoch 108/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173234935500852166656.0000 - val_loss: 2167120104259584.0000\n",
      "Epoch 109/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173234935500852166656.0000 - val_loss: 2167077959892992.0000\n",
      "Epoch 110/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173234935500852166656.0000 - val_loss: 2167026822938624.0000\n",
      "Epoch 111/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173234935500852166656.0000 - val_loss: 2166977967685632.0000\n",
      "Epoch 112/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173234935500852166656.0000 - val_loss: 2166939044544512.0000\n",
      "Epoch 113/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173234953093038211072.0000 - val_loss: 2166869519761408.0000\n",
      "Epoch 114/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173234935500852166656.0000 - val_loss: 2166814758928384.0000\n",
      "Epoch 115/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173234917908666122240.0000 - val_loss: 2166759729659904.0000\n",
      "Epoch 116/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173234935500852166656.0000 - val_loss: 2166707787399168.0000\n",
      "Epoch 117/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173234917908666122240.0000 - val_loss: 2166647657857024.0000\n",
      "Epoch 118/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173234935500852166656.0000 - val_loss: 2166599339474944.0000\n",
      "Epoch 119/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173234935500852166656.0000 - val_loss: 2166539344150528.0000\n",
      "Epoch 120/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173234935500852166656.0000 - val_loss: 2166480959438848.0000\n",
      "Epoch 121/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173234935500852166656.0000 - val_loss: 2166423380033536.0000\n",
      "Epoch 122/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173234953093038211072.0000 - val_loss: 2166353318379520.0000\n",
      "Epoch 123/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13/13 [==============================] - 0s 8ms/step - loss: 173234935500852166656.0000 - val_loss: 2166287551692800.0000\n",
      "Epoch 124/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173234953093038211072.0000 - val_loss: 2166236280520704.0000\n",
      "Epoch 125/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173234917908666122240.0000 - val_loss: 2166161118593024.0000\n",
      "Epoch 126/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173234935500852166656.0000 - val_loss: 2166087701495808.0000\n",
      "Epoch 127/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173234935500852166656.0000 - val_loss: 2166019921543168.0000\n",
      "Epoch 128/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173234900316480077824.0000 - val_loss: 2165944625397760.0000\n",
      "Epoch 129/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173234935500852166656.0000 - val_loss: 2165865839591424.0000\n",
      "Epoch 130/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173234935500852166656.0000 - val_loss: 2165793898889216.0000\n",
      "Epoch 131/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173234953093038211072.0000 - val_loss: 2165721152880640.0000\n",
      "Epoch 132/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173234935500852166656.0000 - val_loss: 2165638072107008.0000\n",
      "Epoch 133/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173234917908666122240.0000 - val_loss: 2165549085753344.0000\n",
      "Epoch 134/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173234935500852166656.0000 - val_loss: 2165481037365248.0000\n",
      "Epoch 135/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173234935500852166656.0000 - val_loss: 2165361717805056.0000\n",
      "Epoch 136/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173234917908666122240.0000 - val_loss: 2165279576555520.0000\n",
      "Epoch 137/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173234900316480077824.0000 - val_loss: 2165149251141632.0000\n",
      "Epoch 138/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173234900316480077824.0000 - val_loss: 2165056104038400.0000\n",
      "Epoch 139/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173234900316480077824.0000 - val_loss: 2164964030676992.0000\n",
      "Epoch 140/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173234900316480077824.0000 - val_loss: 2164869809831936.0000\n",
      "Epoch 141/1000\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 173234917908666122240.0000 - val_loss: 2164789279195136.0000\n",
      "Epoch 142/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173234900316480077824.0000 - val_loss: 2164682844536832.0000\n",
      "Epoch 143/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173234900316480077824.0000 - val_loss: 2164579899539456.0000\n",
      "Epoch 144/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173234900316480077824.0000 - val_loss: 2164494939717632.0000\n",
      "Epoch 145/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173234917908666122240.0000 - val_loss: 2164378841382912.0000\n",
      "Epoch 146/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 173234900316480077824.0000 - val_loss: 2164281265094656.0000\n",
      "Epoch 147/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173234900316480077824.0000 - val_loss: 2164170401251328.0000\n",
      "Epoch 148/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173234917908666122240.0000 - val_loss: 2164052826521600.0000\n",
      "Epoch 149/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173234900316480077824.0000 - val_loss: 2163938607235072.0000\n",
      "Epoch 150/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173234882724294033408.0000 - val_loss: 2163828280262656.0000\n",
      "Epoch 151/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173234917908666122240.0000 - val_loss: 2163707886960640.0000\n",
      "Epoch 152/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173234917908666122240.0000 - val_loss: 2163591251755008.0000\n",
      "Epoch 153/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 173234900316480077824.0000 - val_loss: 2163469113622528.0000\n",
      "Epoch 154/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 173234882724294033408.0000 - val_loss: 2163339996168192.0000\n",
      "Epoch 155/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173234900316480077824.0000 - val_loss: 2163218126471168.0000\n",
      "Epoch 156/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 173234900316480077824.0000 - val_loss: 2163092901330944.0000\n",
      "Epoch 157/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173234900316480077824.0000 - val_loss: 2162951570063360.0000\n",
      "Epoch 158/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173234882724294033408.0000 - val_loss: 2162825539616768.0000\n",
      "Epoch 159/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173234900316480077824.0000 - val_loss: 2162690919235584.0000\n",
      "Epoch 160/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173234882724294033408.0000 - val_loss: 2162552943411200.0000\n",
      "Epoch 161/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173234900316480077824.0000 - val_loss: 2162407048740864.0000\n",
      "Epoch 162/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173234917908666122240.0000 - val_loss: 2162268536045568.0000\n",
      "Epoch 163/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173234900316480077824.0000 - val_loss: 2162133244575744.0000\n",
      "Epoch 164/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 173234882724294033408.0000 - val_loss: 2161970170036224.0000\n",
      "Epoch 165/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173234917908666122240.0000 - val_loss: 2161808303456256.0000\n",
      "Epoch 166/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 173234900316480077824.0000 - val_loss: 2161672877768704.0000\n",
      "Epoch 167/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 173234900316480077824.0000 - val_loss: 2161509669011456.0000\n",
      "Epoch 168/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 173234900316480077824.0000 - val_loss: 2161341762633728.0000\n",
      "Epoch 169/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 173234900316480077824.0000 - val_loss: 2161198015447040.0000\n",
      "Epoch 170/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173234900316480077824.0000 - val_loss: 2161041517576192.0000\n",
      "Epoch 171/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 173234900316480077824.0000 - val_loss: 2160845828128768.0000\n",
      "Epoch 172/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173234900316480077824.0000 - val_loss: 2160679263928320.0000\n",
      "Epoch 173/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173234917908666122240.0000 - val_loss: 2160473105498112.0000\n",
      "Epoch 174/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173234900316480077824.0000 - val_loss: 2160303722725376.0000\n",
      "Epoch 175/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 173234900316480077824.0000 - val_loss: 2160115281035264.0000\n",
      "Epoch 176/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173234900316480077824.0000 - val_loss: 2159962943913984.0000\n",
      "Epoch 177/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173234900316480077824.0000 - val_loss: 2159744840105984.0000\n",
      "Epoch 178/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173234900316480077824.0000 - val_loss: 2159567941140480.0000\n",
      "Epoch 179/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173234865132107988992.0000 - val_loss: 2159376949313536.0000\n",
      "Epoch 180/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173234865132107988992.0000 - val_loss: 2159151731965952.0000\n",
      "Epoch 181/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173234900316480077824.0000 - val_loss: 2158963156058112.0000\n",
      "Epoch 182/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173234865132107988992.0000 - val_loss: 2158759279329280.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 183/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173234882724294033408.0000 - val_loss: 2158563053010944.0000\n",
      "Epoch 184/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173234882724294033408.0000 - val_loss: 2158349244170240.0000\n",
      "Epoch 185/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173234865132107988992.0000 - val_loss: 2158066178981888.0000\n",
      "Epoch 186/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173234865132107988992.0000 - val_loss: 2157786469236736.0000\n",
      "Epoch 187/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173234865132107988992.0000 - val_loss: 2157594806321152.0000\n",
      "Epoch 188/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173234882724294033408.0000 - val_loss: 2157348785225728.0000\n",
      "Epoch 189/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173234882724294033408.0000 - val_loss: 2157121017741312.0000\n",
      "Epoch 190/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173234865132107988992.0000 - val_loss: 2156873251815424.0000\n",
      "Epoch 191/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 173234865132107988992.0000 - val_loss: 2156634075824128.0000\n",
      "Epoch 192/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173234865132107988992.0000 - val_loss: 2156398926364672.0000\n",
      "Epoch 193/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173234847539921944576.0000 - val_loss: 2156176796024832.0000\n",
      "Epoch 194/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173234865132107988992.0000 - val_loss: 2155966611062784.0000\n",
      "Epoch 195/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173234865132107988992.0000 - val_loss: 2155733877522432.0000\n",
      "Epoch 196/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173234882724294033408.0000 - val_loss: 2155454838865920.0000\n",
      "Epoch 197/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173234882724294033408.0000 - val_loss: 2155195127562240.0000\n",
      "Epoch 198/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173234882724294033408.0000 - val_loss: 2154901727608832.0000\n",
      "Epoch 199/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173234865132107988992.0000 - val_loss: 2154665638625280.0000\n",
      "Epoch 200/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173234847539921944576.0000 - val_loss: 2154442434543616.0000\n",
      "Epoch 201/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173234847539921944576.0000 - val_loss: 2154131854721024.0000\n",
      "Epoch 202/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173234847539921944576.0000 - val_loss: 2153902476623872.0000\n",
      "Epoch 203/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173234829947735900160.0000 - val_loss: 2153578340810752.0000\n",
      "Epoch 204/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173234812355549855744.0000 - val_loss: 2153315542499328.0000\n",
      "Epoch 205/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173234847539921944576.0000 - val_loss: 2153019189755904.0000\n",
      "Epoch 206/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173234847539921944576.0000 - val_loss: 2152720286875648.0000\n",
      "Epoch 207/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173234829947735900160.0000 - val_loss: 2152427289575424.0000\n",
      "Epoch 208/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173234829947735900160.0000 - val_loss: 2152169457319936.0000\n",
      "Epoch 209/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173234812355549855744.0000 - val_loss: 2151883573559296.0000\n",
      "Epoch 210/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173234829947735900160.0000 - val_loss: 2151555679649792.0000\n",
      "Epoch 211/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173234847539921944576.0000 - val_loss: 2151291539161088.0000\n",
      "Epoch 212/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173234812355549855744.0000 - val_loss: 2150975993282560.0000\n",
      "Epoch 213/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173234829947735900160.0000 - val_loss: 2150641925357568.0000\n",
      "Epoch 214/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173234829947735900160.0000 - val_loss: 2150315776278528.0000\n",
      "Epoch 215/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173234847539921944576.0000 - val_loss: 2150022913196032.0000\n",
      "Epoch 216/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173234829947735900160.0000 - val_loss: 2149710588542976.0000\n",
      "Epoch 217/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173234812355549855744.0000 - val_loss: 2149365112111104.0000\n",
      "Epoch 218/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173234794763363811328.0000 - val_loss: 2149023393775616.0000\n",
      "Epoch 219/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173234829947735900160.0000 - val_loss: 2148776567373824.0000\n",
      "Epoch 220/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173234812355549855744.0000 - val_loss: 2148422366789632.0000\n",
      "Epoch 221/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173234794763363811328.0000 - val_loss: 2148075682398208.0000\n",
      "Epoch 222/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173234794763363811328.0000 - val_loss: 2147746043658240.0000\n",
      "Epoch 223/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173234777171177766912.0000 - val_loss: 2147289032294400.0000\n",
      "Epoch 224/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173234794763363811328.0000 - val_loss: 2146861548830720.0000\n",
      "Epoch 225/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173234777171177766912.0000 - val_loss: 2146530836348928.0000\n",
      "Epoch 226/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173234777171177766912.0000 - val_loss: 2146173817192448.0000\n",
      "Epoch 227/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173234777171177766912.0000 - val_loss: 2145832501510144.0000\n",
      "Epoch 228/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173234794763363811328.0000 - val_loss: 2145437767172096.0000\n",
      "Epoch 229/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173234777171177766912.0000 - val_loss: 2145086116724736.0000\n",
      "Epoch 230/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173234777171177766912.0000 - val_loss: 2144757149073408.0000\n",
      "Epoch 231/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173234777171177766912.0000 - val_loss: 2144416772915200.0000\n",
      "Epoch 232/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173234794763363811328.0000 - val_loss: 2144017609392128.0000\n",
      "Epoch 233/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173234777171177766912.0000 - val_loss: 2143639786487808.0000\n",
      "Epoch 234/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173234777171177766912.0000 - val_loss: 2143303839514624.0000\n",
      "Epoch 235/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173234741986805678080.0000 - val_loss: 2142892999049216.0000\n",
      "Epoch 236/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173234777171177766912.0000 - val_loss: 2142479205793792.0000\n",
      "Epoch 237/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173234741986805678080.0000 - val_loss: 2142154264674304.0000\n",
      "Epoch 238/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173234741986805678080.0000 - val_loss: 2141816841306112.0000\n",
      "Epoch 239/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173234759578991722496.0000 - val_loss: 2141439689490432.0000\n",
      "Epoch 240/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173234741986805678080.0000 - val_loss: 2140986436222976.0000\n",
      "Epoch 241/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173234777171177766912.0000 - val_loss: 2140580427595776.0000\n",
      "Epoch 242/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173234741986805678080.0000 - val_loss: 2140166365904896.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 243/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173234759578991722496.0000 - val_loss: 2139873100169216.0000\n",
      "Epoch 244/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173234724394619633664.0000 - val_loss: 2139505746247680.0000\n",
      "Epoch 245/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173234777171177766912.0000 - val_loss: 2139060948697088.0000\n",
      "Epoch 246/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173234777171177766912.0000 - val_loss: 2138594542092288.0000\n",
      "Epoch 247/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173234759578991722496.0000 - val_loss: 2138204773810176.0000\n",
      "Epoch 248/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 173234724394619633664.0000 - val_loss: 2137828158865408.0000\n",
      "Epoch 249/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 173234724394619633664.0000 - val_loss: 2137388998459392.0000\n",
      "Epoch 250/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173234741986805678080.0000 - val_loss: 2136914404573184.0000\n",
      "Epoch 251/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173234741986805678080.0000 - val_loss: 2136561680384000.0000\n",
      "Epoch 252/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173234724394619633664.0000 - val_loss: 2136125606985728.0000\n",
      "Epoch 253/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173234706802433589248.0000 - val_loss: 2135654636978176.0000\n",
      "Epoch 254/1000\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 173234706802433589248.0000 - val_loss: 2135249702092800.0000\n",
      "Epoch 255/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173234689210247544832.0000 - val_loss: 2134854699319296.0000\n",
      "Epoch 256/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173234724394619633664.0000 - val_loss: 2134430839734272.0000\n",
      "Epoch 257/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173234689210247544832.0000 - val_loss: 2133973425717248.0000\n",
      "Epoch 258/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 173234689210247544832.0000 - val_loss: 2133481786179584.0000\n",
      "Epoch 259/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173234689210247544832.0000 - val_loss: 2133072690544640.0000\n",
      "Epoch 260/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173234706802433589248.0000 - val_loss: 2132675674505216.0000\n",
      "Epoch 261/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173234689210247544832.0000 - val_loss: 2132136119238656.0000\n",
      "Epoch 262/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173234689210247544832.0000 - val_loss: 2131687832027136.0000\n",
      "Epoch 263/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173234654025875456000.0000 - val_loss: 2131282226053120.0000\n",
      "Epoch 264/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173234689210247544832.0000 - val_loss: 2130846555308032.0000\n",
      "Epoch 265/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 173234654025875456000.0000 - val_loss: 2130461887299584.0000\n",
      "Epoch 266/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 173234689210247544832.0000 - val_loss: 2129977495519232.0000\n",
      "Epoch 267/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 173234654025875456000.0000 - val_loss: 2129625710854144.0000\n",
      "Epoch 268/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173234654025875456000.0000 - val_loss: 2129116488794112.0000\n",
      "Epoch 269/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173234636433689411584.0000 - val_loss: 2128840939798528.0000\n",
      "Epoch 270/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173234618841503367168.0000 - val_loss: 2128150926458880.0000\n",
      "Epoch 271/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173234601249317322752.0000 - val_loss: 2127723442995200.0000\n",
      "Epoch 272/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173234601249317322752.0000 - val_loss: 2127276900614144.0000\n",
      "Epoch 273/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173234601249317322752.0000 - val_loss: 2126899748798464.0000\n",
      "Epoch 274/1000\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 173234601249317322752.0000 - val_loss: 2126505282895872.0000\n",
      "Epoch 275/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173234583657131278336.0000 - val_loss: 2126133902442496.0000\n",
      "Epoch 276/1000\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 173234530880573145088.0000 - val_loss: 2125794063155200.0000\n",
      "Epoch 277/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173234530880573145088.0000 - val_loss: 2125165118881792.0000\n",
      "Epoch 278/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173234495696201056256.0000 - val_loss: 2124443832811520.0000\n",
      "Epoch 279/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173234530880573145088.0000 - val_loss: 2123946287693824.0000\n",
      "Epoch 280/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173234460511828967424.0000 - val_loss: 2123549137436672.0000\n",
      "Epoch 281/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173234478104015011840.0000 - val_loss: 2123200976650240.0000\n",
      "Epoch 282/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173234495696201056256.0000 - val_loss: 2122855366000640.0000\n",
      "Epoch 283/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173234478104015011840.0000 - val_loss: 2122526129913856.0000\n",
      "Epoch 284/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173234460511828967424.0000 - val_loss: 2122226690162688.0000\n",
      "Epoch 285/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173234442919642923008.0000 - val_loss: 2121822426365952.0000\n",
      "Epoch 286/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173234442919642923008.0000 - val_loss: 2121443663937536.0000\n",
      "Epoch 287/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173234460511828967424.0000 - val_loss: 2121121004519424.0000\n",
      "Epoch 288/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173234372550898745344.0000 - val_loss: 2120928938950656.0000\n",
      "Epoch 289/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173234425327456878592.0000 - val_loss: 2120599971299328.0000\n",
      "Epoch 290/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173234442919642923008.0000 - val_loss: 2120131954081792.0000\n",
      "Epoch 291/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173234407735270834176.0000 - val_loss: 2119749567774720.0000\n",
      "Epoch 292/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173234372550898745344.0000 - val_loss: 2119368120991744.0000\n",
      "Epoch 293/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173234302182154567680.0000 - val_loss: 2119184645357568.0000\n",
      "Epoch 294/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173234319774340612096.0000 - val_loss: 2118704011673600.0000\n",
      "Epoch 295/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173234302182154567680.0000 - val_loss: 2118434770911232.0000\n",
      "Epoch 296/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173234337366526656512.0000 - val_loss: 2118073993658368.0000\n",
      "Epoch 297/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173234302182154567680.0000 - val_loss: 2117646375976960.0000\n",
      "Epoch 298/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173234354958712700928.0000 - val_loss: 2117361029087232.0000\n",
      "Epoch 299/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173234302182154567680.0000 - val_loss: 2117110981459968.0000\n",
      "Epoch 300/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173234266997782478848.0000 - val_loss: 2116705912356864.0000\n",
      "Epoch 301/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173234249405596434432.0000 - val_loss: 2116331847548928.0000\n",
      "Epoch 302/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173234214221224345600.0000 - val_loss: 2115981270843392.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 303/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173234249405596434432.0000 - val_loss: 2115644115910656.0000\n",
      "Epoch 304/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173234231813410390016.0000 - val_loss: 2115375948890112.0000\n",
      "Epoch 305/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173234214221224345600.0000 - val_loss: 2115095568056320.0000\n",
      "Epoch 306/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173234214221224345600.0000 - val_loss: 2114729556312064.0000\n",
      "Epoch 307/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173234179036852256768.0000 - val_loss: 2114405957369856.0000\n",
      "Epoch 308/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173234161444666212352.0000 - val_loss: 2114178726756352.0000\n",
      "Epoch 309/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173234214221224345600.0000 - val_loss: 2113847343185920.0000\n",
      "Epoch 310/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173234143852480167936.0000 - val_loss: 2113518375534592.0000\n",
      "Epoch 311/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173234161444666212352.0000 - val_loss: 2113316646289408.0000\n",
      "Epoch 312/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173234108668108079104.0000 - val_loss: 2112777225240576.0000\n",
      "Epoch 313/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173234108668108079104.0000 - val_loss: 2112364505726976.0000\n",
      "Epoch 314/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173234126260294123520.0000 - val_loss: 2112165595054080.0000\n",
      "Epoch 315/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173234091075922034688.0000 - val_loss: 2111781732352000.0000\n",
      "Epoch 316/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173234055891549945856.0000 - val_loss: 2111507122880512.0000\n",
      "Epoch 317/1000\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 173234073483735990272.0000 - val_loss: 2111327807995904.0000\n",
      "Epoch 318/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173234073483735990272.0000 - val_loss: 2110854824722432.0000\n",
      "Epoch 319/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173234003114991812608.0000 - val_loss: 2110496463388672.0000\n",
      "Epoch 320/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173233967930619723776.0000 - val_loss: 2110251784470528.0000\n",
      "Epoch 321/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173233932746247634944.0000 - val_loss: 2109993952215040.0000\n",
      "Epoch 322/1000\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 173233932746247634944.0000 - val_loss: 2109786048954368.0000\n",
      "Epoch 323/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173233967930619723776.0000 - val_loss: 2109425271701504.0000\n",
      "Epoch 324/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173233915154061590528.0000 - val_loss: 2109146501480448.0000\n",
      "Epoch 325/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173233932746247634944.0000 - val_loss: 2108994432794624.0000\n",
      "Epoch 326/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173233862377503457280.0000 - val_loss: 2108759820206080.0000\n",
      "Epoch 327/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173233862377503457280.0000 - val_loss: 2108499169378304.0000\n",
      "Epoch 328/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173233915154061590528.0000 - val_loss: 2108248719097856.0000\n",
      "Epoch 329/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173233827193131368448.0000 - val_loss: 2108046989852672.0000\n",
      "Epoch 330/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173233827193131368448.0000 - val_loss: 2107711982403584.0000\n",
      "Epoch 331/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173233844785317412864.0000 - val_loss: 2107438849327104.0000\n",
      "Epoch 332/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173233827193131368448.0000 - val_loss: 2107245844234240.0000\n",
      "Epoch 333/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173233774416573235200.0000 - val_loss: 2106970026803200.0000\n",
      "Epoch 334/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173233739232201146368.0000 - val_loss: 2106819702947840.0000\n",
      "Epoch 335/1000\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 173233739232201146368.0000 - val_loss: 2106567910490112.0000\n",
      "Epoch 336/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173233721640015101952.0000 - val_loss: 2106336921780224.0000\n",
      "Epoch 337/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173233739232201146368.0000 - val_loss: 2106107812118528.0000\n",
      "Epoch 338/1000\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 173233704047829057536.0000 - val_loss: 2105882057900032.0000\n",
      "Epoch 339/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173233704047829057536.0000 - val_loss: 2105642345037824.0000\n",
      "Epoch 340/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173233651271270924288.0000 - val_loss: 2105432965382144.0000\n",
      "Epoch 341/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173233651271270924288.0000 - val_loss: 2105222646202368.0000\n",
      "Epoch 342/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173233686455643013120.0000 - val_loss: 2105061853364224.0000\n",
      "Epoch 343/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173233616086898835456.0000 - val_loss: 2104910321549312.0000\n",
      "Epoch 344/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173233598494712791040.0000 - val_loss: 2104646583713792.0000\n",
      "Epoch 345/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 173233616086898835456.0000 - val_loss: 2104498944212992.0000\n",
      "Epoch 346/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 173233563310340702208.0000 - val_loss: 2104309160345600.0000\n",
      "Epoch 347/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 173233580902526746624.0000 - val_loss: 2104101391302656.0000\n",
      "Epoch 348/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 173233563310340702208.0000 - val_loss: 2103902480629760.0000\n",
      "Epoch 349/1000\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 173233475349410480128.0000 - val_loss: 2103799938285568.0000\n",
      "Epoch 350/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 173233457757224435712.0000 - val_loss: 2103478084173824.0000\n",
      "Epoch 351/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 173233422572852346880.0000 - val_loss: 2103255282745344.0000\n",
      "Epoch 352/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 173233457757224435712.0000 - val_loss: 2103128581210112.0000\n",
      "Epoch 353/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 173233369796294213632.0000 - val_loss: 2102940944826368.0000\n",
      "Epoch 354/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173233387388480258048.0000 - val_loss: 2102778004504576.0000\n",
      "Epoch 355/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173233404980666302464.0000 - val_loss: 2102657476984832.0000\n",
      "Epoch 356/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 173233299427550035968.0000 - val_loss: 2102537486336000.0000\n",
      "Epoch 357/1000\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 173233387388480258048.0000 - val_loss: 2102389309964288.0000\n",
      "Epoch 358/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 173233369796294213632.0000 - val_loss: 2102260192509952.0000\n",
      "Epoch 359/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 173233281835363991552.0000 - val_loss: 2102109734436864.0000\n",
      "Epoch 360/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173233334611922124800.0000 - val_loss: 2102008400052224.0000\n",
      "Epoch 361/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173233352204108169216.0000 - val_loss: 2101934446084096.0000\n",
      "Epoch 362/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173233317019736080384.0000 - val_loss: 2101779424608256.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 363/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 173233264243177947136.0000 - val_loss: 2101695270092800.0000\n",
      "Epoch 364/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173233229058805858304.0000 - val_loss: 2101574876790784.0000\n",
      "Epoch 365/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173233176282247725056.0000 - val_loss: 2101501325475840.0000\n",
      "Epoch 366/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 173233229058805858304.0000 - val_loss: 2101401601703936.0000\n",
      "Epoch 367/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 173233211466619813888.0000 - val_loss: 2101331808485376.0000\n",
      "Epoch 368/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 173233141097875636224.0000 - val_loss: 2101291140513792.0000\n",
      "Epoch 369/1000\n",
      "13/13 [==============================] - 0s 11ms/step - loss: 173233070729131458560.0000 - val_loss: 2101213428449280.0000\n",
      "Epoch 370/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 173233070729131458560.0000 - val_loss: 2101124442095616.0000\n",
      "Epoch 371/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 173233088321317502976.0000 - val_loss: 2101043374587904.0000\n",
      "Epoch 372/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173233088321317502976.0000 - val_loss: 2100972507627520.0000\n",
      "Epoch 373/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173233017952573325312.0000 - val_loss: 2100903251279872.0000\n",
      "Epoch 374/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173233088321317502976.0000 - val_loss: 2100851711672320.0000\n",
      "Epoch 375/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173233035544759369728.0000 - val_loss: 2100801514242048.0000\n",
      "Epoch 376/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173233053136945414144.0000 - val_loss: 2100775476002816.0000\n",
      "Epoch 377/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 173233017952573325312.0000 - val_loss: 2100750645723136.0000\n",
      "Epoch 378/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 173232894807271014400.0000 - val_loss: 2100723533742080.0000\n",
      "Epoch 379/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 173232982768201236480.0000 - val_loss: 2100698166591488.0000\n",
      "Epoch 380/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173232877215084969984.0000 - val_loss: 2100674544271360.0000\n",
      "Epoch 381/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173232842030712881152.0000 - val_loss: 2100673738964992.0000\n",
      "Epoch 382/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173232877215084969984.0000 - val_loss: 2100677899714560.0000\n",
      "Epoch 383/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173232842030712881152.0000 - val_loss: 2100676825972736.0000\n",
      "Epoch 384/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173232736477596614656.0000 - val_loss: 2100678033932288.0000\n",
      "Epoch 385/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173232859622898925568.0000 - val_loss: 2100690381963264.0000\n",
      "Epoch 386/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173232824438526836736.0000 - val_loss: 2100712393670656.0000\n",
      "Epoch 387/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173232736477596614656.0000 - val_loss: 2100745142796288.0000\n",
      "Epoch 388/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173232701293224525824.0000 - val_loss: 2100766080761856.0000\n",
      "Epoch 389/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 173232683701038481408.0000 - val_loss: 2100810372612096.0000\n",
      "Epoch 390/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173232595740108259328.0000 - val_loss: 2100860838477824.0000\n",
      "Epoch 391/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173232542963550126080.0000 - val_loss: 2100912646520832.0000\n",
      "Epoch 392/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173232648516666392576.0000 - val_loss: 2100975863070720.0000\n",
      "Epoch 393/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173232578147922214912.0000 - val_loss: 2101027134242816.0000\n",
      "Epoch 394/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173232595740108259328.0000 - val_loss: 2101082029293568.0000\n",
      "Epoch 395/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173232507779178037248.0000 - val_loss: 2101175042179072.0000\n",
      "Epoch 396/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173232525371364081664.0000 - val_loss: 2101250472542208.0000\n",
      "Epoch 397/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173232437410433859584.0000 - val_loss: 2101401467486208.0000\n",
      "Epoch 398/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 173232472594805948416.0000 - val_loss: 2101525484666880.0000\n",
      "Epoch 399/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173232437410433859584.0000 - val_loss: 2101592056659968.0000\n",
      "Epoch 400/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173232490186991992832.0000 - val_loss: 2101746138611712.0000\n",
      "Epoch 401/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 173232472594805948416.0000 - val_loss: 2101903710224384.0000\n",
      "Epoch 402/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173232349449503637504.0000 - val_loss: 2102043430879232.0000\n",
      "Epoch 403/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173232349449503637504.0000 - val_loss: 2102122082467840.0000\n",
      "Epoch 404/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173232349449503637504.0000 - val_loss: 2102275895984128.0000\n",
      "Epoch 405/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173232261488573415424.0000 - val_loss: 2102434407120896.0000\n",
      "Epoch 406/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173232261488573415424.0000 - val_loss: 2102667811749888.0000\n",
      "Epoch 407/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173232314265131548672.0000 - val_loss: 2102807398187008.0000\n",
      "Epoch 408/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173232279080759459840.0000 - val_loss: 2102996376748032.0000\n",
      "Epoch 409/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173232138343271104512.0000 - val_loss: 2103246155939840.0000\n",
      "Epoch 410/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 173232138343271104512.0000 - val_loss: 2103437281984512.0000\n",
      "Epoch 411/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 173232155935457148928.0000 - val_loss: 2103654177832960.0000\n",
      "Epoch 412/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 173232085566712971264.0000 - val_loss: 2103881005793280.0000\n",
      "Epoch 413/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 173232067974526926848.0000 - val_loss: 2104035758833664.0000\n",
      "Epoch 414/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 173232103158899015680.0000 - val_loss: 2104217892290560.0000\n",
      "Epoch 415/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 173232103158899015680.0000 - val_loss: 2104777714434048.0000\n",
      "Epoch 416/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 173232085566712971264.0000 - val_loss: 2105112453447680.0000\n",
      "Epoch 417/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 173231892052666482688.0000 - val_loss: 2105312840515584.0000\n",
      "Epoch 418/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173231962421410660352.0000 - val_loss: 2105652008714240.0000\n",
      "Epoch 419/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173231997605782749184.0000 - val_loss: 2105958696222720.0000\n",
      "Epoch 420/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173231980013596704768.0000 - val_loss: 2106269410263040.0000\n",
      "Epoch 421/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 173231927237038571520.0000 - val_loss: 2106439061471232.0000\n",
      "Epoch 422/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 173231786499550216192.0000 - val_loss: 2106809233965056.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 423/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173231839276108349440.0000 - val_loss: 2107041699069952.0000\n",
      "Epoch 424/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173231786499550216192.0000 - val_loss: 2107720438120448.0000\n",
      "Epoch 425/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173231751315178127360.0000 - val_loss: 2108214627794944.0000\n",
      "Epoch 426/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173231663354247905280.0000 - val_loss: 2108442529497088.0000\n",
      "Epoch 427/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 173231663354247905280.0000 - val_loss: 2108912828416000.0000\n",
      "Epoch 428/1000\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 173231592985503727616.0000 - val_loss: 2109177103122432.0000\n",
      "Epoch 429/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 173231592985503727616.0000 - val_loss: 2109745917853696.0000\n",
      "Epoch 430/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173231680946433949696.0000 - val_loss: 2110197426290688.0000\n",
      "Epoch 431/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173231575393317683200.0000 - val_loss: 2110372311990272.0000\n",
      "Epoch 432/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 173231522616759549952.0000 - val_loss: 2110804627292160.0000\n",
      "Epoch 433/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173231505024573505536.0000 - val_loss: 2111146345627648.0000\n",
      "Epoch 434/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173231434655829327872.0000 - val_loss: 2111797033172992.0000\n",
      "Epoch 435/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173231452248015372288.0000 - val_loss: 2112276324679680.0000\n",
      "Epoch 436/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173231381879271194624.0000 - val_loss: 2112685286096896.0000\n",
      "Epoch 437/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173231399471457239040.0000 - val_loss: 2113049955663872.0000\n",
      "Epoch 438/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 173231346694899105792.0000 - val_loss: 2113621454749696.0000\n",
      "Epoch 439/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 173231346694899105792.0000 - val_loss: 2114224092348416.0000\n",
      "Epoch 440/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 173231258733968883712.0000 - val_loss: 2114677614051328.0000\n",
      "Epoch 441/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173231364287085150208.0000 - val_loss: 2115193949650944.0000\n",
      "Epoch 442/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173231205957410750464.0000 - val_loss: 2115499831853056.0000\n",
      "Epoch 443/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 173231153180852617216.0000 - val_loss: 2116658399281152.0000\n",
      "Epoch 444/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173231205957410750464.0000 - val_loss: 2117420621758464.0000\n",
      "Epoch 445/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173231205957410750464.0000 - val_loss: 2118019635478528.0000\n",
      "Epoch 446/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173231258733968883712.0000 - val_loss: 2118310082641920.0000\n",
      "Epoch 447/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173231153180852617216.0000 - val_loss: 2119105859551232.0000\n",
      "Epoch 448/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173231082812108439552.0000 - val_loss: 2119674405847040.0000\n",
      "Epoch 449/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173231030035550306304.0000 - val_loss: 2120353010679808.0000\n",
      "Epoch 450/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 173230994851178217472.0000 - val_loss: 2120846395047936.0000\n",
      "Epoch 451/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173230906890247995392.0000 - val_loss: 2121621636644864.0000\n",
      "Epoch 452/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 173230959666806128640.0000 - val_loss: 2122086701072384.0000\n",
      "Epoch 453/1000\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 173230959666806128640.0000 - val_loss: 2122639275458560.0000\n",
      "Epoch 454/1000\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 173230906890247995392.0000 - val_loss: 2123156416364544.0000\n",
      "Epoch 455/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 173230766152759640064.0000 - val_loss: 2123942932250624.0000\n",
      "Epoch 456/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 173230766152759640064.0000 - val_loss: 2124649588588544.0000\n",
      "Epoch 457/1000\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 173230854113689862144.0000 - val_loss: 2125383222689792.0000\n",
      "Epoch 458/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 173230836521503817728.0000 - val_loss: 2126181683953664.0000\n",
      "Epoch 459/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 173230730968387551232.0000 - val_loss: 2126746337935360.0000\n",
      "Epoch 460/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173230607823085240320.0000 - val_loss: 2127346022744064.0000\n",
      "Epoch 461/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173230643007457329152.0000 - val_loss: 2128510361534464.0000\n",
      "Epoch 462/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173230590230899195904.0000 - val_loss: 2129155948806144.0000\n",
      "Epoch 463/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173230713376201506816.0000 - val_loss: 2130019371450368.0000\n",
      "Epoch 464/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173230449493410840576.0000 - val_loss: 2130532083171328.0000\n",
      "Epoch 465/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173230555046527107072.0000 - val_loss: 2131262496047104.0000\n",
      "Epoch 466/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173230519862155018240.0000 - val_loss: 2132415828983808.0000\n",
      "Epoch 467/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173230431901224796160.0000 - val_loss: 2133350118588416.0000\n",
      "Epoch 468/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173230273571550396416.0000 - val_loss: 2133763240755200.0000\n",
      "Epoch 469/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173230431901224796160.0000 - val_loss: 2135590883557376.0000\n",
      "Epoch 470/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173230326348108529664.0000 - val_loss: 2136456587902976.0000\n",
      "Epoch 471/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173230273571550396416.0000 - val_loss: 2137821582196736.0000\n",
      "Epoch 472/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 173230273571550396416.0000 - val_loss: 2138786205007872.0000\n",
      "Epoch 473/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173230044873131819008.0000 - val_loss: 2139318915170304.0000\n",
      "Epoch 474/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173230203202806218752.0000 - val_loss: 2140609015971840.0000\n",
      "Epoch 475/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173229956912201596928.0000 - val_loss: 2141596590014464.0000\n",
      "Epoch 476/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173230150426248085504.0000 - val_loss: 2142525108256768.0000\n",
      "Epoch 477/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173230044873131819008.0000 - val_loss: 2143103720882176.0000\n",
      "Epoch 478/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173229939320015552512.0000 - val_loss: 2144067001516032.0000\n",
      "Epoch 479/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173229886543457419264.0000 - val_loss: 2146492718514176.0000\n",
      "Epoch 480/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173230009688759730176.0000 - val_loss: 2147643232878592.0000\n",
      "Epoch 481/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173229992096573685760.0000 - val_loss: 2148306402672640.0000\n",
      "Epoch 482/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173229921727829508096.0000 - val_loss: 2149297734811648.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 483/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173229780990341152768.0000 - val_loss: 2150562200027136.0000\n",
      "Epoch 484/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173229605068480708608.0000 - val_loss: 2152026381221888.0000\n",
      "Epoch 485/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173229780990341152768.0000 - val_loss: 2153215147638784.0000\n",
      "Epoch 486/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173229763398155108352.0000 - val_loss: 2154169569902592.0000\n",
      "Epoch 487/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173229728213783019520.0000 - val_loss: 2155021584039936.0000\n",
      "Epoch 488/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173229693029410930688.0000 - val_loss: 2156149952479232.0000\n",
      "Epoch 489/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173229569884108619776.0000 - val_loss: 2157668491853824.0000\n",
      "Epoch 490/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173229517107550486528.0000 - val_loss: 2158652307800064.0000\n",
      "Epoch 491/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173229534699736530944.0000 - val_loss: 2159601361354752.0000\n",
      "Epoch 492/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173229605068480708608.0000 - val_loss: 2162451072155648.0000\n",
      "Epoch 493/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173229481923178397696.0000 - val_loss: 2164116982595584.0000\n",
      "Epoch 494/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173229464330992353280.0000 - val_loss: 2165185355710464.0000\n",
      "Epoch 495/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173229429146620264448.0000 - val_loss: 2166726712098816.0000\n",
      "Epoch 496/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173229165263829598208.0000 - val_loss: 2167354582630400.0000\n",
      "Epoch 497/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173229429146620264448.0000 - val_loss: 2170218252075008.0000\n",
      "Epoch 498/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173229253224759820288.0000 - val_loss: 2171392254541824.0000\n",
      "Epoch 499/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173229446738806308864.0000 - val_loss: 2172630815735808.0000\n",
      "Epoch 500/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173229200448201687040.0000 - val_loss: 2173924808851456.0000\n",
      "Epoch 501/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173229165263829598208.0000 - val_loss: 2175264167559168.0000\n",
      "Epoch 502/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173228883788852887552.0000 - val_loss: 2177209519308800.0000\n",
      "Epoch 503/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173228954157597065216.0000 - val_loss: 2178587532722176.0000\n",
      "Epoch 504/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173228866196666843136.0000 - val_loss: 2180104327266304.0000\n",
      "Epoch 505/1000\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 173229006934155198464.0000 - val_loss: 2180985600868352.0000\n",
      "Epoch 506/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173228971749783109632.0000 - val_loss: 2182657014235136.0000\n",
      "Epoch 507/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173228795827922665472.0000 - val_loss: 2184528009363456.0000\n",
      "Epoch 508/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173228971749783109632.0000 - val_loss: 2185656914673664.0000\n",
      "Epoch 509/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173228848604480798720.0000 - val_loss: 2186799375974400.0000\n",
      "Epoch 510/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173228831012294754304.0000 - val_loss: 2188202622320640.0000\n",
      "Epoch 511/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173228707866992443392.0000 - val_loss: 2189988254973952.0000\n",
      "Epoch 512/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173228690274806398976.0000 - val_loss: 2192208753065984.0000\n",
      "Epoch 513/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 173228426392015732736.0000 - val_loss: 2193909560115200.0000\n",
      "Epoch 514/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173228637498248265728.0000 - val_loss: 2195100339798016.0000\n",
      "Epoch 515/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 173228672682620354560.0000 - val_loss: 2196596196376576.0000\n",
      "Epoch 516/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173228567129504088064.0000 - val_loss: 2198070309683200.0000\n",
      "Epoch 517/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173228426392015732736.0000 - val_loss: 2199826682871808.0000\n",
      "Epoch 518/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173228356023271555072.0000 - val_loss: 2201991614824448.0000\n",
      "Epoch 519/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173228443984201777152.0000 - val_loss: 2203660478054400.0000\n",
      "Epoch 520/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173228285654527377408.0000 - val_loss: 2204565776629760.0000\n",
      "Epoch 521/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173228461576387821568.0000 - val_loss: 2206155719835648.0000\n",
      "Epoch 522/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173228373615457599488.0000 - val_loss: 2208206566719488.0000\n",
      "Epoch 523/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173228250470155288576.0000 - val_loss: 2210731067965440.0000\n",
      "Epoch 524/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 173228162509225066496.0000 - val_loss: 2212543678382080.0000\n",
      "Epoch 525/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173228250470155288576.0000 - val_loss: 2214175497519104.0000\n",
      "Epoch 526/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173228109732666933248.0000 - val_loss: 2215727457107968.0000\n",
      "Epoch 527/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173228127324852977664.0000 - val_loss: 2217524095614976.0000\n",
      "Epoch 528/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173227968995178577920.0000 - val_loss: 2219190542925824.0000\n",
      "Epoch 529/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173228109732666933248.0000 - val_loss: 2221943616962560.0000\n",
      "Epoch 530/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 173228021771736711168.0000 - val_loss: 2223713814577152.0000\n",
      "Epoch 531/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 173227810665504178176.0000 - val_loss: 2225220005920768.0000\n",
      "Epoch 532/1000\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 173228004179550666752.0000 - val_loss: 2227870671831040.0000\n",
      "Epoch 533/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 173227916218620444672.0000 - val_loss: 2229991311933440.0000\n",
      "Epoch 534/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173227793073318133760.0000 - val_loss: 2232076652773376.0000\n",
      "Epoch 535/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 173227722704573956096.0000 - val_loss: 2234613502050304.0000\n",
      "Epoch 536/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173227687520201867264.0000 - val_loss: 2236833731706880.0000\n",
      "Epoch 537/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173227722704573956096.0000 - val_loss: 2238634396745728.0000\n",
      "Epoch 538/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173227722704573956096.0000 - val_loss: 2240917708734464.0000\n",
      "Epoch 539/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173227705112387911680.0000 - val_loss: 2243318058582016.0000\n",
      "Epoch 540/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173227458821783289856.0000 - val_loss: 2244929208188928.0000\n",
      "Epoch 541/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 173227476413969334272.0000 - val_loss: 2247099240415232.0000\n",
      "Epoch 542/1000\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 173227458821783289856.0000 - val_loss: 2249701587943424.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 543/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 173227494006155378688.0000 - val_loss: 2252374265561088.0000\n",
      "Epoch 544/1000\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 173227494006155378688.0000 - val_loss: 2254658651291648.0000\n",
      "Epoch 545/1000\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 173227247715550756864.0000 - val_loss: 2256962901245952.0000\n",
      "Epoch 546/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 173227335676480978944.0000 - val_loss: 2258011947008000.0000\n",
      "Epoch 547/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 173227230123364712448.0000 - val_loss: 2263249659625472.0000\n",
      "Epoch 548/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 173227071793690312704.0000 - val_loss: 2265316075765760.0000\n",
      "Epoch 549/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173227247715550756864.0000 - val_loss: 2268752586473472.0000\n",
      "Epoch 550/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173227071793690312704.0000 - val_loss: 2271517471670272.0000\n",
      "Epoch 551/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173226948648388001792.0000 - val_loss: 2274213637390336.0000\n",
      "Epoch 552/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173227071793690312704.0000 - val_loss: 2276022087057408.0000\n",
      "Epoch 553/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173227036609318223872.0000 - val_loss: 2278228358070272.0000\n",
      "Epoch 554/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173227054201504268288.0000 - val_loss: 2280957004480512.0000\n",
      "Epoch 555/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173226755134341513216.0000 - val_loss: 2283128647319552.0000\n",
      "Epoch 556/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173226790318713602048.0000 - val_loss: 2286567305510912.0000\n",
      "Epoch 557/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173226772726527557632.0000 - val_loss: 2289185893384192.0000\n",
      "Epoch 558/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173226508843736891392.0000 - val_loss: 2291194595901440.0000\n",
      "Epoch 559/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173226544028108980224.0000 - val_loss: 2294859008311296.0000\n",
      "Epoch 560/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173226508843736891392.0000 - val_loss: 2297906019172352.0000\n",
      "Epoch 561/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173226649581225246720.0000 - val_loss: 2300284357312512.0000\n",
      "Epoch 562/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173226684765597335552.0000 - val_loss: 2303572691648512.0000\n",
      "Epoch 563/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173226579212481069056.0000 - val_loss: 2305947003256832.0000\n",
      "Epoch 564/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173226438474992713728.0000 - val_loss: 2309303788634112.0000\n",
      "Epoch 565/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173226350514062491648.0000 - val_loss: 2312220339863552.0000\n",
      "Epoch 566/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173226227368760180736.0000 - val_loss: 2313491113312256.0000\n",
      "Epoch 567/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173226420882806669312.0000 - val_loss: 2317206528458752.0000\n",
      "Epoch 568/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173226227368760180736.0000 - val_loss: 2320502378987520.0000\n",
      "Epoch 569/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173226297737504358400.0000 - val_loss: 2323236662542336.0000\n",
      "Epoch 570/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173226297737504358400.0000 - val_loss: 2325836728369152.0000\n",
      "Epoch 571/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173226139407829958656.0000 - val_loss: 2329585966383104.0000\n",
      "Epoch 572/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173226227368760180736.0000 - val_loss: 2332529092722688.0000\n",
      "Epoch 573/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173225910709411381248.0000 - val_loss: 2335308204998656.0000\n",
      "Epoch 574/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173225787564109070336.0000 - val_loss: 2337437166600192.0000\n",
      "Epoch 575/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173226069039085780992.0000 - val_loss: 2340413578936320.0000\n",
      "Epoch 576/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173225910709411381248.0000 - val_loss: 2344821289123840.0000\n",
      "Epoch 577/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 173226033854713692160.0000 - val_loss: 2347629929299968.0000\n",
      "Epoch 578/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 173225629234434670592.0000 - val_loss: 2351026443124736.0000\n",
      "Epoch 579/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173225734787550937088.0000 - val_loss: 2354525230858240.0000\n",
      "Epoch 580/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173225822748481159168.0000 - val_loss: 2357181399695360.0000\n",
      "Epoch 581/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 173225682010992803840.0000 - val_loss: 2361008215556096.0000\n",
      "Epoch 582/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 173225418128202137600.0000 - val_loss: 2364023282597888.0000\n",
      "Epoch 583/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 173225435720388182016.0000 - val_loss: 2369071748218880.0000\n",
      "Epoch 584/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 173225558865690492928.0000 - val_loss: 2371624300969984.0000\n",
      "Epoch 585/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 173225400536016093184.0000 - val_loss: 2375523594403840.0000\n",
      "Epoch 586/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 173225488496946315264.0000 - val_loss: 2378934066872320.0000\n",
      "Epoch 587/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173225382943830048768.0000 - val_loss: 2383694501249024.0000\n",
      "Epoch 588/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173225470904760270848.0000 - val_loss: 2387670567223296.0000\n",
      "Epoch 589/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173225013507923116032.0000 - val_loss: 2389953073905664.0000\n",
      "Epoch 590/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 173225171837597515776.0000 - val_loss: 2394184690434048.0000\n",
      "Epoch 591/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 173224872770434760704.0000 - val_loss: 2398331481358336.0000\n",
      "Epoch 592/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 173224608887644094464.0000 - val_loss: 2402527932841984.0000\n",
      "Epoch 593/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 173224943139178938368.0000 - val_loss: 2406374076055552.0000\n",
      "Epoch 594/1000\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 173225101468853338112.0000 - val_loss: 2409271299932160.0000\n",
      "Epoch 595/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 173224802401690583040.0000 - val_loss: 2414219102257152.0000\n",
      "Epoch 596/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 173224485742341783552.0000 - val_loss: 2416704277708800.0000\n",
      "Epoch 597/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173224485742341783552.0000 - val_loss: 2422479129673728.0000\n",
      "Epoch 598/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173224714440760360960.0000 - val_loss: 2426594782085120.0000\n",
      "Epoch 599/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 173224679256388272128.0000 - val_loss: 2429948614672384.0000\n",
      "Epoch 600/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 173224397781411561472.0000 - val_loss: 2434412964741120.0000\n",
      "Epoch 601/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 173224802401690583040.0000 - val_loss: 2439322380795904.0000\n",
      "Epoch 602/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 173224292228295294976.0000 - val_loss: 2442245642911744.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 603/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 173224608887644094464.0000 - val_loss: 2447276123357184.0000\n",
      "Epoch 604/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 173224556111085961216.0000 - val_loss: 2450481511137280.0000\n",
      "Epoch 605/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173224274636109250560.0000 - val_loss: 2455596817186816.0000\n",
      "Epoch 606/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173224045937690673152.0000 - val_loss: 2460535224270848.0000\n",
      "Epoch 607/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173224098714248806400.0000 - val_loss: 2462712772689920.0000\n",
      "Epoch 608/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173224063529876717568.0000 - val_loss: 2468575403048960.0000\n",
      "Epoch 609/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173224239451737161728.0000 - val_loss: 2472908219744256.0000\n",
      "Epoch 610/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173223940384574406656.0000 - val_loss: 2478850038562816.0000\n",
      "Epoch 611/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 173224204267365072896.0000 - val_loss: 2482824225488896.0000\n",
      "Epoch 612/1000\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 173223711686155829248.0000 - val_loss: 2486845657055232.0000\n",
      "Epoch 613/1000\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 173223957976760451072.0000 - val_loss: 2489898841931776.0000\n",
      "Epoch 614/1000\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 173223922792388362240.0000 - val_loss: 2496102385319936.0000\n",
      "Epoch 615/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 173224045937690673152.0000 - val_loss: 2500499894960128.0000\n",
      "Epoch 616/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 173223535764295385088.0000 - val_loss: 2506353398513664.0000\n",
      "Epoch 617/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173223799647086051328.0000 - val_loss: 2508670264934400.0000\n",
      "Epoch 618/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173223518172109340672.0000 - val_loss: 2514380692389888.0000\n",
      "Epoch 619/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173223676501783740416.0000 - val_loss: 2520718185070592.0000\n",
      "Epoch 620/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173223500579923296256.0000 - val_loss: 2523267516596224.0000\n",
      "Epoch 621/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173223535764295385088.0000 - val_loss: 2530473666412544.0000\n",
      "Epoch 622/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 173223359842434940928.0000 - val_loss: 2533155605053440.0000\n",
      "Epoch 623/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173223166328388452352.0000 - val_loss: 2539401292808192.0000\n",
      "Epoch 624/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173222972814341963776.0000 - val_loss: 2545778782371840.0000\n",
      "Epoch 625/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173223307065876807680.0000 - val_loss: 2550879324471296.0000\n",
      "Epoch 626/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173223482987737251840.0000 - val_loss: 2555889403822080.0000\n",
      "Epoch 627/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173222920037783830528.0000 - val_loss: 2561254623281152.0000\n",
      "Epoch 628/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 173223148736202407936.0000 - val_loss: 2567152687120384.0000\n",
      "Epoch 629/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173222902445597786112.0000 - val_loss: 2572515222224896.0000\n",
      "Epoch 630/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 173223201512760541184.0000 - val_loss: 2577230559444992.0000\n",
      "Epoch 631/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 173222374680016453632.0000 - val_loss: 2582288420306944.0000\n",
      "Epoch 632/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 173222708931551297536.0000 - val_loss: 2587511905845248.0000\n",
      "Epoch 633/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 173222427456574586880.0000 - val_loss: 2592033969537024.0000\n",
      "Epoch 634/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 173222796892481519616.0000 - val_loss: 2599096774819840.0000\n",
      "Epoch 635/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173222568194062942208.0000 - val_loss: 2605155094626304.0000\n",
      "Epoch 636/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 173222726523737341952.0000 - val_loss: 2611351390257152.0000\n",
      "Epoch 637/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 173222198758156009472.0000 - val_loss: 2617426621497344.0000\n",
      "Epoch 638/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173222093205039742976.0000 - val_loss: 2622759091830784.0000\n",
      "Epoch 639/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 173222198758156009472.0000 - val_loss: 2627476308099072.0000\n",
      "Epoch 640/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173222216350342053888.0000 - val_loss: 2634488915951616.0000\n",
      "Epoch 641/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173222374680016453632.0000 - val_loss: 2640606291558400.0000\n",
      "Epoch 642/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173222269126900187136.0000 - val_loss: 2647177054650368.0000\n",
      "Epoch 643/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173221987651923476480.0000 - val_loss: 2653298188353536.0000\n",
      "Epoch 644/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173221846914435121152.0000 - val_loss: 2658775076962304.0000\n",
      "Epoch 645/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173221864506621165568.0000 - val_loss: 2663689593290752.0000\n",
      "Epoch 646/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173221424701970055168.0000 - val_loss: 2670583284236288.0000\n",
      "Epoch 647/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173221706176946765824.0000 - val_loss: 2677694944772096.0000\n",
      "Epoch 648/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173221653400388632576.0000 - val_loss: 2682776159518720.0000\n",
      "Epoch 649/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173221864506621165568.0000 - val_loss: 2689324105596928.0000\n",
      "Epoch 650/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173221706176946765824.0000 - val_loss: 2693424457187328.0000\n",
      "Epoch 651/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173221758953504899072.0000 - val_loss: 2701024133382144.0000\n",
      "Epoch 652/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173221583031644454912.0000 - val_loss: 2708325577785344.0000\n",
      "Epoch 653/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173221125634807300096.0000 - val_loss: 2711618743959552.0000\n",
      "Epoch 654/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173221108042621255680.0000 - val_loss: 2721692052881408.0000\n",
      "Epoch 655/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173221371925411921920.0000 - val_loss: 2727748762075136.0000\n",
      "Epoch 656/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173221160819179388928.0000 - val_loss: 2734088939110400.0000\n",
      "Epoch 657/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173221002489504989184.0000 - val_loss: 2741810753437696.0000\n",
      "Epoch 658/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173220404355179479040.0000 - val_loss: 2748946573164544.0000\n",
      "Epoch 659/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173221055266063122432.0000 - val_loss: 2755272523120640.0000\n",
      "Epoch 660/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173220826567644545024.0000 - val_loss: 2762047834030080.0000\n",
      "Epoch 661/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173220281209877168128.0000 - val_loss: 2767689273573376.0000\n",
      "Epoch 662/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173220984897318944768.0000 - val_loss: 2777143570333696.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 663/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173220721014528278528.0000 - val_loss: 2781774350385152.0000\n",
      "Epoch 664/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173220826567644545024.0000 - val_loss: 2790695802765312.0000\n",
      "Epoch 665/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173220633053598056448.0000 - val_loss: 2798041002147840.0000\n",
      "Epoch 666/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173220228433319034880.0000 - val_loss: 2802282013917184.0000\n",
      "Epoch 667/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173220527500481789952.0000 - val_loss: 2812151848763392.0000\n",
      "Epoch 668/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173220738606714322944.0000 - val_loss: 2815604465598464.0000\n",
      "Epoch 669/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173220404355179479040.0000 - val_loss: 2823687325614080.0000\n",
      "Epoch 670/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173220193248946946048.0000 - val_loss: 2833201751916544.0000\n",
      "Epoch 671/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173220263617691123712.0000 - val_loss: 2837872260415488.0000\n",
      "Epoch 672/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173219665483365613568.0000 - val_loss: 2844753334894592.0000\n",
      "Epoch 673/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 173220263617691123712.0000 - val_loss: 2863782724370432.0000\n",
      "Epoch 674/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173220298802063212544.0000 - val_loss: 2869763197894656.0000\n",
      "Epoch 675/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 173219771036481880064.0000 - val_loss: 2881251430105088.0000\n",
      "Epoch 676/1000\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 173219683075551657984.0000 - val_loss: 2889992493858816.0000\n",
      "Epoch 677/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 173219559930249347072.0000 - val_loss: 2898234267664384.0000\n",
      "Epoch 678/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 173219436784947036160.0000 - val_loss: 2901990753435648.0000\n",
      "Epoch 679/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 173219348824016814080.0000 - val_loss: 2909597408952320.0000\n",
      "Epoch 680/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 173219806220853968896.0000 - val_loss: 2920314157662208.0000\n",
      "Epoch 681/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 173219243270900547584.0000 - val_loss: 2927137250082816.0000\n",
      "Epoch 682/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 173219084941226147840.0000 - val_loss: 2936235869863936.0000\n",
      "Epoch 683/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 173219260863086592000.0000 - val_loss: 2942546787434496.0000\n",
      "Epoch 684/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 173218961795923836928.0000 - val_loss: 2954004686438400.0000\n",
      "Epoch 685/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 173218944203737792512.0000 - val_loss: 2957539176087552.0000\n",
      "Epoch 686/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 173218521991272726528.0000 - val_loss: 2966441032679424.0000\n",
      "Epoch 687/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 173219313639644725248.0000 - val_loss: 2977523122044928.0000\n",
      "Epoch 688/1000\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 173219120125598236672.0000 - val_loss: 2987221963505664.0000\n",
      "Epoch 689/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 173218697913133170688.0000 - val_loss: 2993396784300032.0000\n",
      "Epoch 690/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 173219102533412192256.0000 - val_loss: 3003056434184192.0000\n",
      "Epoch 691/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 173219190494342414336.0000 - val_loss: 3008193751941120.0000\n",
      "Epoch 692/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 173218891427179659264.0000 - val_loss: 3017526984310784.0000\n",
      "Epoch 693/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 173218416438156460032.0000 - val_loss: 3022932469088256.0000\n",
      "Epoch 694/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 173218592360016904192.0000 - val_loss: 3034328359501824.0000\n",
      "Epoch 695/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 173218398845970415616.0000 - val_loss: 3040165220057088.0000\n",
      "Epoch 696/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 173217818303830949888.0000 - val_loss: 3059407478849536.0000\n",
      "Epoch 697/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 173218574767830859776.0000 - val_loss: 3074419731726336.0000\n",
      "Epoch 698/1000\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 173217800711644905472.0000 - val_loss: 3084218162741248.0000\n",
      "Epoch 699/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 173217959041319305216.0000 - val_loss: 3091395053092864.0000\n",
      "Epoch 700/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 173218170147551838208.0000 - val_loss: 3100883978027008.0000\n",
      "Epoch 701/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 173217448867924017152.0000 - val_loss: 3110685093396480.0000\n",
      "Epoch 702/1000\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 173218082186621616128.0000 - val_loss: 3118409592078336.0000\n",
      "Epoch 703/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 173217906264761171968.0000 - val_loss: 3125237247901696.0000\n",
      "Epoch 704/1000\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 173218082186621616128.0000 - val_loss: 3131778214658048.0000\n",
      "Epoch 705/1000\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 173217255353877528576.0000 - val_loss: 3146160717955072.0000\n",
      "Epoch 706/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 173217484052296105984.0000 - val_loss: 3151651565207552.0000\n",
      "Epoch 707/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 173217536828854239232.0000 - val_loss: 3165443242065920.0000\n",
      "Epoch 708/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 173217519236668194816.0000 - val_loss: 3169422260830208.0000\n",
      "Epoch 709/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 173217695158528638976.0000 - val_loss: 3182319242313728.0000\n",
      "Epoch 710/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 173217835896016994304.0000 - val_loss: 3190794018095104.0000\n",
      "Epoch 711/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 173216973878900817920.0000 - val_loss: 3201807119548416.0000\n",
      "Epoch 712/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 173217132208575217664.0000 - val_loss: 3214021201231872.0000\n",
      "Epoch 713/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173217132208575217664.0000 - val_loss: 3224128333021184.0000\n",
      "Epoch 714/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173216938694528729088.0000 - val_loss: 3231374479720448.0000\n",
      "Epoch 715/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173216850733598507008.0000 - val_loss: 3243354217250816.0000\n",
      "Epoch 716/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 173216657219552018432.0000 - val_loss: 3253430478962688.0000\n",
      "Epoch 717/1000\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 173217519236668194816.0000 - val_loss: 3261468778692608.0000\n",
      "Epoch 718/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 173216481297691574272.0000 - val_loss: 3268291065806848.0000\n",
      "Epoch 719/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 173216850733598507008.0000 - val_loss: 3281325217808384.0000\n",
      "Epoch 720/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 173216393336761352192.0000 - val_loss: 3290930912165888.0000\n",
      "Epoch 721/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 173216727588296196096.0000 - val_loss: 3301652224278528.0000\n",
      "Epoch 722/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 173216428521133441024.0000 - val_loss: 3312740756094976.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 723/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 173216446113319485440.0000 - val_loss: 3320520015609856.0000\n",
      "Epoch 724/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 173216428521133441024.0000 - val_loss: 3327464172421120.0000\n",
      "Epoch 725/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 173216129453970685952.0000 - val_loss: 3339355057815552.0000\n",
      "Epoch 726/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 173215795202435842048.0000 - val_loss: 3353817823313920.0000\n",
      "Epoch 727/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173215988716482330624.0000 - val_loss: 3365546842128384.0000\n",
      "Epoch 728/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173216569258621796352.0000 - val_loss: 3370004481310720.0000\n",
      "Epoch 729/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 173215496135273086976.0000 - val_loss: 3385272553177088.0000\n",
      "Epoch 730/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173215513727459131392.0000 - val_loss: 3392346364313600.0000\n",
      "Epoch 731/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173216111861784641536.0000 - val_loss: 3408752334077952.0000\n",
      "Epoch 732/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173216023900854419456.0000 - val_loss: 3413970987778048.0000\n",
      "Epoch 733/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173215285029040553984.0000 - val_loss: 3435607690838016.0000\n",
      "Epoch 734/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173214722079087132672.0000 - val_loss: 3467285490565120.0000\n",
      "Epoch 735/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173215636872761442304.0000 - val_loss: 3476238081458176.0000\n",
      "Epoch 736/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173215021146249887744.0000 - val_loss: 3489639721598976.0000\n",
      "Epoch 737/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 173214985961877798912.0000 - val_loss: 3503255841669120.0000\n",
      "Epoch 738/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173214440604110422016.0000 - val_loss: 3516948734279680.0000\n",
      "Epoch 739/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173214880408761532416.0000 - val_loss: 3525711809740800.0000\n",
      "Epoch 740/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 173215038738435932160.0000 - val_loss: 3537489952243712.0000\n",
      "Epoch 741/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173214950777505710080.0000 - val_loss: 3542521774866432.0000\n",
      "Epoch 742/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173214933185319665664.0000 - val_loss: 3569743546023936.0000\n",
      "Epoch 743/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 173214739671273177088.0000 - val_loss: 3584543198019584.0000\n",
      "Epoch 744/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173214546157226688512.0000 - val_loss: 3591368706359296.0000\n",
      "Epoch 745/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 173214616525970866176.0000 - val_loss: 3621270268674048.0000\n",
      "Epoch 746/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173214686894715043840.0000 - val_loss: 3628682845356032.0000\n",
      "Epoch 747/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173214634118156910592.0000 - val_loss: 3645409327054848.0000\n",
      "Epoch 748/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 173214581341598777344.0000 - val_loss: 3658643731906560.0000\n",
      "Epoch 749/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173214035983831400448.0000 - val_loss: 3669363701841920.0000\n",
      "Epoch 750/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 173213385072947757056.0000 - val_loss: 3682095595520000.0000\n",
      "Epoch 751/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173213508218250067968.0000 - val_loss: 3685947375878144.0000\n",
      "Epoch 752/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173214141536947666944.0000 - val_loss: 3702286773649408.0000\n",
      "Epoch 753/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173214106352575578112.0000 - val_loss: 3714940821045248.0000\n",
      "Epoch 754/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173213349888575668224.0000 - val_loss: 3727391662800896.0000\n",
      "Epoch 755/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173214211905691844608.0000 - val_loss: 3732369798332416.0000\n",
      "Epoch 756/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173213842469784911872.0000 - val_loss: 3748118101229568.0000\n",
      "Epoch 757/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 173213701732296556544.0000 - val_loss: 3757569713635328.0000\n",
      "Epoch 758/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 173213244335459401728.0000 - val_loss: 3774320622960640.0000\n",
      "Epoch 759/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173213121190157090816.0000 - val_loss: 3785227155537920.0000\n",
      "Epoch 760/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 173213068413598957568.0000 - val_loss: 3796099596812288.0000\n",
      "Epoch 761/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 173212998044854779904.0000 - val_loss: 3802973155098624.0000\n",
      "Epoch 762/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 173213050821412913152.0000 - val_loss: 3834980593565696.0000\n",
      "Epoch 763/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173212822122994335744.0000 - val_loss: 3853876302184448.0000\n",
      "Epoch 764/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173212012882436292608.0000 - val_loss: 3860784757080064.0000\n",
      "Epoch 765/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 173212804530808291328.0000 - val_loss: 3878806439854080.0000\n",
      "Epoch 766/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173213086005785001984.0000 - val_loss: 3891429348737024.0000\n",
      "Epoch 767/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 173213349888575668224.0000 - val_loss: 3903067904802816.0000\n",
      "Epoch 768/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173212487871459491840.0000 - val_loss: 3917316995678208.0000\n",
      "Epoch 769/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173212927676110602240.0000 - val_loss: 3928933271601152.0000\n",
      "Epoch 770/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 173212347133971136512.0000 - val_loss: 3934165347074048.0000\n",
      "Epoch 771/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 173211801776203759616.0000 - val_loss: 3945254147325952.0000\n",
      "Epoch 772/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 173212716569878069248.0000 - val_loss: 3965598333665280.0000\n",
      "Epoch 773/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 173211889737133981696.0000 - val_loss: 3982731226644480.0000\n",
      "Epoch 774/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 173211238826250338304.0000 - val_loss: 3993053106798592.0000\n",
      "Epoch 775/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 173211625854343315456.0000 - val_loss: 4008577266089984.0000\n",
      "Epoch 776/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 173211907329320026112.0000 - val_loss: 4021072936566784.0000\n",
      "Epoch 777/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173211924921506070528.0000 - val_loss: 4032422119211008.0000\n",
      "Epoch 778/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 173211449932482871296.0000 - val_loss: 4047212375965696.0000\n",
      "Epoch 779/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173211643446529359872.0000 - val_loss: 4058769864523776.0000\n",
      "Epoch 780/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173212065658994425856.0000 - val_loss: 4075853902249984.0000\n",
      "Epoch 781/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 173211326787180560384.0000 - val_loss: 4084105071296512.0000\n",
      "Epoch 782/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 173211027720017805312.0000 - val_loss: 4105163295948800.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 783/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173211397155924738048.0000 - val_loss: 4118126849425408.0000\n",
      "Epoch 784/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173211326787180560384.0000 - val_loss: 4133377741422592.0000\n",
      "Epoch 785/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173211766591831670784.0000 - val_loss: 4147223642243072.0000\n",
      "Epoch 786/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173210095334157451264.0000 - val_loss: 4159919834005504.0000\n",
      "Epoch 787/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173210499954436472832.0000 - val_loss: 4175215823159296.0000\n",
      "Epoch 788/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173211361971552649216.0000 - val_loss: 4189155407953920.0000\n",
      "Epoch 789/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173210781429413183488.0000 - val_loss: 4199682104360960.0000\n",
      "Epoch 790/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173210499954436472832.0000 - val_loss: 4217146246692864.0000\n",
      "Epoch 791/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173210922166901538816.0000 - val_loss: 4222842178633728.0000\n",
      "Epoch 792/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173210834205971316736.0000 - val_loss: 4241064114257920.0000\n",
      "Epoch 793/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173211080496575938560.0000 - val_loss: 4256109116260352.0000\n",
      "Epoch 794/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173210957351273627648.0000 - val_loss: 4274500434657280.0000\n",
      "Epoch 795/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173210552730994606080.0000 - val_loss: 4291765129445376.0000\n",
      "Epoch 796/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173211256418436382720.0000 - val_loss: 4305118988075008.0000\n",
      "Epoch 797/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 173210904574715494400.0000 - val_loss: 4320934936707072.0000\n",
      "Epoch 798/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 173210218479459762176.0000 - val_loss: 4324791012032512.0000\n",
      "Epoch 799/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173209426831087763456.0000 - val_loss: 4337770134765568.0000\n",
      "Epoch 800/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 173210746245041094656.0000 - val_loss: 4362647659085824.0000\n",
      "Epoch 801/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173210236071645806592.0000 - val_loss: 4378063907323904.0000\n",
      "Epoch 802/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173210728652855050240.0000 - val_loss: 4395252366442496.0000\n",
      "Epoch 803/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 173210499954436472832.0000 - val_loss: 4408106129817600.0000\n",
      "Epoch 804/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173209919412297007104.0000 - val_loss: 4415925654650880.0000\n",
      "Epoch 805/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173209497199831941120.0000 - val_loss: 4459327104483328.0000\n",
      "Epoch 806/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173209743490436562944.0000 - val_loss: 4480867976085504.0000\n",
      "Epoch 807/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173209250909227319296.0000 - val_loss: 4494877354098688.0000\n",
      "Epoch 808/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173209374054529630208.0000 - val_loss: 4514202827882496.0000\n",
      "Epoch 809/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173208916657692475392.0000 - val_loss: 4528556608585728.0000\n",
      "Epoch 810/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173209426831087763456.0000 - val_loss: 4539965652336640.0000\n",
      "Epoch 811/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173208881473320386560.0000 - val_loss: 4553876514537472.0000\n",
      "Epoch 812/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 173209585160762163200.0000 - val_loss: 4567977965912064.0000\n",
      "Epoch 813/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173209215724855230464.0000 - val_loss: 4591790875213824.0000\n",
      "Epoch 814/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 173208388892111142912.0000 - val_loss: 4609887719915520.0000\n",
      "Epoch 815/1000\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 173208476853041364992.0000 - val_loss: 4626247250345984.0000\n",
      "Epoch 816/1000\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 173209110171738963968.0000 - val_loss: 4644258195701760.0000\n",
      "Epoch 817/1000\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 173208617590529720320.0000 - val_loss: 4651695468445696.0000\n",
      "Epoch 818/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 173209022210808741888.0000 - val_loss: 4671754039459840.0000\n",
      "Epoch 819/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 173209338870157541376.0000 - val_loss: 4683819676336128.0000\n",
      "Epoch 820/1000\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 173208529629599498240.0000 - val_loss: 4707242280484864.0000\n",
      "Epoch 821/1000\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 173209110171738963968.0000 - val_loss: 4721441442365440.0000\n",
      "Epoch 822/1000\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 173207509282808922112.0000 - val_loss: 4734367683313664.0000\n",
      "Epoch 823/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 173208494445227409408.0000 - val_loss: 4758694378078208.0000\n",
      "Epoch 824/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 173207861126529810432.0000 - val_loss: 4779049301835776.0000\n",
      "Epoch 825/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 173208793512390164480.0000 - val_loss: 4785992116469760.0000\n",
      "Epoch 826/1000\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 173208142601506521088.0000 - val_loss: 4811906338521088.0000\n",
      "Epoch 827/1000\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 173207861126529810432.0000 - val_loss: 4830266249969664.0000\n",
      "Epoch 828/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 173208265746808832000.0000 - val_loss: 4845895635959808.0000\n",
      "Epoch 829/1000\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 173207315768762433536.0000 - val_loss: 4851869935468544.0000\n",
      "Epoch 830/1000\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 173207403729692655616.0000 - val_loss: 4866172176564224.0000\n",
      "Epoch 831/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 173206911148483411968.0000 - val_loss: 4892855734632448.0000\n",
      "Epoch 832/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 173207773165599588352.0000 - val_loss: 4905157594710016.0000\n",
      "Epoch 833/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173207262992204300288.0000 - val_loss: 4931153421139968.0000\n",
      "Epoch 834/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173207984271832121344.0000 - val_loss: 4946765627260928.0000\n",
      "Epoch 835/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 173206594489134612480.0000 - val_loss: 4967125919727616.0000\n",
      "Epoch 836/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173207861126529810432.0000 - val_loss: 4980424749088768.0000\n",
      "Epoch 837/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173206999109413634048.0000 - val_loss: 4999998525669376.0000\n",
      "Epoch 838/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 173207280584390344704.0000 - val_loss: 5022404833181696.0000\n",
      "Epoch 839/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 173206788003181101056.0000 - val_loss: 5040316457418752.0000\n",
      "Epoch 840/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 173207386137506611200.0000 - val_loss: 5053670584483840.0000\n",
      "Epoch 841/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 173206277829785812992.0000 - val_loss: 5071934396039168.0000\n",
      "Epoch 842/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173205908393878880256.0000 - val_loss: 5086082555183104.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 843/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173205503773599858688.0000 - val_loss: 5134253834633216.0000\n",
      "Epoch 844/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173205873209506791424.0000 - val_loss: 5163298014101504.0000\n",
      "Epoch 845/1000\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 173206313014157901824.0000 - val_loss: 5190340201938944.0000\n",
      "Epoch 846/1000\n",
      "13/13 [==============================] - 0s 11ms/step - loss: 173204923231460392960.0000 - val_loss: 5207248414441472.0000\n",
      "Epoch 847/1000\n",
      "13/13 [==============================] - 0s 12ms/step - loss: 173206700042250878976.0000 - val_loss: 5222526686855168.0000\n",
      "Epoch 848/1000\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 173206471343832301568.0000 - val_loss: 5235513057345536.0000\n",
      "Epoch 849/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 173205961170437013504.0000 - val_loss: 5265671243956224.0000\n",
      "Epoch 850/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173205169522065014784.0000 - val_loss: 5273896106328064.0000\n",
      "Epoch 851/1000\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 173206313014157901824.0000 - val_loss: 5287196009431040.0000\n",
      "Epoch 852/1000\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 173205767656390524928.0000 - val_loss: 5319551071813632.0000\n",
      "Epoch 853/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 173206189868855590912.0000 - val_loss: 5338041442893824.0000\n",
      "Epoch 854/1000\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 173205609326716125184.0000 - val_loss: 5348043884855296.0000\n",
      "Epoch 855/1000\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 173205011192390615040.0000 - val_loss: 5373791140052992.0000\n",
      "Epoch 856/1000\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 173205134337692925952.0000 - val_loss: 5400138616930304.0000\n",
      "Epoch 857/1000\n",
      "13/13 [==============================] - 0s 11ms/step - loss: 173205239890809192448.0000 - val_loss: 5420907468161024.0000\n",
      "Epoch 858/1000\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 173204237136204660736.0000 - val_loss: 5436782204157952.0000\n",
      "Epoch 859/1000\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 173205785248576569344.0000 - val_loss: 5447800942755840.0000\n",
      "Epoch 860/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 173205327851739414528.0000 - val_loss: 5467523969449984.0000\n",
      "Epoch 861/1000\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 173204166767460483072.0000 - val_loss: 5498571214290944.0000\n",
      "Epoch 862/1000\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 173205486181413814272.0000 - val_loss: 5517612414926848.0000\n",
      "Epoch 863/1000\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 173205222298623148032.0000 - val_loss: 5537326851686400.0000\n",
      "Epoch 864/1000\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 173204606572111593472.0000 - val_loss: 5556705744125952.0000\n",
      "Epoch 865/1000\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 173203867700297728000.0000 - val_loss: 5576964567990272.0000\n",
      "Epoch 866/1000\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 173205081561134792704.0000 - val_loss: 5599464827912192.0000\n",
      "Epoch 867/1000\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 173203867700297728000.0000 - val_loss: 5617584758063104.0000\n",
      "Epoch 868/1000\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 173203832515925639168.0000 - val_loss: 5632908127633408.0000\n",
      "Epoch 869/1000\n",
      "13/13 [==============================] - 0s 12ms/step - loss: 173202988090995507200.0000 - val_loss: 5660552684634112.0000\n",
      "Epoch 870/1000\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 173203093644111773696.0000 - val_loss: 5674449051320320.0000\n",
      "Epoch 871/1000\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 173204026029972127744.0000 - val_loss: 5693659903164416.0000\n",
      "Epoch 872/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 173204888047088304128.0000 - val_loss: 5730087668285440.0000\n",
      "Epoch 873/1000\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 173204342689320927232.0000 - val_loss: 5736524750520320.0000\n",
      "Epoch 874/1000\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 173203709370623328256.0000 - val_loss: 5769340985016320.0000\n",
      "Epoch 875/1000\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 173203445487832662016.0000 - val_loss: 5789795766763520.0000\n",
      "Epoch 876/1000\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 173203093644111773696.0000 - val_loss: 5818142014046208.0000\n",
      "Epoch 877/1000\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 173204219544018616320.0000 - val_loss: 5838117370068992.0000\n",
      "Epoch 878/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 173204272320576749568.0000 - val_loss: 5857124747837440.0000\n",
      "Epoch 879/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 173203392711274528768.0000 - val_loss: 5883860382384128.0000\n",
      "Epoch 880/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 173202425141042085888.0000 - val_loss: 5898617353142272.0000\n",
      "Epoch 881/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 173203005683181551616.0000 - val_loss: 5927708777250816.0000\n",
      "Epoch 882/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173202794576949018624.0000 - val_loss: 5945409948090368.0000\n",
      "Epoch 883/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173203111236297818112.0000 - val_loss: 5974245284773888.0000\n",
      "Epoch 884/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173202143666065375232.0000 - val_loss: 5989543421411328.0000\n",
      "Epoch 885/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173202442733228130304.0000 - val_loss: 6015018483056640.0000\n",
      "Epoch 886/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173202900130065285120.0000 - val_loss: 6026442559193088.0000\n",
      "Epoch 887/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 173203128828483862528.0000 - val_loss: 6055691823349760.0000\n",
      "Epoch 888/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 173201439978623598592.0000 - val_loss: 6070381685243904.0000\n",
      "Epoch 889/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173201862191088664576.0000 - val_loss: 6105355067064320.0000\n",
      "Epoch 890/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 173201281648949198848.0000 - val_loss: 6112636110372864.0000\n",
      "Epoch 891/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173201844598902620160.0000 - val_loss: 6178827461984256.0000\n",
      "Epoch 892/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173201756637972398080.0000 - val_loss: 6203496680390656.0000\n",
      "Epoch 893/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 173200015011554000896.0000 - val_loss: 6238802922176512.0000\n",
      "Epoch 894/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173202038112949108736.0000 - val_loss: 6249558057156608.0000\n",
      "Epoch 895/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173201158503646887936.0000 - val_loss: 6269242966016000.0000\n",
      "Epoch 896/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173201827006716575744.0000 - val_loss: 6301254431014912.0000\n",
      "Epoch 897/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173200366855274889216.0000 - val_loss: 6324926143266816.0000\n",
      "Epoch 898/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 173202073297321197568.0000 - val_loss: 6335586788966400.0000\n",
      "Epoch 899/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 173202231626995597312.0000 - val_loss: 6372748456624128.0000\n",
      "Epoch 900/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 173200331670902800384.0000 - val_loss: 6398411959959552.0000\n",
      "Epoch 901/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 173199891866251689984.0000 - val_loss: 6425300065845248.0000\n",
      "Epoch 902/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13/13 [==============================] - 0s 9ms/step - loss: 173200525184949288960.0000 - val_loss: 6430330009419776.0000\n",
      "Epoch 903/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 173201352017693376512.0000 - val_loss: 6452474860797952.0000\n",
      "Epoch 904/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173200824252112044032.0000 - val_loss: 6482205161291776.0000\n",
      "Epoch 905/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173199768720949379072.0000 - val_loss: 6514089924755456.0000\n",
      "Epoch 906/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 173200472408391155712.0000 - val_loss: 6538770417451008.0000\n",
      "Epoch 907/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 173200138156856311808.0000 - val_loss: 6551340176113664.0000\n",
      "Epoch 908/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 173201211280205021184.0000 - val_loss: 6573358862827520.0000\n",
      "Epoch 909/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173199962234995867648.0000 - val_loss: 6602623696240640.0000\n",
      "Epoch 910/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173201017766158532608.0000 - val_loss: 6633856161546240.0000\n",
      "Epoch 911/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 173199240955368046592.0000 - val_loss: 6648979278266368.0000\n",
      "Epoch 912/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 173201211280205021184.0000 - val_loss: 6686981685772288.0000\n",
      "Epoch 913/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 173197675250810093568.0000 - val_loss: 6698824521220096.0000\n",
      "Epoch 914/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 173200278894344667136.0000 - val_loss: 6736240665690112.0000\n",
      "Epoch 915/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 173198713189786714112.0000 - val_loss: 6759289070813184.0000\n",
      "Epoch 916/1000\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 173199680760019156992.0000 - val_loss: 6770802267521024.0000\n",
      "Epoch 917/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 173199170586623868928.0000 - val_loss: 6810325093449728.0000\n",
      "Epoch 918/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 173200085380298178560.0000 - val_loss: 6833289209839616.0000\n",
      "Epoch 919/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 173199874274065645568.0000 - val_loss: 6867331120627712.0000\n",
      "Epoch 920/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 173198097463275159552.0000 - val_loss: 6894582151249920.0000\n",
      "Epoch 921/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 173197217853972938752.0000 - val_loss: 6897604197613568.0000\n",
      "Epoch 922/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 173198220608577470464.0000 - val_loss: 6973809265475584.0000\n",
      "Epoch 923/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173197692842996137984.0000 - val_loss: 7014154577641472.0000\n",
      "Epoch 924/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173197710435182182400.0000 - val_loss: 7032984788008960.0000\n",
      "Epoch 925/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 173199240955368046592.0000 - val_loss: 7064855055958016.0000\n",
      "Epoch 926/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 173198167832019337216.0000 - val_loss: 7093200229498880.0000\n",
      "Epoch 927/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173199065033507602432.0000 - val_loss: 7113135857074176.0000\n",
      "Epoch 928/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 173197200261786894336.0000 - val_loss: 7136387199401984.0000\n",
      "Epoch 929/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 173198062278903070720.0000 - val_loss: 7149386991665152.0000\n",
      "Epoch 930/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173197270630531072000.0000 - val_loss: 7206594882306048.0000\n",
      "Epoch 931/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 173198889111647158272.0000 - val_loss: 7288490781835264.0000\n",
      "Epoch 932/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173198255792949559296.0000 - val_loss: 7331558029524992.0000\n",
      "Epoch 933/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173198115055461203968.0000 - val_loss: 7362604737495040.0000\n",
      "Epoch 934/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 173196232691554451456.0000 - val_loss: 7367313632264192.0000\n",
      "Epoch 935/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173197692842996137984.0000 - val_loss: 7395124082376704.0000\n",
      "Epoch 936/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173198343753879781376.0000 - val_loss: 7428236669616128.0000\n",
      "Epoch 937/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 173197024339926450176.0000 - val_loss: 7440566983852032.0000\n",
      "Epoch 938/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173197868764856582144.0000 - val_loss: 7468927726649344.0000\n",
      "Epoch 939/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173197728027368226816.0000 - val_loss: 7489444248551424.0000\n",
      "Epoch 940/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173196179914996318208.0000 - val_loss: 7519095092150272.0000\n",
      "Epoch 941/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173197059524298539008.0000 - val_loss: 7547894995353600.0000\n",
      "Epoch 942/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173198009502344937472.0000 - val_loss: 7575629209796608.0000\n",
      "Epoch 943/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173199135402251780096.0000 - val_loss: 7588275204128768.0000\n",
      "Epoch 944/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173196707680577650688.0000 - val_loss: 7671171831037952.0000\n",
      "Epoch 945/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 173196778049321828352.0000 - val_loss: 7708421545525248.0000\n",
      "Epoch 946/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173196725272763695104.0000 - val_loss: 7733230887239680.0000\n",
      "Epoch 947/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173195106791647608832.0000 - val_loss: 7758894390575104.0000\n",
      "Epoch 948/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173195634557228941312.0000 - val_loss: 7788188215017472.0000\n",
      "Epoch 949/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173196478982159073280.0000 - val_loss: 7796461932642304.0000\n",
      "Epoch 950/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173195001238531342336.0000 - val_loss: 7822539363450880.0000\n",
      "Epoch 951/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 173196303060298629120.0000 - val_loss: 7862682275282944.0000\n",
      "Epoch 952/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173196074361880051712.0000 - val_loss: 7880153128501248.0000\n",
      "Epoch 953/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173195968808763785216.0000 - val_loss: 7920275102367744.0000\n",
      "Epoch 954/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173194966054159253504.0000 - val_loss: 7926249938747392.0000\n",
      "Epoch 955/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 173195916032205651968.0000 - val_loss: 7953804838305792.0000\n",
      "Epoch 956/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 173195898440019607552.0000 - val_loss: 7984672768262144.0000\n",
      "Epoch 957/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 173196391021228851200.0000 - val_loss: 8022400834732032.0000\n",
      "Epoch 958/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 173195476227554541568.0000 - val_loss: 8054714558054400.0000\n",
      "Epoch 959/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173194860501042987008.0000 - val_loss: 8085091787997184.0000\n",
      "Epoch 960/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173193629048019877888.0000 - val_loss: 8095184424271872.0000\n",
      "Epoch 961/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 173195124383833653248.0000 - val_loss: 8123223044521984.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 962/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173193400349601300480.0000 - val_loss: 8150316771966976.0000\n",
      "Epoch 963/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173194948461973209088.0000 - val_loss: 8172495983083520.0000\n",
      "Epoch 964/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173194191997973299200.0000 - val_loss: 8207980466012160.0000\n",
      "Epoch 965/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 173195405858810363904.0000 - val_loss: 8242828756910080.0000\n",
      "Epoch 966/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173194174405787254784.0000 - val_loss: 8256051350601728.0000\n",
      "Epoch 967/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173195229936949919744.0000 - val_loss: 8325197639712768.0000\n",
      "Epoch 968/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173192608701229301760.0000 - val_loss: 8371632141762560.0000\n",
      "Epoch 969/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173192925360578101248.0000 - val_loss: 8410079078383616.0000\n",
      "Epoch 970/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173194649394810454016.0000 - val_loss: 8423924442333184.0000\n",
      "Epoch 971/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173193664232391966720.0000 - val_loss: 8451411159285760.0000\n",
      "Epoch 972/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173192890176206012416.0000 - val_loss: 8499941068505088.0000\n",
      "Epoch 973/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173193048505880412160.0000 - val_loss: 8529818471628800.0000\n",
      "Epoch 974/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173192890176206012416.0000 - val_loss: 8561345142194176.0000\n",
      "Epoch 975/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173193963299554721792.0000 - val_loss: 8588875882561536.0000\n",
      "Epoch 976/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 173192028159089836032.0000 - val_loss: 8623582439538688.0000\n",
      "Epoch 977/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173193857746438455296.0000 - val_loss: 8636467341426688.0000\n",
      "Epoch 978/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 173193277204298989568.0000 - val_loss: 8660761823936512.0000\n",
      "Epoch 979/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173193611455833833472.0000 - val_loss: 8696028874145792.0000\n",
      "Epoch 980/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 173193963299554721792.0000 - val_loss: 8732675682598912.0000\n",
      "Epoch 981/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173192819807461834752.0000 - val_loss: 8751945053372416.0000\n",
      "Epoch 982/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173190233756113305600.0000 - val_loss: 8768083392987136.0000\n",
      "Epoch 983/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 173193699416764055552.0000 - val_loss: 8800346113572864.0000\n",
      "Epoch 984/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173192292041880502272.0000 - val_loss: 8827084969345024.0000\n",
      "Epoch 985/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173191992974717747200.0000 - val_loss: 8860761807912960.0000\n",
      "Epoch 986/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 173190761521694638080.0000 - val_loss: 8900670107156480.0000\n",
      "Epoch 987/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173191306879462014976.0000 - val_loss: 8936507851145216.0000\n",
      "Epoch 988/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173189055079648329728.0000 - val_loss: 8973263107522560.0000\n",
      "Epoch 989/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 173193083690252500992.0000 - val_loss: 8981526624600064.0000\n",
      "Epoch 990/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 173192344818438635520.0000 - val_loss: 9001974963896320.0000\n",
      "Epoch 991/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173190849482624860160.0000 - val_loss: 9057963620696064.0000\n",
      "Epoch 992/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 173192133712206102528.0000 - val_loss: 9092173035208704.0000\n",
      "Epoch 993/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 173192907768392056832.0000 - val_loss: 9117326511177728.0000\n",
      "Epoch 994/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173191482801322459136.0000 - val_loss: 9145793554415616.0000\n",
      "Epoch 995/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173191042996671348736.0000 - val_loss: 9172249479217152.0000\n",
      "Epoch 996/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 173191517985694547968.0000 - val_loss: 9209713405198336.0000\n",
      "Epoch 997/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173190743929508593664.0000 - val_loss: 9221708175114240.0000\n",
      "Epoch 998/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173191517985694547968.0000 - val_loss: 9273279994920960.0000\n",
      "Epoch 999/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 173191007812299259904.0000 - val_loss: 9308855209033728.0000\n",
      "Epoch 1000/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173190145795183083520.0000 - val_loss: 9333201231151104.0000\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(np.asarray(X_train).astype('float32'), np.asarray(y_train).astype('float32'), batch_size = 1024, epochs = 1000, \n",
    "                    validation_split = 0.25, shuffle = True, callbacks = None,\n",
    "                    use_multiprocessing=True, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learningCurve(history):\n",
    "    plt.figure(figsize=(10,8))\n",
    "    plt.plot(history.history['loss'], linewidth=1)\n",
    "    plt.plot(history.history['val_loss'], linewidth=1)\n",
    "    plt.title('Model Loss over Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['training sample loss','validation sample loss'])\n",
    "    plt.savefig('Learning_curve')\n",
    "    plt.show\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learningCurve(history)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights('modelJ.h5')\n",
    "model_json = model.to_json()\n",
    "with open(\"modelJ.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df.critical_temp >= 10 ] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>wtd_mean_fie</th>\n",
       "      <th>gmean_fie</th>\n",
       "      <th>wtd_gmean_fie</th>\n",
       "      <th>entropy_fie</th>\n",
       "      <th>wtd_entropy_fie</th>\n",
       "      <th>range_fie</th>\n",
       "      <th>wtd_range_fie</th>\n",
       "      <th>std_fie</th>\n",
       "      <th>wtd_std_fie</th>\n",
       "      <th>mean_atomic_radius</th>\n",
       "      <th>...</th>\n",
       "      <th>W</th>\n",
       "      <th>Re</th>\n",
       "      <th>Os</th>\n",
       "      <th>Ir</th>\n",
       "      <th>Pt</th>\n",
       "      <th>Au</th>\n",
       "      <th>Hg</th>\n",
       "      <th>Tl</th>\n",
       "      <th>Pb</th>\n",
       "      <th>Bi</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1010.268571</td>\n",
       "      <td>718.1529</td>\n",
       "      <td>938.01678</td>\n",
       "      <td>1.305967</td>\n",
       "      <td>0.791488</td>\n",
       "      <td>810.6</td>\n",
       "      <td>735.985714</td>\n",
       "      <td>323.811808</td>\n",
       "      <td>355.562967</td>\n",
       "      <td>160.25</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1010.612857</td>\n",
       "      <td>720.605511</td>\n",
       "      <td>938.745413</td>\n",
       "      <td>1.544145</td>\n",
       "      <td>0.807078</td>\n",
       "      <td>810.6</td>\n",
       "      <td>743.164286</td>\n",
       "      <td>290.183029</td>\n",
       "      <td>354.963511</td>\n",
       "      <td>161.2</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1010.82</td>\n",
       "      <td>718.1529</td>\n",
       "      <td>939.009036</td>\n",
       "      <td>1.305967</td>\n",
       "      <td>0.77362</td>\n",
       "      <td>810.6</td>\n",
       "      <td>743.164286</td>\n",
       "      <td>323.811808</td>\n",
       "      <td>354.804183</td>\n",
       "      <td>160.25</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1010.544286</td>\n",
       "      <td>718.1529</td>\n",
       "      <td>938.512777</td>\n",
       "      <td>1.305967</td>\n",
       "      <td>0.783207</td>\n",
       "      <td>810.6</td>\n",
       "      <td>739.575</td>\n",
       "      <td>323.811808</td>\n",
       "      <td>355.183884</td>\n",
       "      <td>160.25</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1009.717143</td>\n",
       "      <td>718.1529</td>\n",
       "      <td>937.025573</td>\n",
       "      <td>1.305967</td>\n",
       "      <td>0.80523</td>\n",
       "      <td>810.6</td>\n",
       "      <td>728.807143</td>\n",
       "      <td>323.811808</td>\n",
       "      <td>356.319281</td>\n",
       "      <td>160.25</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21258</th>\n",
       "      <td>753.793333</td>\n",
       "      <td>651.611213</td>\n",
       "      <td>750.570867</td>\n",
       "      <td>1.371139</td>\n",
       "      <td>0.92705</td>\n",
       "      <td>273.0</td>\n",
       "      <td>427.546667</td>\n",
       "      <td>114.383355</td>\n",
       "      <td>64.428777</td>\n",
       "      <td>176.5</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21259</th>\n",
       "      <td>989.819048</td>\n",
       "      <td>702.115184</td>\n",
       "      <td>923.426093</td>\n",
       "      <td>1.541006</td>\n",
       "      <td>0.98847</td>\n",
       "      <td>810.6</td>\n",
       "      <td>659.771429</td>\n",
       "      <td>293.286136</td>\n",
       "      <td>345.450969</td>\n",
       "      <td>159.2</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21260</th>\n",
       "      <td>691.58</td>\n",
       "      <td>730.207231</td>\n",
       "      <td>689.480961</td>\n",
       "      <td>0.688594</td>\n",
       "      <td>0.54212</td>\n",
       "      <td>139.9</td>\n",
       "      <td>370.18</td>\n",
       "      <td>69.95</td>\n",
       "      <td>55.96</td>\n",
       "      <td>183.5</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21261</th>\n",
       "      <td>706.969</td>\n",
       "      <td>730.207231</td>\n",
       "      <td>704.143255</td>\n",
       "      <td>0.688594</td>\n",
       "      <td>0.648876</td>\n",
       "      <td>139.9</td>\n",
       "      <td>208.799</td>\n",
       "      <td>69.95</td>\n",
       "      <td>64.702805</td>\n",
       "      <td>183.5</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21262</th>\n",
       "      <td>821.19</td>\n",
       "      <td>852.858789</td>\n",
       "      <td>818.631908</td>\n",
       "      <td>1.094784</td>\n",
       "      <td>0.968771</td>\n",
       "      <td>181.3</td>\n",
       "      <td>285.51</td>\n",
       "      <td>74.569624</td>\n",
       "      <td>65.291691</td>\n",
       "      <td>127.333333</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>21263 rows  148 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      wtd_mean_fie   gmean_fie wtd_gmean_fie entropy_fie wtd_entropy_fie  \\\n",
       "0      1010.268571    718.1529     938.01678    1.305967        0.791488   \n",
       "1      1010.612857  720.605511    938.745413    1.544145        0.807078   \n",
       "2          1010.82    718.1529    939.009036    1.305967         0.77362   \n",
       "3      1010.544286    718.1529    938.512777    1.305967        0.783207   \n",
       "4      1009.717143    718.1529    937.025573    1.305967         0.80523   \n",
       "...            ...         ...           ...         ...             ...   \n",
       "21258   753.793333  651.611213    750.570867    1.371139         0.92705   \n",
       "21259   989.819048  702.115184    923.426093    1.541006         0.98847   \n",
       "21260       691.58  730.207231    689.480961    0.688594         0.54212   \n",
       "21261      706.969  730.207231    704.143255    0.688594        0.648876   \n",
       "21262       821.19  852.858789    818.631908    1.094784        0.968771   \n",
       "\n",
       "      range_fie wtd_range_fie     std_fie wtd_std_fie mean_atomic_radius  ...  \\\n",
       "0         810.6    735.985714  323.811808  355.562967             160.25  ...   \n",
       "1         810.6    743.164286  290.183029  354.963511              161.2  ...   \n",
       "2         810.6    743.164286  323.811808  354.804183             160.25  ...   \n",
       "3         810.6       739.575  323.811808  355.183884             160.25  ...   \n",
       "4         810.6    728.807143  323.811808  356.319281             160.25  ...   \n",
       "...         ...           ...         ...         ...                ...  ...   \n",
       "21258     273.0    427.546667  114.383355   64.428777              176.5  ...   \n",
       "21259     810.6    659.771429  293.286136  345.450969              159.2  ...   \n",
       "21260     139.9        370.18       69.95       55.96              183.5  ...   \n",
       "21261     139.9       208.799       69.95   64.702805              183.5  ...   \n",
       "21262     181.3        285.51   74.569624   65.291691         127.333333  ...   \n",
       "\n",
       "         W   Re   Os   Ir   Pt   Au   Hg   Tl   Pb   Bi  \n",
       "0      0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "1      0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "2      0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "3      0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "4      0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "...    ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  \n",
       "21258  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "21259  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  \n",
       "21260  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "21261  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "21262  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "\n",
       "[21263 rows x 148 columns]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('tcPred',df.drop('critical_temp', axis=1).astype('float32'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('gitCol', df.drop('critical_temp', axis=1).columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.clo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "df5fa5efdf1c60a896ccc8bc52bcd2fc69320846d08f7bc80e2522b0c75b6345"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
