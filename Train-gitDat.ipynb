{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import PIL\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "import time\n",
    "from tensorflow import data\n",
    "import pandas as pd\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Input, Dense,ZeroPadding2D, BatchNormalization, Activation, Layer, ReLU, LeakyReLU,Conv2D,AveragePooling2D,UpSampling2D,Reshape,Flatten\n",
    "from tensorflow.keras.models import Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 1024\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.load('tcPred.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M2 Max\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-24 14:57:10.775332: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2023-02-24 14:57:10.775355: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    }
   ],
   "source": [
    "train_dataset = tf.data.Dataset.from_tensor_slices(data).shuffle(60000).batch(1024)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(21263, 147)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "DSHAPE = 147"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_generator_model():\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(layers.Dense(256, activation='relu', use_bias=False, input_shape=(DSHAPE,)))\n",
    "    model.add(layers.BatchNormalization())\n",
    "\n",
    "    model.add(layers.Dropout(0.3))\n",
    "\n",
    "    model.add(layers.Dense(128,activation='relu'))\n",
    "    model.add(layers.Dropout(0.3))\n",
    "\n",
    "    model.add(layers.Dense(64,activation='relu'))\n",
    "    model.add(layers.Dense(64,activation='relu'))\n",
    "\n",
    "    model.add(layers.Dense(DSHAPE, activation='relu', use_bias=False, input_shape=(DSHAPE,)))\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "noise = tf.random.normal([1, DSHAPE])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_discriminator_model():\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(layers.Dense(256, activation='relu', use_bias=False, input_shape=(DSHAPE,)))\n",
    "    model.add(layers.BatchNormalization())\n",
    "\n",
    "    model.add(layers.Dropout(0.3))\n",
    "\n",
    "    model.add(layers.Dense(128,activation='relu'))\n",
    "    model.add(layers.Dropout(0.3))\n",
    "   \n",
    "    model.add(layers.Dense(128,activation='relu'))\n",
    "    model.add(layers.Dropout(0.3))\n",
    "\n",
    "    model.add(layers.Dense(64, activation='relu'))\n",
    "    model.add(layers.Dense(64, activation='relu'))\n",
    "    model.add(layers.Dense(1, activation='sigmoid'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_entropy = tf.keras.losses.BinaryCrossentropy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator_loss(real_output, fake_output):\n",
    "    real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n",
    "    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n",
    "    return real_loss + fake_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator_loss(fake_output):\n",
    "    return cross_entropy(tf.ones_like(fake_output), fake_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_optimizer = tf.keras.optimizers.legacy.Adam(1e-4)\n",
    "discriminator_optimizer = tf.keras.optimizers.legacy.Adam(1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = './training_checkpoints3'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer,\n",
    "                                 discriminator_optimizer=discriminator_optimizer,\n",
    "                                 generator=generator,\n",
    "                                 discriminator=discriminator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 40\n",
    "noise_dim = DSHAPE\n",
    "num_examples_to_generate = 50\n",
    "\n",
    "# You will reuse this seed overtime (so it's easier)\n",
    "# to visualize progress in the animated GIF)\n",
    "seed = tf.random.normal([num_examples_to_generate, noise_dim])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset element_spec=TensorSpec(shape=(None, 147), dtype=tf.float32, name=None)>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(images, gen_losses, disc_losses):\n",
    "    noise = tf.random.normal([BATCH_SIZE, noise_dim])\n",
    "\n",
    "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "        generated_images = generator(noise, training=True)\n",
    "        \n",
    "        real_output = discriminator(images, training=True)\n",
    "        fake_output = discriminator(generated_images, training=True)\n",
    "        gen_loss = generator_loss(fake_output)\n",
    "\n",
    "        disc_loss = discriminator_loss(real_output, fake_output)\n",
    "        gen_losses = gen_losses.append(gen_loss.numpy())\n",
    "        disc_losses = disc_losses.append(disc_loss.numpy())\n",
    "    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
    "    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
    "\n",
    "    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
    "    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train(dataset, epochs, gen_losses, disc_losses, gloss, dloss):\n",
    "  for epoch in range(epochs):\n",
    "    start = time.time()\n",
    "\n",
    "\n",
    "    for image_batch in dataset:\n",
    "        train_step(image_batch, gen_losses,disc_losses)\n",
    "    print(\"gen_loss =\" + str(gen_losses[-1]))\n",
    "    print(\"disc_loss =\" + str(disc_losses[-1]))\n",
    "    gloss.append(gen_losses[-1])\n",
    "    dloss.append(disc_losses[-1])\n",
    "\n",
    "    # Produce images for the GIF as you go\n",
    "    x = generate_and_save_images(generator,\n",
    "                             epoch + 1,\n",
    "                             seed)\n",
    "\n",
    "    # Save the model every 15 epochs\n",
    "    if (epoch + 1) % 15 == 0:\n",
    "      checkpoint.save(file_prefix = checkpoint_prefix)\n",
    "\n",
    "    print ('Time for epoch {} is {} sec'.format(epoch + 1, time.time()-start))\n",
    "\n",
    "  # Generate after the final epoch\n",
    "  saved = generate_and_save_images(generator,\n",
    "                           epochs,\n",
    "                           seed)\n",
    "  return saved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_and_save_images(model, epoch, test_input):\n",
    "  # Notice `training` is set to False.\n",
    "  # This is so all layers run in inference mode (batchnorm).\n",
    "  predictions = model(test_input, training=False)\n",
    "  return predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_losses = []\n",
    "disc_losses = []\n",
    "gloss = []\n",
    "dloss = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator = make_discriminator_model()\n",
    "generator = make_generator_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gen_loss =0.7598083\n",
      "disc_loss =1.3518896\n",
      "Time for epoch 1 is 1.876565933227539 sec\n",
      "gen_loss =0.82013905\n",
      "disc_loss =1.2123873\n",
      "Time for epoch 2 is 1.207535982131958 sec\n",
      "gen_loss =0.9826574\n",
      "disc_loss =0.9979788\n",
      "Time for epoch 3 is 1.2297310829162598 sec\n",
      "gen_loss =1.2618387\n",
      "disc_loss =0.76745796\n",
      "Time for epoch 4 is 1.2117040157318115 sec\n",
      "gen_loss =1.581557\n",
      "disc_loss =0.60295224\n",
      "Time for epoch 5 is 1.1917388439178467 sec\n",
      "gen_loss =1.939887\n",
      "disc_loss =0.4535389\n",
      "Time for epoch 6 is 1.1821820735931396 sec\n",
      "gen_loss =2.2210493\n",
      "disc_loss =0.38744238\n",
      "Time for epoch 7 is 1.1916718482971191 sec\n",
      "gen_loss =2.448646\n",
      "disc_loss =0.3535519\n",
      "Time for epoch 8 is 1.210179090499878 sec\n",
      "gen_loss =2.6446161\n",
      "disc_loss =0.31141812\n",
      "Time for epoch 9 is 1.2960278987884521 sec\n",
      "gen_loss =2.7952325\n",
      "disc_loss =0.2839938\n",
      "Time for epoch 10 is 1.2748379707336426 sec\n",
      "gen_loss =2.8918185\n",
      "disc_loss =0.3407809\n",
      "Time for epoch 11 is 1.2347829341888428 sec\n",
      "gen_loss =2.9161575\n",
      "disc_loss =0.31672424\n",
      "Time for epoch 12 is 1.2121498584747314 sec\n",
      "gen_loss =2.859252\n",
      "disc_loss =0.37762928\n",
      "Time for epoch 13 is 1.27708101272583 sec\n",
      "gen_loss =2.9453254\n",
      "disc_loss =0.32592285\n",
      "Time for epoch 14 is 1.2641470432281494 sec\n",
      "gen_loss =2.9527535\n",
      "disc_loss =0.32116407\n",
      "Time for epoch 15 is 1.2711181640625 sec\n",
      "gen_loss =3.013908\n",
      "disc_loss =0.27343822\n",
      "Time for epoch 16 is 1.226912021636963 sec\n",
      "gen_loss =2.9424763\n",
      "disc_loss =0.26928797\n",
      "Time for epoch 17 is 1.4057250022888184 sec\n",
      "gen_loss =2.937179\n",
      "disc_loss =0.27210277\n",
      "Time for epoch 18 is 1.2935469150543213 sec\n",
      "gen_loss =2.9078722\n",
      "disc_loss =0.23369882\n",
      "Time for epoch 19 is 1.2351438999176025 sec\n",
      "gen_loss =3.0207238\n",
      "disc_loss =0.24967223\n",
      "Time for epoch 20 is 1.287520408630371 sec\n",
      "gen_loss =2.952991\n",
      "disc_loss =0.26091337\n",
      "Time for epoch 21 is 1.2450618743896484 sec\n",
      "gen_loss =3.1057863\n",
      "disc_loss =0.30190188\n",
      "Time for epoch 22 is 1.2675199508666992 sec\n",
      "gen_loss =3.231945\n",
      "disc_loss =0.24459879\n",
      "Time for epoch 23 is 1.21645188331604 sec\n",
      "gen_loss =3.368424\n",
      "disc_loss =0.23069751\n",
      "Time for epoch 24 is 1.238130807876587 sec\n",
      "gen_loss =3.4893813\n",
      "disc_loss =0.2713892\n",
      "Time for epoch 25 is 1.214292287826538 sec\n",
      "gen_loss =3.547041\n",
      "disc_loss =0.2563755\n",
      "Time for epoch 26 is 1.2638769149780273 sec\n",
      "gen_loss =3.4301996\n",
      "disc_loss =0.19665055\n",
      "Time for epoch 27 is 1.2667648792266846 sec\n",
      "gen_loss =3.6701136\n",
      "disc_loss =0.24807978\n",
      "Time for epoch 28 is 1.2273998260498047 sec\n",
      "gen_loss =3.6153235\n",
      "disc_loss =0.24259883\n",
      "Time for epoch 29 is 1.2605111598968506 sec\n",
      "gen_loss =3.4238272\n",
      "disc_loss =0.3107692\n",
      "Time for epoch 30 is 1.2641737461090088 sec\n",
      "gen_loss =3.4785745\n",
      "disc_loss =0.3472633\n",
      "Time for epoch 31 is 1.234870195388794 sec\n",
      "gen_loss =3.2544968\n",
      "disc_loss =0.45183653\n",
      "Time for epoch 32 is 1.2204060554504395 sec\n",
      "gen_loss =3.2084837\n",
      "disc_loss =0.4353122\n",
      "Time for epoch 33 is 1.1995089054107666 sec\n",
      "gen_loss =3.116579\n",
      "disc_loss =0.43791455\n",
      "Time for epoch 34 is 1.1716930866241455 sec\n",
      "gen_loss =3.111834\n",
      "disc_loss =0.37279236\n",
      "Time for epoch 35 is 1.2257399559020996 sec\n",
      "gen_loss =2.9360297\n",
      "disc_loss =0.36383274\n",
      "Time for epoch 36 is 1.2299590110778809 sec\n",
      "gen_loss =3.007336\n",
      "disc_loss =0.3505991\n",
      "Time for epoch 37 is 1.222372055053711 sec\n",
      "gen_loss =3.0218186\n",
      "disc_loss =0.35311857\n",
      "Time for epoch 38 is 1.275195837020874 sec\n",
      "gen_loss =3.3211164\n",
      "disc_loss =0.39584213\n",
      "Time for epoch 39 is 1.2190659046173096 sec\n",
      "gen_loss =2.9634252\n",
      "disc_loss =0.39709228\n",
      "Time for epoch 40 is 1.2184381484985352 sec\n"
     ]
    }
   ],
   "source": [
    "final = train(train_dataset,40, gen_losses, disc_losses, gloss, dloss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method _EagerTensorBase.eval of <tf.Tensor: shape=(50, 147), dtype=float32, numpy=\n",
       "array([[0.41432533, 0.65544325, 0.28528917, ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.3588624 , 0.42854884, 0.41608822, ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.53015906, 0.08060289, 0.5188161 , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       ...,\n",
       "       [0.12347867, 0.        , 0.41887438, ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.29593462, 0.34047148, 0.56619775, ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.27839258, 0.32688344, 0.4162595 , ..., 0.        , 0.        ,\n",
       "        0.        ]], dtype=float32)>>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final.eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_images = generator(tf.random.normal([1000, noise_dim]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1000, 147), dtype=float32, numpy=\n",
       "array([[0.41432533, 0.65544325, 0.28528917, ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.3588624 , 0.42854884, 0.41608822, ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.53015906, 0.08060289, 0.5188161 , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       ...,\n",
       "       [0.6150852 , 0.5877808 , 0.5859526 , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.5114996 , 0.55857164, 0.22718066, ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.03506737, 0.2801032 , ..., 0.        , 0.        ,\n",
       "        0.        ]], dtype=float32)>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "col = np.load('gitCol.npy', allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(147,)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "col.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated = pd.DataFrame(generated_images, columns=col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "save = generated.to_pickle(\"gen_data3.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.figure(figsize=(10,8))\n",
    "plt.plot(list(range(0,EPOCHS)),gloss)\n",
    "plt.plot(list(range(0,EPOCHS)),dloss)\n",
    "plt.title('Model Loss over Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['generator loss','discriminator loss'])\n",
    "plt.savefig('Learning_curve gitdnn')\n",
    "plt.show\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "df5fa5efdf1c60a896ccc8bc52bcd2fc69320846d08f7bc80e2522b0c75b6345"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
