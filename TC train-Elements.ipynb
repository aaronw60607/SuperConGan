{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.layers import Input, Dense, BatchNormalization\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "import h5py\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from jarvis.db.figshare import data\n",
    "import csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(21263, 170)\n"
     ]
    }
   ],
   "source": [
    "df1 = pd.read_csv('data/train.csv')\n",
    "other = pd.read_csv('data/unique_m.csv')\n",
    "df = pd.concat([df1,other],axis=1)\n",
    "original_columns = len(df.columns)\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.T.drop_duplicates().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop('material', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number_of_elements\n",
      "mean_atomic_mass\n",
      "wtd_mean_atomic_mass\n",
      "gmean_atomic_mass\n",
      "wtd_gmean_atomic_mass\n",
      "entropy_atomic_mass\n",
      "wtd_entropy_atomic_mass\n",
      "range_atomic_mass\n",
      "wtd_range_atomic_mass\n",
      "std_atomic_mass\n",
      "wtd_std_atomic_mass\n",
      "mean_fie\n",
      "wtd_mean_fie\n",
      "gmean_fie\n",
      "wtd_gmean_fie\n",
      "entropy_fie\n",
      "wtd_entropy_fie\n",
      "range_fie\n",
      "wtd_range_fie\n",
      "std_fie\n",
      "wtd_std_fie\n",
      "mean_atomic_radius\n",
      "wtd_mean_atomic_radius\n",
      "gmean_atomic_radius\n",
      "wtd_gmean_atomic_radius\n",
      "entropy_atomic_radius\n",
      "wtd_entropy_atomic_radius\n",
      "range_atomic_radius\n",
      "wtd_range_atomic_radius\n",
      "std_atomic_radius\n",
      "wtd_std_atomic_radius\n",
      "mean_Density\n",
      "wtd_mean_Density\n",
      "gmean_Density\n",
      "wtd_gmean_Density\n",
      "entropy_Density\n",
      "wtd_entropy_Density\n",
      "range_Density\n",
      "wtd_range_Density\n",
      "std_Density\n",
      "wtd_std_Density\n",
      "mean_ElectronAffinity\n",
      "wtd_mean_ElectronAffinity\n",
      "gmean_ElectronAffinity\n",
      "wtd_gmean_ElectronAffinity\n",
      "entropy_ElectronAffinity\n",
      "wtd_entropy_ElectronAffinity\n",
      "range_ElectronAffinity\n",
      "wtd_range_ElectronAffinity\n",
      "std_ElectronAffinity\n",
      "wtd_std_ElectronAffinity\n",
      "mean_FusionHeat\n",
      "wtd_mean_FusionHeat\n",
      "gmean_FusionHeat\n",
      "wtd_gmean_FusionHeat\n",
      "entropy_FusionHeat\n",
      "wtd_entropy_FusionHeat\n",
      "range_FusionHeat\n",
      "wtd_range_FusionHeat\n",
      "std_FusionHeat\n",
      "wtd_std_FusionHeat\n",
      "mean_ThermalConductivity\n",
      "wtd_mean_ThermalConductivity\n",
      "gmean_ThermalConductivity\n",
      "wtd_gmean_ThermalConductivity\n",
      "entropy_ThermalConductivity\n",
      "wtd_entropy_ThermalConductivity\n",
      "range_ThermalConductivity\n",
      "wtd_range_ThermalConductivity\n",
      "std_ThermalConductivity\n",
      "wtd_std_ThermalConductivity\n",
      "mean_Valence\n",
      "wtd_mean_Valence\n",
      "gmean_Valence\n",
      "wtd_gmean_Valence\n",
      "entropy_Valence\n",
      "wtd_entropy_Valence\n",
      "range_Valence\n",
      "wtd_range_Valence\n",
      "std_Valence\n",
      "wtd_std_Valence\n",
      "critical_temp\n",
      "H\n",
      "He\n",
      "Li\n",
      "Be\n",
      "B\n",
      "C\n",
      "N\n",
      "O\n",
      "F\n",
      "Na\n",
      "Mg\n",
      "Al\n",
      "Si\n",
      "P\n",
      "S\n",
      "Cl\n",
      "K\n",
      "Ca\n",
      "Sc\n",
      "Ti\n",
      "V\n",
      "Cr\n",
      "Mn\n",
      "Fe\n",
      "Co\n",
      "Ni\n",
      "Cu\n",
      "Zn\n",
      "Ga\n",
      "Ge\n",
      "As\n",
      "Se\n",
      "Br\n",
      "Rb\n",
      "Sr\n",
      "Y\n",
      "Zr\n",
      "Nb\n",
      "Mo\n",
      "Tc\n",
      "Ru\n",
      "Rh\n",
      "Pd\n",
      "Ag\n",
      "Cd\n",
      "In\n",
      "Sn\n",
      "Sb\n",
      "Te\n",
      "I\n",
      "Cs\n",
      "Ba\n",
      "La\n",
      "Ce\n",
      "Pr\n",
      "Nd\n",
      "Sm\n",
      "Eu\n",
      "Gd\n",
      "Tb\n",
      "Dy\n",
      "Ho\n",
      "Er\n",
      "Tm\n",
      "Yb\n",
      "Lu\n",
      "Hf\n",
      "Ta\n",
      "W\n",
      "Re\n",
      "Os\n",
      "Ir\n",
      "Pt\n",
      "Au\n",
      "Hg\n",
      "Tl\n",
      "Pb\n",
      "Bi\n"
     ]
    }
   ],
   "source": [
    "for x in df.columns:\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(df.columns[[0,1,2,3,4,5,6,7,8,9,10,11]], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "tc = df['critical_temp']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>H</th>\n",
       "      <th>He</th>\n",
       "      <th>Li</th>\n",
       "      <th>Be</th>\n",
       "      <th>B</th>\n",
       "      <th>C</th>\n",
       "      <th>N</th>\n",
       "      <th>O</th>\n",
       "      <th>F</th>\n",
       "      <th>Na</th>\n",
       "      <th>...</th>\n",
       "      <th>W</th>\n",
       "      <th>Re</th>\n",
       "      <th>Os</th>\n",
       "      <th>Ir</th>\n",
       "      <th>Pt</th>\n",
       "      <th>Au</th>\n",
       "      <th>Hg</th>\n",
       "      <th>Tl</th>\n",
       "      <th>Pb</th>\n",
       "      <th>Bi</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21258</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21259</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21260</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21261</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21262</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>21263 rows × 78 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         H He   Li   Be    B    C    N     O    F   Na  ...    W   Re   Os  \\\n",
       "0      0.0  0  0.0  0.0  0.0  0.0  0.0   4.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
       "1      0.0  0  0.0  0.0  0.0  0.0  0.0   4.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
       "2      0.0  0  0.0  0.0  0.0  0.0  0.0   4.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
       "3      0.0  0  0.0  0.0  0.0  0.0  0.0   4.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
       "4      0.0  0  0.0  0.0  0.0  0.0  0.0   4.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
       "...    ... ..  ...  ...  ...  ...  ...   ...  ...  ...  ...  ...  ...  ...   \n",
       "21258  0.0  0  0.0  0.0  0.0  0.0  0.0   0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
       "21259  0.0  0  0.0  0.0  0.0  0.0  0.0  11.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
       "21260  0.0  0  0.0  0.0  0.0  0.0  0.0   0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
       "21261  0.0  0  0.0  0.0  0.0  0.0  0.0   0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
       "21262  0.0  0  0.0  0.0  0.0  0.0  0.0   0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
       "\n",
       "        Ir   Pt   Au   Hg   Tl   Pb   Bi  \n",
       "0      0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "1      0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "2      0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "3      0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "4      0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "...    ...  ...  ...  ...  ...  ...  ...  \n",
       "21258  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "21259  0.0  0.0  0.0  0.0  1.0  0.0  0.0  \n",
       "21260  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "21261  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "21262  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "\n",
       "[21263 rows x 78 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.loc[:,'H':'Bi']\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = df[df.critical_temp >25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = df[df.critical_temp != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'critical_temp'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/envs/tensorflow/lib/python3.10/site-packages/pandas/core/indexes/base.py:3802\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3801\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3802\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3803\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32m~/miniconda3/envs/tensorflow/lib/python3.10/site-packages/pandas/_libs/index.pyx:138\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/tensorflow/lib/python3.10/site-packages/pandas/_libs/index.pyx:165\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:5745\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:5753\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'critical_temp'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcritical_temp\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/tensorflow/lib/python3.10/site-packages/pandas/core/frame.py:3807\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3805\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   3806\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 3807\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3808\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   3809\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m~/miniconda3/envs/tensorflow/lib/python3.10/site-packages/pandas/core/indexes/base.py:3804\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3802\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[1;32m   3803\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m-> 3804\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3805\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3806\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3807\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3808\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3809\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'critical_temp'"
     ]
    }
   ],
   "source": [
    "df['critical_temp']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df, tc, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17010, 78)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 78)]              0         \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 78)               312       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " fc1_relu (Dense)            (None, 256)               20224     \n",
      "                                                                 \n",
      " fc2 (Dense)                 (None, 128)               32896     \n",
      "                                                                 \n",
      " fc3 (Dense)                 (None, 64)                8256      \n",
      "                                                                 \n",
      " fc4 (Dense)                 (None, 32)                2080      \n",
      "                                                                 \n",
      " relu (Dense)                (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 63,801\n",
      "Trainable params: 63,645\n",
      "Non-trainable params: 156\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "Inputs = Input(shape=(78))\n",
    "x = BatchNormalization()(Inputs)\n",
    "\n",
    "x = Dense(256, activation='relu', kernel_initializer='lecun_uniform', name='fc1_relu')(x)\n",
    "x = Dense(128, activation='relu', kernel_initializer='lecun_uniform', name = 'fc2')(x)\n",
    "\n",
    "x = Dense(64, activation='relu', kernel_initializer='lecun_uniform', name = 'fc3')(x)\n",
    "x = Dense(32, activation='relu', kernel_initializer='lecun_uniform', name='fc4')(x)\n",
    "predictions = Dense(1, activation='relu', kernel_initializer='lecun_uniform', name = 'relu')(x)\n",
    "model = Model(inputs=Inputs, outputs=predictions)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "adam = Adam(lr=0.0001)\n",
    "model.compile(optimizer=adam, loss='MSE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-24 02:05:53.971984: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13/13 [==============================] - 2s 47ms/step - loss: 2363.6416 - val_loss: 2380.6477\n",
      "Epoch 2/1000\n",
      " 8/13 [=================>............] - ETA: 0s - loss: 2354.1389"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-24 02:05:56.154005: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13/13 [==============================] - 0s 10ms/step - loss: 2348.2415 - val_loss: 2363.9602\n",
      "Epoch 3/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 2326.2764 - val_loss: 2348.8745\n",
      "Epoch 4/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 2300.3752 - val_loss: 2330.5723\n",
      "Epoch 5/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 2267.5706 - val_loss: 2306.1934\n",
      "Epoch 6/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 2224.1230 - val_loss: 2273.5325\n",
      "Epoch 7/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 2167.0034 - val_loss: 2229.0059\n",
      "Epoch 8/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 2090.9949 - val_loss: 2168.1150\n",
      "Epoch 9/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 1992.3171 - val_loss: 2088.5491\n",
      "Epoch 10/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 1871.9310 - val_loss: 1986.7924\n",
      "Epoch 11/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 1732.7423 - val_loss: 1862.1119\n",
      "Epoch 12/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 1578.2430 - val_loss: 1716.4601\n",
      "Epoch 13/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 1424.4290 - val_loss: 1560.9330\n",
      "Epoch 14/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 1269.5642 - val_loss: 1402.9927\n",
      "Epoch 15/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 1137.2953 - val_loss: 1254.1324\n",
      "Epoch 16/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 1010.8094 - val_loss: 1119.9150\n",
      "Epoch 17/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 901.2162 - val_loss: 1007.1285\n",
      "Epoch 18/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 810.0402 - val_loss: 911.4738\n",
      "Epoch 19/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 736.3123 - val_loss: 828.7057\n",
      "Epoch 20/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 660.2330 - val_loss: 761.6738\n",
      "Epoch 21/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 607.0797 - val_loss: 704.5463\n",
      "Epoch 22/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 561.6948 - val_loss: 659.3607\n",
      "Epoch 23/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 533.8959 - val_loss: 625.4398\n",
      "Epoch 24/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 501.5221 - val_loss: 597.1527\n",
      "Epoch 25/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 486.9179 - val_loss: 580.4234\n",
      "Epoch 26/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 472.9581 - val_loss: 562.8301\n",
      "Epoch 27/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 463.2359 - val_loss: 546.9243\n",
      "Epoch 28/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 451.1348 - val_loss: 533.8098\n",
      "Epoch 29/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 445.3860 - val_loss: 523.2337\n",
      "Epoch 30/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 434.8800 - val_loss: 513.8495\n",
      "Epoch 31/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 426.4939 - val_loss: 497.7432\n",
      "Epoch 32/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 414.5827 - val_loss: 490.4061\n",
      "Epoch 33/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 414.2083 - val_loss: 479.7854\n",
      "Epoch 34/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 404.2556 - val_loss: 468.0747\n",
      "Epoch 35/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 395.8123 - val_loss: 461.5279\n",
      "Epoch 36/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 388.2480 - val_loss: 453.3720\n",
      "Epoch 37/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 387.3068 - val_loss: 439.6715\n",
      "Epoch 38/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 376.7551 - val_loss: 434.7829\n",
      "Epoch 39/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 367.3875 - val_loss: 423.4485\n",
      "Epoch 40/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 362.1358 - val_loss: 414.7812\n",
      "Epoch 41/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 356.4759 - val_loss: 410.1340\n",
      "Epoch 42/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 351.6526 - val_loss: 398.7452\n",
      "Epoch 43/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 351.2782 - val_loss: 394.6389\n",
      "Epoch 44/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 341.6208 - val_loss: 382.3090\n",
      "Epoch 45/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 337.7976 - val_loss: 375.9145\n",
      "Epoch 46/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 331.0274 - val_loss: 369.9237\n",
      "Epoch 47/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 325.4722 - val_loss: 360.3877\n",
      "Epoch 48/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 324.5609 - val_loss: 358.7887\n",
      "Epoch 49/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 315.8481 - val_loss: 346.1551\n",
      "Epoch 50/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 311.1332 - val_loss: 350.8202\n",
      "Epoch 51/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 309.3435 - val_loss: 335.7225\n",
      "Epoch 52/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 305.5135 - val_loss: 337.4890\n",
      "Epoch 53/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 302.1797 - val_loss: 329.9523\n",
      "Epoch 54/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 295.0577 - val_loss: 328.6331\n",
      "Epoch 55/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 295.8246 - val_loss: 321.3506\n",
      "Epoch 56/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 285.5245 - val_loss: 318.2184\n",
      "Epoch 57/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 286.8040 - val_loss: 315.8628\n",
      "Epoch 58/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 279.4245 - val_loss: 304.2059\n",
      "Epoch 59/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 280.2234 - val_loss: 313.6767\n",
      "Epoch 60/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 274.4937 - val_loss: 298.1553\n",
      "Epoch 61/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 274.3777 - val_loss: 302.5981\n",
      "Epoch 62/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 269.9023 - val_loss: 294.6387\n",
      "Epoch 63/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 267.0372 - val_loss: 293.4844\n",
      "Epoch 64/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 267.2443 - val_loss: 294.9808\n",
      "Epoch 65/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 265.6142 - val_loss: 286.7523\n",
      "Epoch 66/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 264.8085 - val_loss: 288.3284\n",
      "Epoch 67/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 258.5206 - val_loss: 281.2857\n",
      "Epoch 68/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 257.6121 - val_loss: 284.8839\n",
      "Epoch 69/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 257.0281 - val_loss: 281.6084\n",
      "Epoch 70/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 254.9801 - val_loss: 281.9146\n",
      "Epoch 71/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 258.0699 - val_loss: 277.5752\n",
      "Epoch 72/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 252.6338 - val_loss: 275.8945\n",
      "Epoch 73/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 250.4778 - val_loss: 271.2483\n",
      "Epoch 74/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 249.8016 - val_loss: 276.0279\n",
      "Epoch 75/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 247.1370 - val_loss: 271.4709\n",
      "Epoch 76/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 249.3624 - val_loss: 276.7520\n",
      "Epoch 77/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 246.8638 - val_loss: 266.2336\n",
      "Epoch 78/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 248.9339 - val_loss: 268.7413\n",
      "Epoch 79/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 243.6874 - val_loss: 265.6543\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 80/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 240.7343 - val_loss: 270.6443\n",
      "Epoch 81/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 240.9202 - val_loss: 267.9993\n",
      "Epoch 82/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 242.0274 - val_loss: 261.7976\n",
      "Epoch 83/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 239.3710 - val_loss: 266.6173\n",
      "Epoch 84/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 239.3892 - val_loss: 261.2038\n",
      "Epoch 85/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 240.9118 - val_loss: 266.3891\n",
      "Epoch 86/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 239.6429 - val_loss: 256.5780\n",
      "Epoch 87/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 235.1493 - val_loss: 266.9340\n",
      "Epoch 88/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 235.5667 - val_loss: 258.6528\n",
      "Epoch 89/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 231.9471 - val_loss: 259.3885\n",
      "Epoch 90/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 235.7368 - val_loss: 255.5093\n",
      "Epoch 91/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 233.0939 - val_loss: 260.4294\n",
      "Epoch 92/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 231.8991 - val_loss: 264.4681\n",
      "Epoch 93/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 232.2832 - val_loss: 251.0039\n",
      "Epoch 94/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 229.4721 - val_loss: 257.1181\n",
      "Epoch 95/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 229.6337 - val_loss: 255.3851\n",
      "Epoch 96/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 230.8015 - val_loss: 256.3116\n",
      "Epoch 97/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 229.5371 - val_loss: 249.4675\n",
      "Epoch 98/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 226.7226 - val_loss: 256.5513\n",
      "Epoch 99/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 228.3781 - val_loss: 250.7108\n",
      "Epoch 100/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 225.9967 - val_loss: 259.3863\n",
      "Epoch 101/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 224.5696 - val_loss: 249.0578\n",
      "Epoch 102/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 223.3735 - val_loss: 252.6533\n",
      "Epoch 103/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 222.2884 - val_loss: 250.6399\n",
      "Epoch 104/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 226.5841 - val_loss: 252.1795\n",
      "Epoch 105/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 221.7139 - val_loss: 245.0216\n",
      "Epoch 106/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 224.0278 - val_loss: 245.5466\n",
      "Epoch 107/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 220.7492 - val_loss: 256.8316\n",
      "Epoch 108/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 225.6532 - val_loss: 247.0984\n",
      "Epoch 109/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 218.8522 - val_loss: 251.2233\n",
      "Epoch 110/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 221.0912 - val_loss: 245.5488\n",
      "Epoch 111/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 217.8918 - val_loss: 240.0536\n",
      "Epoch 112/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 219.8829 - val_loss: 242.7771\n",
      "Epoch 113/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 217.5545 - val_loss: 243.2072\n",
      "Epoch 114/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 217.3224 - val_loss: 244.3983\n",
      "Epoch 115/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 218.3157 - val_loss: 240.6165\n",
      "Epoch 116/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 216.7757 - val_loss: 247.0504\n",
      "Epoch 117/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 213.0573 - val_loss: 240.6246\n",
      "Epoch 118/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 214.4767 - val_loss: 241.1436\n",
      "Epoch 119/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 212.4448 - val_loss: 238.5284\n",
      "Epoch 120/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 216.2833 - val_loss: 236.1033\n",
      "Epoch 121/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 216.3872 - val_loss: 248.1482\n",
      "Epoch 122/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 216.1563 - val_loss: 241.0718\n",
      "Epoch 123/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 211.0296 - val_loss: 237.7463\n",
      "Epoch 124/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 208.4614 - val_loss: 241.5408\n",
      "Epoch 125/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 210.5430 - val_loss: 241.6538\n",
      "Epoch 126/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 212.5236 - val_loss: 239.7577\n",
      "Epoch 127/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 213.9212 - val_loss: 234.0906\n",
      "Epoch 128/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 209.3924 - val_loss: 238.8497\n",
      "Epoch 129/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 210.0985 - val_loss: 240.6319\n",
      "Epoch 130/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 212.0682 - val_loss: 239.6958\n",
      "Epoch 131/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 210.3521 - val_loss: 232.9836\n",
      "Epoch 132/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 208.2144 - val_loss: 240.1002\n",
      "Epoch 133/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 209.0529 - val_loss: 234.0772\n",
      "Epoch 134/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 210.4107 - val_loss: 238.0405\n",
      "Epoch 135/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 204.3797 - val_loss: 236.0280\n",
      "Epoch 136/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 210.0562 - val_loss: 234.7270\n",
      "Epoch 137/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 211.0698 - val_loss: 240.1292\n",
      "Epoch 138/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 210.4850 - val_loss: 239.1559\n",
      "Epoch 139/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 210.8769 - val_loss: 230.5805\n",
      "Epoch 140/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 206.6871 - val_loss: 239.5572\n",
      "Epoch 141/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 205.6891 - val_loss: 232.0083\n",
      "Epoch 142/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 203.9456 - val_loss: 231.6058\n",
      "Epoch 143/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 203.9820 - val_loss: 228.9656\n",
      "Epoch 144/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 204.1675 - val_loss: 237.5263\n",
      "Epoch 145/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 205.8728 - val_loss: 236.1939\n",
      "Epoch 146/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 203.9379 - val_loss: 227.3141\n",
      "Epoch 147/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 205.7219 - val_loss: 229.2131\n",
      "Epoch 148/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 203.3258 - val_loss: 236.1606\n",
      "Epoch 149/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 202.5154 - val_loss: 226.4933\n",
      "Epoch 150/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 202.9198 - val_loss: 243.5874\n",
      "Epoch 151/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 204.0062 - val_loss: 226.7999\n",
      "Epoch 152/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 205.8396 - val_loss: 234.7585\n",
      "Epoch 153/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 200.1214 - val_loss: 225.7511\n",
      "Epoch 154/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 202.3814 - val_loss: 229.7172\n",
      "Epoch 155/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 200.7955 - val_loss: 230.4909\n",
      "Epoch 156/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 201.9419 - val_loss: 227.4697\n",
      "Epoch 157/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13/13 [==============================] - 0s 8ms/step - loss: 197.8619 - val_loss: 238.1397\n",
      "Epoch 158/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 198.8904 - val_loss: 224.0365\n",
      "Epoch 159/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 197.7497 - val_loss: 228.2659\n",
      "Epoch 160/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 198.9286 - val_loss: 239.9172\n",
      "Epoch 161/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 203.4453 - val_loss: 225.1497\n",
      "Epoch 162/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 202.4147 - val_loss: 239.9515\n",
      "Epoch 163/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 199.1833 - val_loss: 224.8928\n",
      "Epoch 164/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 196.9191 - val_loss: 229.9717\n",
      "Epoch 165/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 202.0022 - val_loss: 223.2560\n",
      "Epoch 166/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 197.7436 - val_loss: 226.8404\n",
      "Epoch 167/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 198.6445 - val_loss: 232.7838\n",
      "Epoch 168/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 200.9621 - val_loss: 221.8233\n",
      "Epoch 169/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 198.4525 - val_loss: 242.4342\n",
      "Epoch 170/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 201.0688 - val_loss: 223.2961\n",
      "Epoch 171/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 199.9845 - val_loss: 228.1934\n",
      "Epoch 172/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 195.2774 - val_loss: 226.3051\n",
      "Epoch 173/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 195.7661 - val_loss: 230.8277\n",
      "Epoch 174/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 195.0483 - val_loss: 229.1375\n",
      "Epoch 175/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 198.0900 - val_loss: 223.3270\n",
      "Epoch 176/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 195.6568 - val_loss: 229.6676\n",
      "Epoch 177/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 195.5047 - val_loss: 221.8508\n",
      "Epoch 178/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 196.6330 - val_loss: 225.5659\n",
      "Epoch 179/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 199.0602 - val_loss: 235.9819\n",
      "Epoch 180/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 196.3586 - val_loss: 222.7800\n",
      "Epoch 181/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 194.1387 - val_loss: 227.1951\n",
      "Epoch 182/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 194.3613 - val_loss: 226.7755\n",
      "Epoch 183/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 197.1303 - val_loss: 222.9641\n",
      "Epoch 184/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 197.0691 - val_loss: 232.5311\n",
      "Epoch 185/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 196.2109 - val_loss: 226.5177\n",
      "Epoch 186/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 194.6311 - val_loss: 219.9720\n",
      "Epoch 187/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 195.3530 - val_loss: 235.3703\n",
      "Epoch 188/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 194.3147 - val_loss: 222.8885\n",
      "Epoch 189/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 193.2911 - val_loss: 217.7477\n",
      "Epoch 190/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 197.5011 - val_loss: 224.5769\n",
      "Epoch 191/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 192.4834 - val_loss: 219.1555\n",
      "Epoch 192/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 192.3634 - val_loss: 227.4675\n",
      "Epoch 193/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 190.6048 - val_loss: 222.6849\n",
      "Epoch 194/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 193.7305 - val_loss: 224.8286\n",
      "Epoch 195/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 191.7825 - val_loss: 231.1836\n",
      "Epoch 196/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 196.8523 - val_loss: 218.3891\n",
      "Epoch 197/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 194.5196 - val_loss: 225.2299\n",
      "Epoch 198/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 190.6807 - val_loss: 218.3793\n",
      "Epoch 199/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 197.0648 - val_loss: 220.7806\n",
      "Epoch 200/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 189.4487 - val_loss: 222.1414\n",
      "Epoch 201/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 190.2761 - val_loss: 216.2981\n",
      "Epoch 202/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 192.2680 - val_loss: 222.2205\n",
      "Epoch 203/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 191.4305 - val_loss: 223.4505\n",
      "Epoch 204/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 192.2604 - val_loss: 225.2588\n",
      "Epoch 205/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 188.2405 - val_loss: 219.2979\n",
      "Epoch 206/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 192.5237 - val_loss: 218.7329\n",
      "Epoch 207/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 194.5557 - val_loss: 221.1626\n",
      "Epoch 208/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 187.7706 - val_loss: 225.8953\n",
      "Epoch 209/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 189.1182 - val_loss: 216.8485\n",
      "Epoch 210/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 192.3733 - val_loss: 220.8692\n",
      "Epoch 211/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 191.5910 - val_loss: 220.8462\n",
      "Epoch 212/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 189.9967 - val_loss: 219.3449\n",
      "Epoch 213/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 186.5240 - val_loss: 218.7337\n",
      "Epoch 214/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 188.4340 - val_loss: 221.7605\n",
      "Epoch 215/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 189.5903 - val_loss: 216.6936\n",
      "Epoch 216/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 189.9849 - val_loss: 218.3590\n",
      "Epoch 217/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 188.5830 - val_loss: 215.7543\n",
      "Epoch 218/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 190.4121 - val_loss: 216.0335\n",
      "Epoch 219/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 188.9406 - val_loss: 218.4277\n",
      "Epoch 220/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 186.7237 - val_loss: 215.1043\n",
      "Epoch 221/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 187.6202 - val_loss: 219.1171\n",
      "Epoch 222/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 190.7910 - val_loss: 222.6267\n",
      "Epoch 223/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 189.2780 - val_loss: 218.3794\n",
      "Epoch 224/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 186.0435 - val_loss: 225.8719\n",
      "Epoch 225/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 190.4743 - val_loss: 217.6229\n",
      "Epoch 226/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 185.5469 - val_loss: 219.9106\n",
      "Epoch 227/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 186.0422 - val_loss: 215.8055\n",
      "Epoch 228/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 188.3325 - val_loss: 217.0035\n",
      "Epoch 229/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 189.2986 - val_loss: 215.2646\n",
      "Epoch 230/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 188.0429 - val_loss: 219.3790\n",
      "Epoch 231/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 186.4805 - val_loss: 221.7405\n",
      "Epoch 232/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 186.6701 - val_loss: 215.7933\n",
      "Epoch 233/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 183.0866 - val_loss: 219.3392\n",
      "Epoch 234/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13/13 [==============================] - 0s 8ms/step - loss: 187.2603 - val_loss: 211.2372\n",
      "Epoch 235/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 185.6311 - val_loss: 226.0954\n",
      "Epoch 236/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 189.6937 - val_loss: 213.3499\n",
      "Epoch 237/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 184.7480 - val_loss: 214.3395\n",
      "Epoch 238/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 186.2456 - val_loss: 223.0905\n",
      "Epoch 239/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 185.2881 - val_loss: 214.5334\n",
      "Epoch 240/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 184.6727 - val_loss: 215.3011\n",
      "Epoch 241/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 187.4641 - val_loss: 211.7006\n",
      "Epoch 242/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 186.7721 - val_loss: 219.4372\n",
      "Epoch 243/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 186.4126 - val_loss: 217.7490\n",
      "Epoch 244/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 183.2423 - val_loss: 210.3605\n",
      "Epoch 245/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 183.4836 - val_loss: 219.5213\n",
      "Epoch 246/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 185.3278 - val_loss: 218.0105\n",
      "Epoch 247/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 185.2847 - val_loss: 215.6384\n",
      "Epoch 248/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 181.4171 - val_loss: 220.1068\n",
      "Epoch 249/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 184.3776 - val_loss: 214.2793\n",
      "Epoch 250/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 186.2266 - val_loss: 211.5697\n",
      "Epoch 251/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 186.2833 - val_loss: 219.6190\n",
      "Epoch 252/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 181.6915 - val_loss: 212.7714\n",
      "Epoch 253/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 185.0546 - val_loss: 213.8175\n",
      "Epoch 254/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 186.0358 - val_loss: 210.4708\n",
      "Epoch 255/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 181.6585 - val_loss: 222.4098\n",
      "Epoch 256/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 185.5292 - val_loss: 208.5127\n",
      "Epoch 257/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 186.5565 - val_loss: 213.1317\n",
      "Epoch 258/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 178.1925 - val_loss: 218.4411\n",
      "Epoch 259/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 182.7050 - val_loss: 212.4356\n",
      "Epoch 260/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 183.7668 - val_loss: 211.2186\n",
      "Epoch 261/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 183.7034 - val_loss: 214.4926\n",
      "Epoch 262/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 183.9365 - val_loss: 216.6759\n",
      "Epoch 263/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 182.7138 - val_loss: 209.7612\n",
      "Epoch 264/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 182.0236 - val_loss: 212.0981\n",
      "Epoch 265/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 181.7911 - val_loss: 217.6230\n",
      "Epoch 266/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 185.0074 - val_loss: 211.1649\n",
      "Epoch 267/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 179.0444 - val_loss: 214.1932\n",
      "Epoch 268/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 184.3635 - val_loss: 225.3084\n",
      "Epoch 269/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 186.0400 - val_loss: 205.1207\n",
      "Epoch 270/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 185.7070 - val_loss: 217.9850\n",
      "Epoch 271/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 183.3095 - val_loss: 212.7758\n",
      "Epoch 272/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 180.8523 - val_loss: 214.0649\n",
      "Epoch 273/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 182.9018 - val_loss: 209.9838\n",
      "Epoch 274/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 181.0981 - val_loss: 212.2245\n",
      "Epoch 275/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 180.2590 - val_loss: 210.3615\n",
      "Epoch 276/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 179.7220 - val_loss: 211.0971\n",
      "Epoch 277/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 182.5307 - val_loss: 211.7153\n",
      "Epoch 278/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 179.5082 - val_loss: 211.7086\n",
      "Epoch 279/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 180.4426 - val_loss: 210.5882\n",
      "Epoch 280/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 186.4435 - val_loss: 205.9428\n",
      "Epoch 281/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 185.4911 - val_loss: 209.6721\n",
      "Epoch 282/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 177.2335 - val_loss: 216.5543\n",
      "Epoch 283/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 178.4243 - val_loss: 208.6428\n",
      "Epoch 284/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 178.8059 - val_loss: 219.0045\n",
      "Epoch 285/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 180.5926 - val_loss: 210.2458\n",
      "Epoch 286/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 184.1639 - val_loss: 217.4206\n",
      "Epoch 287/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 179.9887 - val_loss: 207.4626\n",
      "Epoch 288/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 180.3703 - val_loss: 210.8630\n",
      "Epoch 289/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 177.8406 - val_loss: 212.5395\n",
      "Epoch 290/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 183.7319 - val_loss: 208.2275\n",
      "Epoch 291/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 183.3836 - val_loss: 205.7882\n",
      "Epoch 292/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 181.8448 - val_loss: 213.1811\n",
      "Epoch 293/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 180.6268 - val_loss: 209.6430\n",
      "Epoch 294/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 179.7239 - val_loss: 204.7975\n",
      "Epoch 295/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 177.2557 - val_loss: 214.1087\n",
      "Epoch 296/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 179.2838 - val_loss: 209.0350\n",
      "Epoch 297/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 179.5168 - val_loss: 210.7329\n",
      "Epoch 298/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 177.4265 - val_loss: 214.8374\n",
      "Epoch 299/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 177.9557 - val_loss: 205.9314\n",
      "Epoch 300/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 177.9493 - val_loss: 210.9623\n",
      "Epoch 301/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 177.2865 - val_loss: 215.9603\n",
      "Epoch 302/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 179.0047 - val_loss: 207.6694\n",
      "Epoch 303/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 178.8442 - val_loss: 214.6243\n",
      "Epoch 304/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 177.7426 - val_loss: 209.8634\n",
      "Epoch 305/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 176.9089 - val_loss: 209.6840\n",
      "Epoch 306/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 178.5304 - val_loss: 206.7928\n",
      "Epoch 307/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 179.1308 - val_loss: 208.5498\n",
      "Epoch 308/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 176.5260 - val_loss: 213.8434\n",
      "Epoch 309/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 177.7924 - val_loss: 207.1567\n",
      "Epoch 310/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 178.2237 - val_loss: 210.7184\n",
      "Epoch 311/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13/13 [==============================] - 0s 8ms/step - loss: 179.2559 - val_loss: 210.4348\n",
      "Epoch 312/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 176.8637 - val_loss: 211.3084\n",
      "Epoch 313/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 176.4474 - val_loss: 207.1432\n",
      "Epoch 314/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 179.1937 - val_loss: 208.0473\n",
      "Epoch 315/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 175.2521 - val_loss: 205.8515\n",
      "Epoch 316/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 175.4912 - val_loss: 212.3445\n",
      "Epoch 317/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 174.0918 - val_loss: 201.9564\n",
      "Epoch 318/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 178.7907 - val_loss: 208.4560\n",
      "Epoch 319/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173.7865 - val_loss: 214.3190\n",
      "Epoch 320/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 181.4542 - val_loss: 204.4277\n",
      "Epoch 321/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 175.0369 - val_loss: 207.3095\n",
      "Epoch 322/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 180.0652 - val_loss: 220.0570\n",
      "Epoch 323/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 179.3409 - val_loss: 201.0489\n",
      "Epoch 324/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 177.2311 - val_loss: 207.6642\n",
      "Epoch 325/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173.4050 - val_loss: 211.4924\n",
      "Epoch 326/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173.3980 - val_loss: 204.2464\n",
      "Epoch 327/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 178.5907 - val_loss: 205.5217\n",
      "Epoch 328/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 175.1227 - val_loss: 212.5520\n",
      "Epoch 329/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 172.5409 - val_loss: 209.4612\n",
      "Epoch 330/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 175.5984 - val_loss: 208.0910\n",
      "Epoch 331/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 175.8116 - val_loss: 208.5195\n",
      "Epoch 332/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 177.2217 - val_loss: 202.4439\n",
      "Epoch 333/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 177.3530 - val_loss: 208.3616\n",
      "Epoch 334/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173.1991 - val_loss: 203.3632\n",
      "Epoch 335/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 175.1175 - val_loss: 208.6603\n",
      "Epoch 336/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 177.4952 - val_loss: 203.6332\n",
      "Epoch 337/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 176.0312 - val_loss: 205.9243\n",
      "Epoch 338/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 176.2962 - val_loss: 204.5634\n",
      "Epoch 339/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 175.2698 - val_loss: 206.7309\n",
      "Epoch 340/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 175.1545 - val_loss: 212.0719\n",
      "Epoch 341/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 174.7019 - val_loss: 205.8550\n",
      "Epoch 342/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 174.6808 - val_loss: 204.1546\n",
      "Epoch 343/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173.6097 - val_loss: 206.1672\n",
      "Epoch 344/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 174.6253 - val_loss: 203.5822\n",
      "Epoch 345/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 171.3193 - val_loss: 214.5650\n",
      "Epoch 346/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 172.5213 - val_loss: 208.7500\n",
      "Epoch 347/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 177.0043 - val_loss: 205.4169\n",
      "Epoch 348/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 176.4957 - val_loss: 220.9115\n",
      "Epoch 349/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 174.0924 - val_loss: 206.4729\n",
      "Epoch 350/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173.4475 - val_loss: 205.4393\n",
      "Epoch 351/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 177.6448 - val_loss: 203.0591\n",
      "Epoch 352/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 174.4224 - val_loss: 209.0733\n",
      "Epoch 353/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 175.8599 - val_loss: 208.6328\n",
      "Epoch 354/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 172.2107 - val_loss: 205.7476\n",
      "Epoch 355/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 171.6576 - val_loss: 208.2838\n",
      "Epoch 356/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 171.2129 - val_loss: 204.0106\n",
      "Epoch 357/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 175.4936 - val_loss: 201.6870\n",
      "Epoch 358/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 176.1125 - val_loss: 206.8257\n",
      "Epoch 359/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 173.1225 - val_loss: 211.2198\n",
      "Epoch 360/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 174.2320 - val_loss: 202.4391\n",
      "Epoch 361/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 175.9084 - val_loss: 200.6967\n",
      "Epoch 362/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 176.1584 - val_loss: 209.2534\n",
      "Epoch 363/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 171.9756 - val_loss: 206.3433\n",
      "Epoch 364/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 170.5613 - val_loss: 209.4969\n",
      "Epoch 365/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 174.4372 - val_loss: 212.4814\n",
      "Epoch 366/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 172.7347 - val_loss: 208.0290\n",
      "Epoch 367/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 172.5868 - val_loss: 199.6066\n",
      "Epoch 368/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173.6035 - val_loss: 215.1494\n",
      "Epoch 369/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 175.6376 - val_loss: 208.0134\n",
      "Epoch 370/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 177.2342 - val_loss: 201.3656\n",
      "Epoch 371/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 174.3915 - val_loss: 203.7867\n",
      "Epoch 372/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 172.5120 - val_loss: 206.8056\n",
      "Epoch 373/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 170.1843 - val_loss: 205.8465\n",
      "Epoch 374/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 175.0979 - val_loss: 200.2862\n",
      "Epoch 375/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 172.5467 - val_loss: 207.8909\n",
      "Epoch 376/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173.5153 - val_loss: 203.9279\n",
      "Epoch 377/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 172.1019 - val_loss: 203.6554\n",
      "Epoch 378/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 175.3246 - val_loss: 211.6140\n",
      "Epoch 379/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173.5563 - val_loss: 210.0069\n",
      "Epoch 380/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173.2741 - val_loss: 200.5135\n",
      "Epoch 381/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 175.2554 - val_loss: 204.4182\n",
      "Epoch 382/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 170.4093 - val_loss: 212.9870\n",
      "Epoch 383/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 170.3376 - val_loss: 208.7975\n",
      "Epoch 384/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 175.0995 - val_loss: 210.8834\n",
      "Epoch 385/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 172.5853 - val_loss: 206.7871\n",
      "Epoch 386/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 172.0226 - val_loss: 198.6063\n",
      "Epoch 387/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 174.6563 - val_loss: 207.3971\n",
      "Epoch 388/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13/13 [==============================] - 0s 8ms/step - loss: 172.6919 - val_loss: 205.2637\n",
      "Epoch 389/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 172.6861 - val_loss: 208.5245\n",
      "Epoch 390/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 174.1701 - val_loss: 200.4583\n",
      "Epoch 391/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 170.2663 - val_loss: 197.9653\n",
      "Epoch 392/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 172.0609 - val_loss: 215.7713\n",
      "Epoch 393/1000\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 177.1951 - val_loss: 206.0779\n",
      "Epoch 394/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173.4663 - val_loss: 197.9856\n",
      "Epoch 395/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 171.1724 - val_loss: 223.8023\n",
      "Epoch 396/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 174.8839 - val_loss: 205.9335\n",
      "Epoch 397/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 166.8666 - val_loss: 204.6370\n",
      "Epoch 398/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 169.3674 - val_loss: 203.0250\n",
      "Epoch 399/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 171.9986 - val_loss: 199.7684\n",
      "Epoch 400/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 169.7509 - val_loss: 203.4199\n",
      "Epoch 401/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 170.2556 - val_loss: 205.9839\n",
      "Epoch 402/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 170.8432 - val_loss: 207.6717\n",
      "Epoch 403/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 170.4383 - val_loss: 204.2379\n",
      "Epoch 404/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 169.5159 - val_loss: 207.9990\n",
      "Epoch 405/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 168.1620 - val_loss: 212.7706\n",
      "Epoch 406/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 174.0606 - val_loss: 197.7666\n",
      "Epoch 407/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 170.2530 - val_loss: 211.4098\n",
      "Epoch 408/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 170.9248 - val_loss: 209.3681\n",
      "Epoch 409/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 169.8867 - val_loss: 197.3709\n",
      "Epoch 410/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 172.0876 - val_loss: 202.1432\n",
      "Epoch 411/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 167.3018 - val_loss: 203.3396\n",
      "Epoch 412/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 170.2596 - val_loss: 202.4714\n",
      "Epoch 413/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 167.7571 - val_loss: 205.7457\n",
      "Epoch 414/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 171.9277 - val_loss: 204.7558\n",
      "Epoch 415/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 170.1788 - val_loss: 201.5256\n",
      "Epoch 416/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 166.5846 - val_loss: 207.0222\n",
      "Epoch 417/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 169.5455 - val_loss: 208.4835\n",
      "Epoch 418/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 168.2828 - val_loss: 200.6662\n",
      "Epoch 419/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 172.8543 - val_loss: 201.9667\n",
      "Epoch 420/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 167.7551 - val_loss: 210.3846\n",
      "Epoch 421/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 167.4809 - val_loss: 202.4747\n",
      "Epoch 422/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 171.0960 - val_loss: 199.9494\n",
      "Epoch 423/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 169.0694 - val_loss: 204.9868\n",
      "Epoch 424/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 168.7875 - val_loss: 205.8753\n",
      "Epoch 425/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 167.9835 - val_loss: 205.0518\n",
      "Epoch 426/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 170.6744 - val_loss: 200.5417\n",
      "Epoch 427/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 168.8125 - val_loss: 207.6427\n",
      "Epoch 428/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 168.4433 - val_loss: 199.1827\n",
      "Epoch 429/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 167.9939 - val_loss: 200.2072\n",
      "Epoch 430/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 166.7005 - val_loss: 218.3328\n",
      "Epoch 431/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 171.8084 - val_loss: 195.7879\n",
      "Epoch 432/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 171.8175 - val_loss: 201.6226\n",
      "Epoch 433/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 167.5457 - val_loss: 207.6634\n",
      "Epoch 434/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 164.0564 - val_loss: 202.9094\n",
      "Epoch 435/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 167.9570 - val_loss: 206.0270\n",
      "Epoch 436/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 166.2506 - val_loss: 203.6676\n",
      "Epoch 437/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 166.6776 - val_loss: 202.9907\n",
      "Epoch 438/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 167.6912 - val_loss: 200.0696\n",
      "Epoch 439/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 166.9346 - val_loss: 212.1629\n",
      "Epoch 440/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 169.1460 - val_loss: 202.4418\n",
      "Epoch 441/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 170.3979 - val_loss: 200.1284\n",
      "Epoch 442/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 169.6833 - val_loss: 204.6096\n",
      "Epoch 443/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 168.0075 - val_loss: 195.7134\n",
      "Epoch 444/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173.1928 - val_loss: 209.3156\n",
      "Epoch 445/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 173.2700 - val_loss: 218.6187\n",
      "Epoch 446/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 171.0420 - val_loss: 198.0667\n",
      "Epoch 447/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 168.9308 - val_loss: 199.1221\n",
      "Epoch 448/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 167.0602 - val_loss: 199.9329\n",
      "Epoch 449/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 168.8285 - val_loss: 199.3403\n",
      "Epoch 450/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 169.4292 - val_loss: 196.6977\n",
      "Epoch 451/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 164.3914 - val_loss: 207.8141\n",
      "Epoch 452/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 168.4203 - val_loss: 196.6946\n",
      "Epoch 453/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 165.4377 - val_loss: 207.1576\n",
      "Epoch 454/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 165.0566 - val_loss: 198.4571\n",
      "Epoch 455/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 164.8779 - val_loss: 199.9797\n",
      "Epoch 456/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 163.1421 - val_loss: 210.6355\n",
      "Epoch 457/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 165.8439 - val_loss: 196.6229\n",
      "Epoch 458/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 167.0207 - val_loss: 202.8862\n",
      "Epoch 459/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 166.9750 - val_loss: 207.9129\n",
      "Epoch 460/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 168.4257 - val_loss: 198.1484\n",
      "Epoch 461/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 163.8651 - val_loss: 199.6479\n",
      "Epoch 462/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 168.0178 - val_loss: 206.4013\n",
      "Epoch 463/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 169.1775 - val_loss: 194.8107\n",
      "Epoch 464/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 167.2524 - val_loss: 207.1597\n",
      "Epoch 465/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13/13 [==============================] - 0s 8ms/step - loss: 166.4048 - val_loss: 206.4119\n",
      "Epoch 466/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 167.7908 - val_loss: 198.8672\n",
      "Epoch 467/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 162.8386 - val_loss: 199.0652\n",
      "Epoch 468/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 164.9197 - val_loss: 200.7189\n",
      "Epoch 469/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 162.9758 - val_loss: 198.7027\n",
      "Epoch 470/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 171.3806 - val_loss: 198.4133\n",
      "Epoch 471/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 164.2539 - val_loss: 198.0297\n",
      "Epoch 472/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 166.2540 - val_loss: 202.7871\n",
      "Epoch 473/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 163.8803 - val_loss: 196.9921\n",
      "Epoch 474/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 164.1440 - val_loss: 199.9416\n",
      "Epoch 475/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 168.3160 - val_loss: 198.6298\n",
      "Epoch 476/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 166.5845 - val_loss: 202.6173\n",
      "Epoch 477/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 166.7863 - val_loss: 194.0353\n",
      "Epoch 478/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 164.1285 - val_loss: 213.4646\n",
      "Epoch 479/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 170.4755 - val_loss: 203.8920\n",
      "Epoch 480/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 168.3259 - val_loss: 196.1237\n",
      "Epoch 481/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 170.8970 - val_loss: 201.0092\n",
      "Epoch 482/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 164.4612 - val_loss: 200.4308\n",
      "Epoch 483/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 161.8363 - val_loss: 206.2501\n",
      "Epoch 484/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 165.1097 - val_loss: 196.9315\n",
      "Epoch 485/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 168.3388 - val_loss: 194.2260\n",
      "Epoch 486/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 166.3659 - val_loss: 202.4252\n",
      "Epoch 487/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 167.7898 - val_loss: 213.2730\n",
      "Epoch 488/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 169.4273 - val_loss: 193.1944\n",
      "Epoch 489/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 163.1681 - val_loss: 197.3658\n",
      "Epoch 490/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 164.3191 - val_loss: 206.3049\n",
      "Epoch 491/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 165.2291 - val_loss: 196.9112\n",
      "Epoch 492/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 164.6752 - val_loss: 202.5442\n",
      "Epoch 493/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 163.3798 - val_loss: 199.3150\n",
      "Epoch 494/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 164.3793 - val_loss: 206.7433\n",
      "Epoch 495/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 162.3851 - val_loss: 199.9555\n",
      "Epoch 496/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 164.0709 - val_loss: 199.2920\n",
      "Epoch 497/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 162.3065 - val_loss: 202.6803\n",
      "Epoch 498/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 163.5540 - val_loss: 204.7252\n",
      "Epoch 499/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 167.5744 - val_loss: 194.0231\n",
      "Epoch 500/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 162.6504 - val_loss: 194.5916\n",
      "Epoch 501/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 166.7890 - val_loss: 197.1106\n",
      "Epoch 502/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 163.6952 - val_loss: 200.9221\n",
      "Epoch 503/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 163.0380 - val_loss: 196.1672\n",
      "Epoch 504/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 161.4499 - val_loss: 202.1677\n",
      "Epoch 505/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 162.7252 - val_loss: 195.6501\n",
      "Epoch 506/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 164.4835 - val_loss: 200.4399\n",
      "Epoch 507/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 160.4394 - val_loss: 206.2344\n",
      "Epoch 508/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 161.3307 - val_loss: 200.4336\n",
      "Epoch 509/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 161.7275 - val_loss: 194.3571\n",
      "Epoch 510/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 164.3692 - val_loss: 196.0516\n",
      "Epoch 511/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 162.7023 - val_loss: 197.2727\n",
      "Epoch 512/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 161.6546 - val_loss: 203.2641\n",
      "Epoch 513/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 166.4019 - val_loss: 199.5334\n",
      "Epoch 514/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 165.1627 - val_loss: 194.4975\n",
      "Epoch 515/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 163.5205 - val_loss: 206.6854\n",
      "Epoch 516/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 162.2888 - val_loss: 196.6279\n",
      "Epoch 517/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 165.0121 - val_loss: 191.6208\n",
      "Epoch 518/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 164.7751 - val_loss: 207.9670\n",
      "Epoch 519/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 165.4258 - val_loss: 194.1893\n",
      "Epoch 520/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 165.8910 - val_loss: 193.5271\n",
      "Epoch 521/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 163.2309 - val_loss: 210.7266\n",
      "Epoch 522/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 165.6290 - val_loss: 193.4241\n",
      "Epoch 523/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 162.5465 - val_loss: 196.3218\n",
      "Epoch 524/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 163.0387 - val_loss: 194.9212\n",
      "Epoch 525/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 160.8564 - val_loss: 203.5735\n",
      "Epoch 526/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 160.7766 - val_loss: 198.7984\n",
      "Epoch 527/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 161.5693 - val_loss: 199.9484\n",
      "Epoch 528/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 161.1616 - val_loss: 199.5546\n",
      "Epoch 529/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 163.7833 - val_loss: 193.6787\n",
      "Epoch 530/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 163.1922 - val_loss: 201.7882\n",
      "Epoch 531/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 161.5117 - val_loss: 201.0829\n",
      "Epoch 532/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 158.9280 - val_loss: 196.8375\n",
      "Epoch 533/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 159.4831 - val_loss: 197.7960\n",
      "Epoch 534/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 160.4166 - val_loss: 201.9332\n",
      "Epoch 535/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 158.8658 - val_loss: 196.3644\n",
      "Epoch 536/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 160.2638 - val_loss: 197.7061\n",
      "Epoch 537/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 160.7171 - val_loss: 196.7835\n",
      "Epoch 538/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 160.4651 - val_loss: 199.9237\n",
      "Epoch 539/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 160.5547 - val_loss: 200.0730\n",
      "Epoch 540/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 160.6368 - val_loss: 201.5374\n",
      "Epoch 541/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 160.7168 - val_loss: 193.6037\n",
      "Epoch 542/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13/13 [==============================] - 0s 8ms/step - loss: 166.6552 - val_loss: 192.5607\n",
      "Epoch 543/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 163.4555 - val_loss: 195.2719\n",
      "Epoch 544/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 158.8960 - val_loss: 195.7366\n",
      "Epoch 545/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 162.9198 - val_loss: 196.1163\n",
      "Epoch 546/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 162.7034 - val_loss: 193.5350\n",
      "Epoch 547/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 161.4246 - val_loss: 197.6912\n",
      "Epoch 548/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 164.3421 - val_loss: 198.6755\n",
      "Epoch 549/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 165.1911 - val_loss: 201.5081\n",
      "Epoch 550/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 159.1685 - val_loss: 198.9993\n",
      "Epoch 551/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 160.8477 - val_loss: 194.9411\n",
      "Epoch 552/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 163.1629 - val_loss: 199.5872\n",
      "Epoch 553/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 161.3287 - val_loss: 192.6378\n",
      "Epoch 554/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 161.3042 - val_loss: 197.7284\n",
      "Epoch 555/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 165.9743 - val_loss: 206.5750\n",
      "Epoch 556/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 163.3975 - val_loss: 202.1561\n",
      "Epoch 557/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 162.1660 - val_loss: 195.6128\n",
      "Epoch 558/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 160.7903 - val_loss: 200.8913\n",
      "Epoch 559/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 162.1561 - val_loss: 191.5199\n",
      "Epoch 560/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 160.5325 - val_loss: 196.0875\n",
      "Epoch 561/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 162.2546 - val_loss: 194.8605\n",
      "Epoch 562/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 156.8008 - val_loss: 194.6280\n",
      "Epoch 563/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 160.9163 - val_loss: 199.4329\n",
      "Epoch 564/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 161.3076 - val_loss: 202.7605\n",
      "Epoch 565/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 162.4203 - val_loss: 192.8433\n",
      "Epoch 566/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 160.2560 - val_loss: 193.3776\n",
      "Epoch 567/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 160.0649 - val_loss: 204.5529\n",
      "Epoch 568/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 158.5157 - val_loss: 192.3857\n",
      "Epoch 569/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 160.3075 - val_loss: 194.6088\n",
      "Epoch 570/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 160.2507 - val_loss: 194.6408\n",
      "Epoch 571/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 158.5099 - val_loss: 197.8156\n",
      "Epoch 572/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 158.8675 - val_loss: 196.0327\n",
      "Epoch 573/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 161.8377 - val_loss: 192.2227\n",
      "Epoch 574/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 159.8742 - val_loss: 198.8847\n",
      "Epoch 575/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 159.0776 - val_loss: 192.0453\n",
      "Epoch 576/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 157.6370 - val_loss: 205.6904\n",
      "Epoch 577/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 157.8388 - val_loss: 194.2996\n",
      "Epoch 578/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 160.6861 - val_loss: 194.2109\n",
      "Epoch 579/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 160.8224 - val_loss: 196.7516\n",
      "Epoch 580/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 157.8092 - val_loss: 201.5350\n",
      "Epoch 581/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 158.7531 - val_loss: 197.5958\n",
      "Epoch 582/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 156.9195 - val_loss: 206.3028\n",
      "Epoch 583/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 159.5383 - val_loss: 196.2695\n",
      "Epoch 584/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 156.8836 - val_loss: 196.1430\n",
      "Epoch 585/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 160.1195 - val_loss: 201.5940\n",
      "Epoch 586/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 161.0664 - val_loss: 194.7936\n",
      "Epoch 587/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 157.7027 - val_loss: 196.8633\n",
      "Epoch 588/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 158.6882 - val_loss: 204.8287\n",
      "Epoch 589/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 161.2067 - val_loss: 191.6008\n",
      "Epoch 590/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 156.9533 - val_loss: 195.5645\n",
      "Epoch 591/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 157.3917 - val_loss: 198.5336\n",
      "Epoch 592/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 158.9217 - val_loss: 192.4070\n",
      "Epoch 593/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 155.8092 - val_loss: 201.1842\n",
      "Epoch 594/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 157.1376 - val_loss: 196.4666\n",
      "Epoch 595/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 157.7600 - val_loss: 191.4191\n",
      "Epoch 596/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 159.4729 - val_loss: 195.9440\n",
      "Epoch 597/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 154.8145 - val_loss: 192.4011\n",
      "Epoch 598/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 156.3954 - val_loss: 198.0741\n",
      "Epoch 599/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 159.2289 - val_loss: 190.1772\n",
      "Epoch 600/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 159.3771 - val_loss: 194.8235\n",
      "Epoch 601/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 157.8078 - val_loss: 200.8489\n",
      "Epoch 602/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 155.2830 - val_loss: 197.8639\n",
      "Epoch 603/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 157.2330 - val_loss: 190.3892\n",
      "Epoch 604/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 162.1601 - val_loss: 194.2176\n",
      "Epoch 605/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 157.4517 - val_loss: 195.6837\n",
      "Epoch 606/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 158.2711 - val_loss: 197.2806\n",
      "Epoch 607/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 154.0862 - val_loss: 201.2780\n",
      "Epoch 608/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 156.9002 - val_loss: 192.7569\n",
      "Epoch 609/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 156.2071 - val_loss: 196.8555\n",
      "Epoch 610/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 154.0771 - val_loss: 192.1038\n",
      "Epoch 611/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 158.8262 - val_loss: 190.7480\n",
      "Epoch 612/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 155.6271 - val_loss: 193.9498\n",
      "Epoch 613/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 155.6649 - val_loss: 204.5977\n",
      "Epoch 614/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 154.7800 - val_loss: 190.5892\n",
      "Epoch 615/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 155.5812 - val_loss: 198.2733\n",
      "Epoch 616/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 154.4574 - val_loss: 198.9386\n",
      "Epoch 617/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 155.1487 - val_loss: 196.5966\n",
      "Epoch 618/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 159.8288 - val_loss: 189.9534\n",
      "Epoch 619/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13/13 [==============================] - 0s 8ms/step - loss: 157.4162 - val_loss: 192.1152\n",
      "Epoch 620/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 156.9825 - val_loss: 198.1111\n",
      "Epoch 621/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 155.0519 - val_loss: 193.1172\n",
      "Epoch 622/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 154.8439 - val_loss: 193.3439\n",
      "Epoch 623/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 154.8381 - val_loss: 200.0992\n",
      "Epoch 624/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 156.9672 - val_loss: 194.5453\n",
      "Epoch 625/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 155.7089 - val_loss: 191.4333\n",
      "Epoch 626/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 154.0607 - val_loss: 197.6010\n",
      "Epoch 627/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 154.7221 - val_loss: 190.4984\n",
      "Epoch 628/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 156.5694 - val_loss: 190.9581\n",
      "Epoch 629/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 158.0426 - val_loss: 199.2194\n",
      "Epoch 630/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 156.3011 - val_loss: 203.5291\n",
      "Epoch 631/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 156.2443 - val_loss: 197.6108\n",
      "Epoch 632/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 152.3846 - val_loss: 190.4882\n",
      "Epoch 633/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 154.8476 - val_loss: 193.2595\n",
      "Epoch 634/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 154.6538 - val_loss: 199.9780\n",
      "Epoch 635/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 157.9165 - val_loss: 192.3273\n",
      "Epoch 636/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 152.9343 - val_loss: 197.8891\n",
      "Epoch 637/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 156.6585 - val_loss: 190.3058\n",
      "Epoch 638/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 152.8222 - val_loss: 187.5274\n",
      "Epoch 639/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 158.3756 - val_loss: 197.3382\n",
      "Epoch 640/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 155.2477 - val_loss: 201.1703\n",
      "Epoch 641/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 156.7356 - val_loss: 191.4452\n",
      "Epoch 642/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 153.6798 - val_loss: 190.9068\n",
      "Epoch 643/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 156.4890 - val_loss: 194.9946\n",
      "Epoch 644/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 153.1223 - val_loss: 201.8463\n",
      "Epoch 645/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 153.1042 - val_loss: 192.4228\n",
      "Epoch 646/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 152.5313 - val_loss: 198.3204\n",
      "Epoch 647/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 154.7256 - val_loss: 194.8141\n",
      "Epoch 648/1000\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 151.9081 - val_loss: 193.1177\n",
      "Epoch 649/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 152.2548 - val_loss: 194.0909\n",
      "Epoch 650/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 154.3893 - val_loss: 191.6977\n",
      "Epoch 651/1000\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 152.7575 - val_loss: 188.4565\n",
      "Epoch 652/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 153.8798 - val_loss: 191.8679\n",
      "Epoch 653/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 154.3583 - val_loss: 203.6614\n",
      "Epoch 654/1000\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 156.4677 - val_loss: 199.0929\n",
      "Epoch 655/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 152.2807 - val_loss: 192.6355\n",
      "Epoch 656/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 153.0620 - val_loss: 190.5191\n",
      "Epoch 657/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 154.8710 - val_loss: 194.4057\n",
      "Epoch 658/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 153.4947 - val_loss: 204.8866\n",
      "Epoch 659/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 156.9508 - val_loss: 193.7106\n",
      "Epoch 660/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 159.2220 - val_loss: 189.5484\n",
      "Epoch 661/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 153.2917 - val_loss: 189.0274\n",
      "Epoch 662/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 156.2500 - val_loss: 190.0551\n",
      "Epoch 663/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 157.0668 - val_loss: 192.0490\n",
      "Epoch 664/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 150.9501 - val_loss: 194.6979\n",
      "Epoch 665/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 155.6040 - val_loss: 193.7258\n",
      "Epoch 666/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 151.9144 - val_loss: 192.0662\n",
      "Epoch 667/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 150.6490 - val_loss: 198.2465\n",
      "Epoch 668/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 150.5209 - val_loss: 188.5734\n",
      "Epoch 669/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 153.2379 - val_loss: 190.8829\n",
      "Epoch 670/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 155.0301 - val_loss: 193.1412\n",
      "Epoch 671/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 155.0063 - val_loss: 192.9319\n",
      "Epoch 672/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 155.7923 - val_loss: 193.8550\n",
      "Epoch 673/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 153.7841 - val_loss: 207.4202\n",
      "Epoch 674/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 153.2657 - val_loss: 189.1271\n",
      "Epoch 675/1000\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 151.3330 - val_loss: 192.0913\n",
      "Epoch 676/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 152.8209 - val_loss: 192.7283\n",
      "Epoch 677/1000\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 154.0069 - val_loss: 189.5812\n",
      "Epoch 678/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 153.2743 - val_loss: 197.2692\n",
      "Epoch 679/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 149.2481 - val_loss: 195.4428\n",
      "Epoch 680/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 161.9491 - val_loss: 191.8713\n",
      "Epoch 681/1000\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 152.8147 - val_loss: 188.4421\n",
      "Epoch 682/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 157.0451 - val_loss: 189.6464\n",
      "Epoch 683/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 153.1046 - val_loss: 199.6718\n",
      "Epoch 684/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 149.6509 - val_loss: 190.2436\n",
      "Epoch 685/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 152.2625 - val_loss: 192.6005\n",
      "Epoch 686/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 152.1289 - val_loss: 189.7039\n",
      "Epoch 687/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 151.0976 - val_loss: 192.6769\n",
      "Epoch 688/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 150.4682 - val_loss: 193.3956\n",
      "Epoch 689/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 152.7521 - val_loss: 191.2221\n",
      "Epoch 690/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 150.5837 - val_loss: 195.8160\n",
      "Epoch 691/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 153.6318 - val_loss: 194.5492\n",
      "Epoch 692/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 151.5617 - val_loss: 191.5230\n",
      "Epoch 693/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 148.4722 - val_loss: 195.3765\n",
      "Epoch 694/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 151.3696 - val_loss: 189.7106\n",
      "Epoch 695/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 153.1522 - val_loss: 188.1315\n",
      "Epoch 696/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13/13 [==============================] - 0s 9ms/step - loss: 153.7719 - val_loss: 195.2397\n",
      "Epoch 697/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 149.6504 - val_loss: 196.9046\n",
      "Epoch 698/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 150.3544 - val_loss: 195.3272\n",
      "Epoch 699/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 153.0409 - val_loss: 189.9336\n",
      "Epoch 700/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 150.2617 - val_loss: 197.2868\n",
      "Epoch 701/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 153.5707 - val_loss: 189.6350\n",
      "Epoch 702/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 150.5538 - val_loss: 191.5629\n",
      "Epoch 703/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 154.3640 - val_loss: 191.1705\n",
      "Epoch 704/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 150.7784 - val_loss: 187.4695\n",
      "Epoch 705/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 152.7716 - val_loss: 195.5201\n",
      "Epoch 706/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 150.5423 - val_loss: 192.1453\n",
      "Epoch 707/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 151.7822 - val_loss: 189.6758\n",
      "Epoch 708/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 154.3746 - val_loss: 201.8479\n",
      "Epoch 709/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 152.0301 - val_loss: 189.9562\n",
      "Epoch 710/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 152.0701 - val_loss: 194.1134\n",
      "Epoch 711/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 148.8058 - val_loss: 193.6040\n",
      "Epoch 712/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 150.8972 - val_loss: 190.5779\n",
      "Epoch 713/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 152.5019 - val_loss: 190.7932\n",
      "Epoch 714/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 147.0043 - val_loss: 202.8539\n",
      "Epoch 715/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 154.4415 - val_loss: 191.5798\n",
      "Epoch 716/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 151.9669 - val_loss: 190.3047\n",
      "Epoch 717/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 150.0471 - val_loss: 193.8232\n",
      "Epoch 718/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 150.2298 - val_loss: 188.2797\n",
      "Epoch 719/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 152.9875 - val_loss: 191.5464\n",
      "Epoch 720/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 151.7951 - val_loss: 191.7736\n",
      "Epoch 721/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 149.4015 - val_loss: 191.6074\n",
      "Epoch 722/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 151.8005 - val_loss: 198.3668\n",
      "Epoch 723/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 151.0964 - val_loss: 192.8736\n",
      "Epoch 724/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 155.3158 - val_loss: 188.5847\n",
      "Epoch 725/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 149.5914 - val_loss: 187.6108\n",
      "Epoch 726/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 152.4785 - val_loss: 194.2950\n",
      "Epoch 727/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 150.5217 - val_loss: 197.3765\n",
      "Epoch 728/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 149.9844 - val_loss: 187.7632\n",
      "Epoch 729/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 150.8818 - val_loss: 195.8069\n",
      "Epoch 730/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 149.7666 - val_loss: 187.2321\n",
      "Epoch 731/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 148.8928 - val_loss: 187.4470\n",
      "Epoch 732/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 150.0409 - val_loss: 191.3318\n",
      "Epoch 733/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 151.1866 - val_loss: 188.7164\n",
      "Epoch 734/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 152.1531 - val_loss: 189.5833\n",
      "Epoch 735/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 144.7454 - val_loss: 193.0959\n",
      "Epoch 736/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 148.5524 - val_loss: 197.3568\n",
      "Epoch 737/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 148.0485 - val_loss: 191.4491\n",
      "Epoch 738/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 153.2844 - val_loss: 189.8815\n",
      "Epoch 739/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 148.8798 - val_loss: 193.2173\n",
      "Epoch 740/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 146.7518 - val_loss: 187.0422\n",
      "Epoch 741/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 148.9998 - val_loss: 197.6805\n",
      "Epoch 742/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 150.0625 - val_loss: 190.4047\n",
      "Epoch 743/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 149.5261 - val_loss: 188.6622\n",
      "Epoch 744/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 152.1618 - val_loss: 195.1546\n",
      "Epoch 745/1000\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 152.7166 - val_loss: 189.5473\n",
      "Epoch 746/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 152.1635 - val_loss: 196.4525\n",
      "Epoch 747/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 155.2976 - val_loss: 187.3784\n",
      "Epoch 748/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 150.5670 - val_loss: 189.1491\n",
      "Epoch 749/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 150.5944 - val_loss: 189.8942\n",
      "Epoch 750/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 150.3242 - val_loss: 192.5215\n",
      "Epoch 751/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 147.8708 - val_loss: 191.9606\n",
      "Epoch 752/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 148.5403 - val_loss: 191.3027\n",
      "Epoch 753/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 147.8358 - val_loss: 189.1115\n",
      "Epoch 754/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 146.6684 - val_loss: 192.6044\n",
      "Epoch 755/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 147.2317 - val_loss: 202.1453\n",
      "Epoch 756/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 150.5713 - val_loss: 185.5901\n",
      "Epoch 757/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 151.0271 - val_loss: 187.0083\n",
      "Epoch 758/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 149.1913 - val_loss: 189.0396\n",
      "Epoch 759/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 152.2786 - val_loss: 198.0541\n",
      "Epoch 760/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 148.0506 - val_loss: 191.7720\n",
      "Epoch 761/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 153.5377 - val_loss: 186.3310\n",
      "Epoch 762/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 148.4591 - val_loss: 194.8100\n",
      "Epoch 763/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 148.5834 - val_loss: 186.1352\n",
      "Epoch 764/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 149.3390 - val_loss: 191.3871\n",
      "Epoch 765/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 146.9964 - val_loss: 186.9081\n",
      "Epoch 766/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 146.8111 - val_loss: 198.3949\n",
      "Epoch 767/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 148.2771 - val_loss: 192.6900\n",
      "Epoch 768/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 149.8393 - val_loss: 190.4456\n",
      "Epoch 769/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 149.6193 - val_loss: 185.8775\n",
      "Epoch 770/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 148.0290 - val_loss: 199.2851\n",
      "Epoch 771/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 150.3009 - val_loss: 191.6874\n",
      "Epoch 772/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 148.9269 - val_loss: 189.7422\n",
      "Epoch 773/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13/13 [==============================] - 0s 8ms/step - loss: 149.8889 - val_loss: 187.5145\n",
      "Epoch 774/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 148.8086 - val_loss: 188.5750\n",
      "Epoch 775/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 145.7806 - val_loss: 187.1834\n",
      "Epoch 776/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 149.8514 - val_loss: 187.1780\n",
      "Epoch 777/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 148.3915 - val_loss: 188.8216\n",
      "Epoch 778/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 149.0239 - val_loss: 190.1469\n",
      "Epoch 779/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 150.9518 - val_loss: 203.9802\n",
      "Epoch 780/1000\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 150.2500 - val_loss: 195.1557\n",
      "Epoch 781/1000\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 149.5891 - val_loss: 186.3367\n",
      "Epoch 782/1000\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 150.2921 - val_loss: 187.3257\n",
      "Epoch 783/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 147.2237 - val_loss: 192.3335\n",
      "Epoch 784/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 148.3170 - val_loss: 186.3490\n",
      "Epoch 785/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 146.8020 - val_loss: 190.2541\n",
      "Epoch 786/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 145.6273 - val_loss: 187.4093\n",
      "Epoch 787/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 145.6186 - val_loss: 193.5057\n",
      "Epoch 788/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 148.9118 - val_loss: 198.6974\n",
      "Epoch 789/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 148.8070 - val_loss: 188.8546\n",
      "Epoch 790/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 146.9832 - val_loss: 188.8875\n",
      "Epoch 791/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 147.2608 - val_loss: 197.9493\n",
      "Epoch 792/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 145.8371 - val_loss: 189.7427\n",
      "Epoch 793/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 151.6591 - val_loss: 186.2930\n",
      "Epoch 794/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 146.9371 - val_loss: 186.3453\n",
      "Epoch 795/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 147.4622 - val_loss: 198.8447\n",
      "Epoch 796/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 145.6683 - val_loss: 189.3734\n",
      "Epoch 797/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 146.2576 - val_loss: 189.6697\n",
      "Epoch 798/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 149.7595 - val_loss: 185.9406\n",
      "Epoch 799/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 149.4652 - val_loss: 197.2688\n",
      "Epoch 800/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 149.1602 - val_loss: 191.7386\n",
      "Epoch 801/1000\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 145.4627 - val_loss: 190.9175\n",
      "Epoch 802/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 147.8813 - val_loss: 193.1161\n",
      "Epoch 803/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 150.5903 - val_loss: 185.5711\n",
      "Epoch 804/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 147.2844 - val_loss: 191.0877\n",
      "Epoch 805/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 147.0107 - val_loss: 192.9576\n",
      "Epoch 806/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 145.5579 - val_loss: 192.2842\n",
      "Epoch 807/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 148.4621 - val_loss: 191.4083\n",
      "Epoch 808/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 149.7412 - val_loss: 185.3893\n",
      "Epoch 809/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 149.2623 - val_loss: 188.0164\n",
      "Epoch 810/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 148.2085 - val_loss: 192.9522\n",
      "Epoch 811/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 144.4618 - val_loss: 187.5923\n",
      "Epoch 812/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 146.2052 - val_loss: 189.8691\n",
      "Epoch 813/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 146.4286 - val_loss: 185.9553\n",
      "Epoch 814/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 146.1651 - val_loss: 186.6062\n",
      "Epoch 815/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 146.9090 - val_loss: 188.9218\n",
      "Epoch 816/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 146.5252 - val_loss: 185.3839\n",
      "Epoch 817/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 149.4493 - val_loss: 187.9655\n",
      "Epoch 818/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 145.8016 - val_loss: 187.2213\n",
      "Epoch 819/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 151.6920 - val_loss: 184.8929\n",
      "Epoch 820/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 150.5439 - val_loss: 190.1086\n",
      "Epoch 821/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 145.1925 - val_loss: 186.6963\n",
      "Epoch 822/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 145.8394 - val_loss: 193.2573\n",
      "Epoch 823/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 147.0710 - val_loss: 190.0414\n",
      "Epoch 824/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 142.6956 - val_loss: 192.4466\n",
      "Epoch 825/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 144.2816 - val_loss: 185.9823\n",
      "Epoch 826/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 151.6421 - val_loss: 184.6409\n",
      "Epoch 827/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 149.7981 - val_loss: 193.5689\n",
      "Epoch 828/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 147.4893 - val_loss: 187.9753\n",
      "Epoch 829/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 148.0236 - val_loss: 185.9598\n",
      "Epoch 830/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 148.2896 - val_loss: 186.7232\n",
      "Epoch 831/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 145.2875 - val_loss: 189.0477\n",
      "Epoch 832/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 146.2505 - val_loss: 193.4694\n",
      "Epoch 833/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 146.5688 - val_loss: 191.3867\n",
      "Epoch 834/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 144.9644 - val_loss: 186.3545\n",
      "Epoch 835/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 145.1210 - val_loss: 191.0151\n",
      "Epoch 836/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 144.7158 - val_loss: 192.2712\n",
      "Epoch 837/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 148.5795 - val_loss: 193.9301\n",
      "Epoch 838/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 146.6855 - val_loss: 185.5962\n",
      "Epoch 839/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 147.2456 - val_loss: 192.3402\n",
      "Epoch 840/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 149.5880 - val_loss: 185.8140\n",
      "Epoch 841/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 145.6864 - val_loss: 186.8957\n",
      "Epoch 842/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 144.1941 - val_loss: 187.2075\n",
      "Epoch 843/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 144.9903 - val_loss: 194.4991\n",
      "Epoch 844/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 148.1106 - val_loss: 190.1728\n",
      "Epoch 845/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 146.9237 - val_loss: 186.1105\n",
      "Epoch 846/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 141.4971 - val_loss: 187.0535\n",
      "Epoch 847/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 145.8329 - val_loss: 190.1563\n",
      "Epoch 848/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 148.8346 - val_loss: 192.4099\n",
      "Epoch 849/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 150.1621 - val_loss: 184.1915\n",
      "Epoch 850/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13/13 [==============================] - 0s 8ms/step - loss: 154.2228 - val_loss: 184.3932\n",
      "Epoch 851/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 148.6753 - val_loss: 192.0317\n",
      "Epoch 852/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 146.5186 - val_loss: 197.3560\n",
      "Epoch 853/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 148.0715 - val_loss: 185.3327\n",
      "Epoch 854/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 152.4392 - val_loss: 185.0558\n",
      "Epoch 855/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 146.1754 - val_loss: 194.3156\n",
      "Epoch 856/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 143.1523 - val_loss: 188.4127\n",
      "Epoch 857/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 146.3553 - val_loss: 186.3149\n",
      "Epoch 858/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 146.3797 - val_loss: 205.9519\n",
      "Epoch 859/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 144.8517 - val_loss: 185.4786\n",
      "Epoch 860/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 144.7024 - val_loss: 190.7776\n",
      "Epoch 861/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 142.6905 - val_loss: 194.8143\n",
      "Epoch 862/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 145.8181 - val_loss: 192.4692\n",
      "Epoch 863/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 145.8667 - val_loss: 193.1791\n",
      "Epoch 864/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 147.4166 - val_loss: 185.5683\n",
      "Epoch 865/1000\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 146.0203 - val_loss: 186.9282\n",
      "Epoch 866/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 144.6928 - val_loss: 188.8808\n",
      "Epoch 867/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 143.7776 - val_loss: 188.2873\n",
      "Epoch 868/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 144.6216 - val_loss: 194.3159\n",
      "Epoch 869/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 145.2414 - val_loss: 188.9895\n",
      "Epoch 870/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 142.0884 - val_loss: 189.8503\n",
      "Epoch 871/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 144.8264 - val_loss: 187.2742\n",
      "Epoch 872/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 143.3750 - val_loss: 188.9250\n",
      "Epoch 873/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 143.7198 - val_loss: 192.3053\n",
      "Epoch 874/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 144.6404 - val_loss: 188.1101\n",
      "Epoch 875/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 144.7905 - val_loss: 183.8563\n",
      "Epoch 876/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 142.1131 - val_loss: 189.0001\n",
      "Epoch 877/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 146.2605 - val_loss: 195.2272\n",
      "Epoch 878/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 144.3182 - val_loss: 185.5874\n",
      "Epoch 879/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 146.2274 - val_loss: 185.4115\n",
      "Epoch 880/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 144.7586 - val_loss: 190.1166\n",
      "Epoch 881/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 146.5432 - val_loss: 184.6391\n",
      "Epoch 882/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 143.6596 - val_loss: 188.8322\n",
      "Epoch 883/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 145.5959 - val_loss: 193.0656\n",
      "Epoch 884/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 142.2474 - val_loss: 184.0640\n",
      "Epoch 885/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 145.1823 - val_loss: 188.4926\n",
      "Epoch 886/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 142.6939 - val_loss: 199.6129\n",
      "Epoch 887/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 149.9152 - val_loss: 197.8245\n",
      "Epoch 888/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 145.1902 - val_loss: 188.7608\n",
      "Epoch 889/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 143.9422 - val_loss: 191.0592\n",
      "Epoch 890/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 148.1812 - val_loss: 194.6407\n",
      "Epoch 891/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 146.0285 - val_loss: 184.7541\n",
      "Epoch 892/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 144.2409 - val_loss: 187.3934\n",
      "Epoch 893/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 144.6900 - val_loss: 192.5775\n",
      "Epoch 894/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 142.9241 - val_loss: 188.7185\n",
      "Epoch 895/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 144.0885 - val_loss: 187.7323\n",
      "Epoch 896/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 144.1168 - val_loss: 189.6097\n",
      "Epoch 897/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 144.8777 - val_loss: 187.6906\n",
      "Epoch 898/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 147.5065 - val_loss: 183.9757\n",
      "Epoch 899/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 142.2871 - val_loss: 185.7010\n",
      "Epoch 900/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 142.8371 - val_loss: 188.2158\n",
      "Epoch 901/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 143.4779 - val_loss: 191.8590\n",
      "Epoch 902/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 144.3324 - val_loss: 187.5859\n",
      "Epoch 903/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 143.4970 - val_loss: 186.8125\n",
      "Epoch 904/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 142.6367 - val_loss: 184.4836\n",
      "Epoch 905/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 141.1995 - val_loss: 184.0679\n",
      "Epoch 906/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 143.5759 - val_loss: 199.9458\n",
      "Epoch 907/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 142.0282 - val_loss: 189.2753\n",
      "Epoch 908/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 142.7523 - val_loss: 190.6728\n",
      "Epoch 909/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 141.0059 - val_loss: 184.9502\n",
      "Epoch 910/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 138.7206 - val_loss: 193.4958\n",
      "Epoch 911/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 142.2314 - val_loss: 191.6904\n",
      "Epoch 912/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 141.4401 - val_loss: 185.5598\n",
      "Epoch 913/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 141.7979 - val_loss: 186.5782\n",
      "Epoch 914/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 143.5833 - val_loss: 185.8885\n",
      "Epoch 915/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 143.8278 - val_loss: 198.6735\n",
      "Epoch 916/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 142.5175 - val_loss: 186.6978\n",
      "Epoch 917/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 140.3506 - val_loss: 189.3583\n",
      "Epoch 918/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 144.8188 - val_loss: 185.8074\n",
      "Epoch 919/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 142.0682 - val_loss: 185.1613\n",
      "Epoch 920/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 146.4539 - val_loss: 184.0258\n",
      "Epoch 921/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 141.2875 - val_loss: 188.0495\n",
      "Epoch 922/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 142.6553 - val_loss: 193.6073\n",
      "Epoch 923/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 142.9363 - val_loss: 196.9434\n",
      "Epoch 924/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 142.2445 - val_loss: 188.3432\n",
      "Epoch 925/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 145.0338 - val_loss: 183.5715\n",
      "Epoch 926/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 148.8404 - val_loss: 183.4609\n",
      "Epoch 927/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13/13 [==============================] - 0s 8ms/step - loss: 145.4460 - val_loss: 185.4242\n",
      "Epoch 928/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 144.2249 - val_loss: 200.2738\n",
      "Epoch 929/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 143.7084 - val_loss: 183.9710\n",
      "Epoch 930/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 143.9478 - val_loss: 184.8506\n",
      "Epoch 931/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 143.0067 - val_loss: 188.5596\n",
      "Epoch 932/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 140.3333 - val_loss: 185.2035\n",
      "Epoch 933/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 140.5742 - val_loss: 185.2616\n",
      "Epoch 934/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 141.5810 - val_loss: 188.3222\n",
      "Epoch 935/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 140.5753 - val_loss: 188.7357\n",
      "Epoch 936/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 141.0194 - val_loss: 186.3927\n",
      "Epoch 937/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 139.6689 - val_loss: 190.5595\n",
      "Epoch 938/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 144.4718 - val_loss: 191.6263\n",
      "Epoch 939/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 145.9955 - val_loss: 183.2958\n",
      "Epoch 940/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 146.4944 - val_loss: 183.8754\n",
      "Epoch 941/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 139.0719 - val_loss: 185.4445\n",
      "Epoch 942/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 140.8811 - val_loss: 185.9015\n",
      "Epoch 943/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 144.3438 - val_loss: 183.3380\n",
      "Epoch 944/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 144.7963 - val_loss: 189.7585\n",
      "Epoch 945/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 139.6731 - val_loss: 186.8903\n",
      "Epoch 946/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 142.4650 - val_loss: 186.9704\n",
      "Epoch 947/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 138.2151 - val_loss: 190.8940\n",
      "Epoch 948/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 144.9875 - val_loss: 182.7565\n",
      "Epoch 949/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 142.4971 - val_loss: 183.6347\n",
      "Epoch 950/1000\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 140.1947 - val_loss: 186.7968\n",
      "Epoch 951/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 142.9687 - val_loss: 188.3699\n",
      "Epoch 952/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 142.2495 - val_loss: 183.6202\n",
      "Epoch 953/1000\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 140.7728 - val_loss: 188.1561\n",
      "Epoch 954/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 140.9010 - val_loss: 185.6929\n",
      "Epoch 955/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 143.0366 - val_loss: 199.1646\n",
      "Epoch 956/1000\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 141.8535 - val_loss: 187.4438\n",
      "Epoch 957/1000\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 139.6462 - val_loss: 182.9404\n",
      "Epoch 958/1000\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 140.5921 - val_loss: 184.4614\n",
      "Epoch 959/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 142.1674 - val_loss: 182.5418\n",
      "Epoch 960/1000\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 142.4961 - val_loss: 187.7562\n",
      "Epoch 961/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 139.1772 - val_loss: 189.1379\n",
      "Epoch 962/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 139.3409 - val_loss: 189.6342\n",
      "Epoch 963/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 141.3084 - val_loss: 182.8789\n",
      "Epoch 964/1000\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 141.3962 - val_loss: 186.6128\n",
      "Epoch 965/1000\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 142.0119 - val_loss: 186.4846\n",
      "Epoch 966/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 138.5717 - val_loss: 186.8507\n",
      "Epoch 967/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 140.7421 - val_loss: 196.7321\n",
      "Epoch 968/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 142.5684 - val_loss: 185.0706\n",
      "Epoch 969/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 140.9725 - val_loss: 183.0934\n",
      "Epoch 970/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 143.0619 - val_loss: 185.3043\n",
      "Epoch 971/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 140.2198 - val_loss: 196.3668\n",
      "Epoch 972/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 142.4734 - val_loss: 185.5779\n",
      "Epoch 973/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 146.6744 - val_loss: 193.0017\n",
      "Epoch 974/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 139.3891 - val_loss: 188.1525\n",
      "Epoch 975/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 142.8190 - val_loss: 183.1330\n",
      "Epoch 976/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 140.3328 - val_loss: 184.7204\n",
      "Epoch 977/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 140.3928 - val_loss: 190.3664\n",
      "Epoch 978/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 141.0444 - val_loss: 193.2704\n",
      "Epoch 979/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 141.8224 - val_loss: 189.0854\n",
      "Epoch 980/1000\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 143.9304 - val_loss: 182.2064\n",
      "Epoch 981/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 138.4560 - val_loss: 190.1145\n",
      "Epoch 982/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 141.2618 - val_loss: 192.8768\n",
      "Epoch 983/1000\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 143.8343 - val_loss: 190.1140\n",
      "Epoch 984/1000\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 144.0594 - val_loss: 183.9677\n",
      "Epoch 985/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 141.2569 - val_loss: 182.0276\n",
      "Epoch 986/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 140.7234 - val_loss: 185.5441\n",
      "Epoch 987/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 142.7621 - val_loss: 182.0576\n",
      "Epoch 988/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 141.5921 - val_loss: 182.6438\n",
      "Epoch 989/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 139.3120 - val_loss: 189.5766\n",
      "Epoch 990/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 140.0045 - val_loss: 189.7768\n",
      "Epoch 991/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 145.1784 - val_loss: 197.3773\n",
      "Epoch 992/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 141.2103 - val_loss: 182.2598\n",
      "Epoch 993/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 140.3366 - val_loss: 186.7223\n",
      "Epoch 994/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 139.0373 - val_loss: 188.1534\n",
      "Epoch 995/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 138.8986 - val_loss: 185.7695\n",
      "Epoch 996/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 139.3859 - val_loss: 182.6788\n",
      "Epoch 997/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 140.7082 - val_loss: 182.9309\n",
      "Epoch 998/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 140.4208 - val_loss: 185.1779\n",
      "Epoch 999/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 140.4368 - val_loss: 187.5390\n",
      "Epoch 1000/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 140.3066 - val_loss: 197.0685\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(np.asarray(X_train).astype('float32'), np.asarray(y_train).astype('float32'), batch_size = 1024, epochs = 1000, \n",
    "                    validation_split = 0.25, shuffle = True, callbacks = None,\n",
    "                    use_multiprocessing=True, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learningCurve(history):\n",
    "    plt.figure(figsize=(10,8))\n",
    "    plt.plot(history.history['loss'], linewidth=1)\n",
    "    plt.plot(history.history['val_loss'], linewidth=1)\n",
    "    plt.title('Model Loss over Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['training sample loss','validation sample loss'])\n",
    "    plt.savefig('Learning_curve')\n",
    "    plt.show\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "learningCurve(history)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights('modelJ2.h5')\n",
    "model_json = model.to_json()\n",
    "with open(\"modelJ2.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('tcPredtwo',df.astype('float32'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('gitCol', df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'clo'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[349], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclo\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/tensorflow/lib/python3.10/site-packages/pandas/core/generic.py:5902\u001b[0m, in \u001b[0;36mNDFrame.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   5895\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   5896\u001b[0m     name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_internal_names_set\n\u001b[1;32m   5897\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_metadata\n\u001b[1;32m   5898\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_accessors\n\u001b[1;32m   5899\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_info_axis\u001b[38;5;241m.\u001b[39m_can_hold_identifiers_and_holds_name(name)\n\u001b[1;32m   5900\u001b[0m ):\n\u001b[1;32m   5901\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m[name]\n\u001b[0;32m-> 5902\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mobject\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__getattribute__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'clo'"
     ]
    }
   ],
   "source": [
    "df.clo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "df5fa5efdf1c60a896ccc8bc52bcd2fc69320846d08f7bc80e2522b0c75b6345"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
