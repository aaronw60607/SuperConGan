{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.layers import Input, Dense, BatchNormalization\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "import h5py\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from jarvis.db.figshare import data\n",
    "import csv\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "tcDat = pd.read_csv('data/3DSC_MPFULL.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "structDat = pd.read_csv('data/features_hybrid_veuwhornof.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "elements = structDat.loc[:,'H':'Lr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "magpiedat = pd.read_csv('data/magpie.csv'\n",
    "                       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "tc = tcDat['tc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "finDat = pd.concat([tcDat], axis = 1)\n",
    "finDat = pd.concat([finDat,elements],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "finDat = finDat[0:5772]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = finDat['formula_sc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "finDat = finDat.drop('formula_sc', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       data/source/MP/cleaned/cifs/MP-mp-978986.cif\n",
       "1         data/source/MP/cleaned/cifs/MP-mp-1883.cif\n",
       "2       data/source/MP/cleaned/cifs/MP-mp-978986.cif\n",
       "3         data/source/MP/cleaned/cifs/MP-mp-2597.cif\n",
       "4         data/source/MP/cleaned/cifs/MP-mp-7275.cif\n",
       "                            ...                     \n",
       "5767     data/source/MP/cleaned/cifs/MP-mp-21369.cif\n",
       "5768      data/source/MP/cleaned/cifs/MP-mp-2516.cif\n",
       "5769    data/source/MP/cleaned/cifs/MP-mp-972364.cif\n",
       "5770        data/source/MP/cleaned/cifs/MP-mp-79.cif\n",
       "5771      data/source/MP/cleaned/cifs/MP-mp-1401.cif\n",
       "Name: cif_before_synthetic_doping, Length: 5772, dtype: object"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finDat['cif_before_synthetic_doping']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>formula_similarity</th>\n",
       "      <th>totreldiff</th>\n",
       "      <th>formula_frac</th>\n",
       "      <th>correct_formula_frac</th>\n",
       "      <th>formula_2</th>\n",
       "      <th>orig_formula_cif</th>\n",
       "      <th>tc</th>\n",
       "      <th>sc_class</th>\n",
       "      <th>sc_class_unique_sc</th>\n",
       "      <th>norm_formula_sc</th>\n",
       "      <th>...</th>\n",
       "      <th>Pu</th>\n",
       "      <th>Am</th>\n",
       "      <th>Cm</th>\n",
       "      <th>Bk</th>\n",
       "      <th>Cf</th>\n",
       "      <th>Es</th>\n",
       "      <th>Fm</th>\n",
       "      <th>Md</th>\n",
       "      <th>No</th>\n",
       "      <th>Lr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>0.008000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>Ag0.02Ge2Pd1.98Sr1</td>\n",
       "      <td>Ge2Pd2Sr1</td>\n",
       "      <td>2.640000</td>\n",
       "      <td>Other</td>\n",
       "      <td>True</td>\n",
       "      <td>Ag0.4Ge40Pd39.6Sr20</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>0.150000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>Ag0.15Sn0.85Te1</td>\n",
       "      <td>Sn1Te1</td>\n",
       "      <td>2.150000</td>\n",
       "      <td>Other</td>\n",
       "      <td>True</td>\n",
       "      <td>Ag7.5Sn42.5Te50</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.040000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>Ag0.1Ge2Pd1.9Sr1</td>\n",
       "      <td>Ge2Pd2Sr1</td>\n",
       "      <td>2.620000</td>\n",
       "      <td>Other</td>\n",
       "      <td>True</td>\n",
       "      <td>Ag2Ge40Pd38Sr20</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>Ag0.1In0.9Te1</td>\n",
       "      <td>In1Te1</td>\n",
       "      <td>1.200000</td>\n",
       "      <td>Other</td>\n",
       "      <td>True</td>\n",
       "      <td>Ag5In45Te50</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>4.0</td>\n",
       "      <td>False</td>\n",
       "      <td>Ag0.8Ba4Si7.2</td>\n",
       "      <td>Ba4Si8</td>\n",
       "      <td>3.200000</td>\n",
       "      <td>Other</td>\n",
       "      <td>True</td>\n",
       "      <td>Ag6.667Ba33.333Si60</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5767</th>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>False</td>\n",
       "      <td>W6O2</td>\n",
       "      <td>W6O2</td>\n",
       "      <td>1.675000</td>\n",
       "      <td>Oxide</td>\n",
       "      <td>True</td>\n",
       "      <td>W75O25</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5768</th>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>Y1Zn1</td>\n",
       "      <td>Y1Zn1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Other</td>\n",
       "      <td>True</td>\n",
       "      <td>Y50Zn50</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5769</th>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.0</td>\n",
       "      <td>False</td>\n",
       "      <td>Yb3</td>\n",
       "      <td>Yb3</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Heavy_fermion</td>\n",
       "      <td>True</td>\n",
       "      <td>Yb100</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5770</th>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>False</td>\n",
       "      <td>Zn2</td>\n",
       "      <td>Zn2</td>\n",
       "      <td>0.850800</td>\n",
       "      <td>Other</td>\n",
       "      <td>True</td>\n",
       "      <td>Zn100</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5771</th>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>False</td>\n",
       "      <td>Zn4Zr2</td>\n",
       "      <td>Zn4Zr2</td>\n",
       "      <td>0.296667</td>\n",
       "      <td>Other</td>\n",
       "      <td>True</td>\n",
       "      <td>Zn66.667Zr33.333</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5772 rows × 193 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      formula_similarity  totreldiff  formula_frac  correct_formula_frac  \\\n",
       "0                      2    0.008000           1.0                  True   \n",
       "1                      3    0.150000           1.0                  True   \n",
       "2                      2    0.040000           1.0                  True   \n",
       "3                      3    0.100000           1.0                  True   \n",
       "4                      3    0.133333           4.0                 False   \n",
       "...                  ...         ...           ...                   ...   \n",
       "5767                   1    0.000000           2.0                 False   \n",
       "5768                   1    0.000000           1.0                  True   \n",
       "5769                   1    0.000000           3.0                 False   \n",
       "5770                   1    0.000000           2.0                 False   \n",
       "5771                   1    0.000000           2.0                 False   \n",
       "\n",
       "               formula_2 orig_formula_cif        tc       sc_class  \\\n",
       "0     Ag0.02Ge2Pd1.98Sr1        Ge2Pd2Sr1  2.640000          Other   \n",
       "1        Ag0.15Sn0.85Te1           Sn1Te1  2.150000          Other   \n",
       "2       Ag0.1Ge2Pd1.9Sr1        Ge2Pd2Sr1  2.620000          Other   \n",
       "3          Ag0.1In0.9Te1           In1Te1  1.200000          Other   \n",
       "4          Ag0.8Ba4Si7.2           Ba4Si8  3.200000          Other   \n",
       "...                  ...              ...       ...            ...   \n",
       "5767                W6O2             W6O2  1.675000          Oxide   \n",
       "5768               Y1Zn1            Y1Zn1  0.000000          Other   \n",
       "5769                 Yb3              Yb3  0.000000  Heavy_fermion   \n",
       "5770                 Zn2              Zn2  0.850800          Other   \n",
       "5771              Zn4Zr2           Zn4Zr2  0.296667          Other   \n",
       "\n",
       "      sc_class_unique_sc      norm_formula_sc  ...   Pu   Am   Cm   Bk   Cf  \\\n",
       "0                   True  Ag0.4Ge40Pd39.6Sr20  ...  0.0  0.0  0.0  0.0  0.0   \n",
       "1                   True      Ag7.5Sn42.5Te50  ...  0.0  0.0  0.0  0.0  0.0   \n",
       "2                   True      Ag2Ge40Pd38Sr20  ...  0.0  0.0  0.0  0.0  0.0   \n",
       "3                   True          Ag5In45Te50  ...  0.0  0.0  0.0  0.0  0.0   \n",
       "4                   True  Ag6.667Ba33.333Si60  ...  0.0  0.0  0.0  0.0  0.0   \n",
       "...                  ...                  ...  ...  ...  ...  ...  ...  ...   \n",
       "5767                True               W75O25  ...  0.0  0.0  0.0  0.0  0.0   \n",
       "5768                True              Y50Zn50  ...  0.0  0.0  0.0  0.0  0.0   \n",
       "5769                True                Yb100  ...  0.0  0.0  0.0  0.0  0.0   \n",
       "5770                True                Zn100  ...  0.0  0.0  0.0  0.0  0.0   \n",
       "5771                True     Zn66.667Zr33.333  ...  0.0  0.0  0.0  0.0  0.0   \n",
       "\n",
       "       Es   Fm   Md   No   Lr  \n",
       "0     0.0  0.0  0.0  0.0  0.0  \n",
       "1     0.0  0.0  0.0  0.0  0.0  \n",
       "2     0.0  0.0  0.0  0.0  0.0  \n",
       "3     0.0  0.0  0.0  0.0  0.0  \n",
       "4     0.0  0.0  0.0  0.0  0.0  \n",
       "...   ...  ...  ...  ...  ...  \n",
       "5767  0.0  0.0  0.0  0.0  0.0  \n",
       "5768  0.0  0.0  0.0  0.0  0.0  \n",
       "5769  0.0  0.0  0.0  0.0  0.0  \n",
       "5770  0.0  0.0  0.0  0.0  0.0  \n",
       "5771  0.0  0.0  0.0  0.0  0.0  \n",
       "\n",
       "[5772 rows x 193 columns]"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finDat.drop('crystal_temp_2', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "finDat = finDat.drop('weight', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "finDat = finDat[finDat.tc != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "finDat = finDat[finDat.tc >= 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('columns', finDat.drop('tc',axis=1).columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>formula_similarity</th>\n",
       "      <th>totreldiff</th>\n",
       "      <th>formula_frac</th>\n",
       "      <th>correct_formula_frac</th>\n",
       "      <th>formula_2</th>\n",
       "      <th>orig_formula_cif</th>\n",
       "      <th>tc</th>\n",
       "      <th>sc_class</th>\n",
       "      <th>sc_class_unique_sc</th>\n",
       "      <th>norm_formula_sc</th>\n",
       "      <th>...</th>\n",
       "      <th>Pu</th>\n",
       "      <th>Am</th>\n",
       "      <th>Cm</th>\n",
       "      <th>Bk</th>\n",
       "      <th>Cf</th>\n",
       "      <th>Es</th>\n",
       "      <th>Fm</th>\n",
       "      <th>Md</th>\n",
       "      <th>No</th>\n",
       "      <th>Lr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>2</td>\n",
       "      <td>0.006667</td>\n",
       "      <td>1.00</td>\n",
       "      <td>True</td>\n",
       "      <td>Al0.01B2Mg0.99</td>\n",
       "      <td>B2Mg1</td>\n",
       "      <td>37.95</td>\n",
       "      <td>Other</td>\n",
       "      <td>True</td>\n",
       "      <td>Al0.333B66.667Mg33</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>2</td>\n",
       "      <td>0.014667</td>\n",
       "      <td>1.00</td>\n",
       "      <td>True</td>\n",
       "      <td>Al0.022B2Mg0.978</td>\n",
       "      <td>B2Mg1</td>\n",
       "      <td>34.70</td>\n",
       "      <td>Other</td>\n",
       "      <td>True</td>\n",
       "      <td>Al0.733B66.667Mg32.6</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>2</td>\n",
       "      <td>0.013333</td>\n",
       "      <td>1.00</td>\n",
       "      <td>True</td>\n",
       "      <td>Al0.02B2Mg0.98</td>\n",
       "      <td>B2Mg1</td>\n",
       "      <td>36.70</td>\n",
       "      <td>Other</td>\n",
       "      <td>True</td>\n",
       "      <td>Al0.667B66.667Mg32.667</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>2</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>2.00</td>\n",
       "      <td>False</td>\n",
       "      <td>Al0.04Nb6Sn1.96</td>\n",
       "      <td>Nb6Sn2</td>\n",
       "      <td>17.90</td>\n",
       "      <td>Other</td>\n",
       "      <td>True</td>\n",
       "      <td>Al0.5Nb75Sn24.5</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>2</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>2.00</td>\n",
       "      <td>False</td>\n",
       "      <td>Al0.04Si1.96V6</td>\n",
       "      <td>Si2V6</td>\n",
       "      <td>16.12</td>\n",
       "      <td>Other</td>\n",
       "      <td>True</td>\n",
       "      <td>Al0.5Si24.5V75</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5614</th>\n",
       "      <td>3</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>2.00</td>\n",
       "      <td>False</td>\n",
       "      <td>Si1.6Sn0.4V6</td>\n",
       "      <td>Si2V6</td>\n",
       "      <td>12.85</td>\n",
       "      <td>Other</td>\n",
       "      <td>True</td>\n",
       "      <td>Si20Sn5V75</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5619</th>\n",
       "      <td>3</td>\n",
       "      <td>0.075000</td>\n",
       "      <td>2.00</td>\n",
       "      <td>False</td>\n",
       "      <td>Si2Ti0.3V5.7</td>\n",
       "      <td>Si2V6</td>\n",
       "      <td>14.55</td>\n",
       "      <td>Other</td>\n",
       "      <td>True</td>\n",
       "      <td>Si25Ti3.75V71.25</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5620</th>\n",
       "      <td>3</td>\n",
       "      <td>0.150000</td>\n",
       "      <td>2.00</td>\n",
       "      <td>False</td>\n",
       "      <td>Si2Ti0.6V5.4</td>\n",
       "      <td>Si2V6</td>\n",
       "      <td>10.90</td>\n",
       "      <td>Other</td>\n",
       "      <td>True</td>\n",
       "      <td>Si25Ti7.5V67.5</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5624</th>\n",
       "      <td>3</td>\n",
       "      <td>0.150000</td>\n",
       "      <td>2.00</td>\n",
       "      <td>False</td>\n",
       "      <td>Si2V5.4Zr0.6</td>\n",
       "      <td>Si2V6</td>\n",
       "      <td>13.20</td>\n",
       "      <td>Other</td>\n",
       "      <td>True</td>\n",
       "      <td>Si25V67.5Zr7.5</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5629</th>\n",
       "      <td>2</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.08</td>\n",
       "      <td>False</td>\n",
       "      <td>Si1.8V6</td>\n",
       "      <td>Si2V6</td>\n",
       "      <td>16.10</td>\n",
       "      <td>Other</td>\n",
       "      <td>True</td>\n",
       "      <td>Si23.077V76.923</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1407 rows × 193 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      formula_similarity  totreldiff  formula_frac  correct_formula_frac  \\\n",
       "61                     2    0.006667          1.00                  True   \n",
       "62                     2    0.014667          1.00                  True   \n",
       "63                     2    0.013333          1.00                  True   \n",
       "64                     2    0.010000          2.00                 False   \n",
       "65                     2    0.010000          2.00                 False   \n",
       "...                  ...         ...           ...                   ...   \n",
       "5614                   3    0.100000          2.00                 False   \n",
       "5619                   3    0.075000          2.00                 False   \n",
       "5620                   3    0.150000          2.00                 False   \n",
       "5624                   3    0.150000          2.00                 False   \n",
       "5629                   2    0.038462          0.08                 False   \n",
       "\n",
       "             formula_2 orig_formula_cif     tc sc_class  sc_class_unique_sc  \\\n",
       "61      Al0.01B2Mg0.99            B2Mg1  37.95    Other                True   \n",
       "62    Al0.022B2Mg0.978            B2Mg1  34.70    Other                True   \n",
       "63      Al0.02B2Mg0.98            B2Mg1  36.70    Other                True   \n",
       "64     Al0.04Nb6Sn1.96           Nb6Sn2  17.90    Other                True   \n",
       "65      Al0.04Si1.96V6            Si2V6  16.12    Other                True   \n",
       "...                ...              ...    ...      ...                 ...   \n",
       "5614      Si1.6Sn0.4V6            Si2V6  12.85    Other                True   \n",
       "5619      Si2Ti0.3V5.7            Si2V6  14.55    Other                True   \n",
       "5620      Si2Ti0.6V5.4            Si2V6  10.90    Other                True   \n",
       "5624      Si2V5.4Zr0.6            Si2V6  13.20    Other                True   \n",
       "5629           Si1.8V6            Si2V6  16.10    Other                True   \n",
       "\n",
       "             norm_formula_sc  ...   Pu   Am   Cm   Bk   Cf   Es   Fm   Md  \\\n",
       "61        Al0.333B66.667Mg33  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "62      Al0.733B66.667Mg32.6  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "63    Al0.667B66.667Mg32.667  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "64           Al0.5Nb75Sn24.5  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "65            Al0.5Si24.5V75  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "...                      ...  ...  ...  ...  ...  ...  ...  ...  ...  ...   \n",
       "5614              Si20Sn5V75  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "5619        Si25Ti3.75V71.25  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "5620          Si25Ti7.5V67.5  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "5624          Si25V67.5Zr7.5  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "5629         Si23.077V76.923  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "\n",
       "       No   Lr  \n",
       "61    0.0  0.0  \n",
       "62    0.0  0.0  \n",
       "63    0.0  0.0  \n",
       "64    0.0  0.0  \n",
       "65    0.0  0.0  \n",
       "...   ...  ...  \n",
       "5614  0.0  0.0  \n",
       "5619  0.0  0.0  \n",
       "5620  0.0  0.0  \n",
       "5624  0.0  0.0  \n",
       "5629  0.0  0.0  \n",
       "\n",
       "[1407 rows x 193 columns]"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finDat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "finDat=finDat.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "finDat = finDat.drop('correct_formula_frac', axis=1)\n",
    "finDat = finDat.drop('sc_class_unique_sc', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in np.asarray(finDat.columns):\n",
    "    if type(finDat[x][0]) == str:\n",
    "\n",
    "        finDat = finDat.drop(x, axis=1)\n",
    "        \n",
    "for x in np.asarray(finDat.columns):\n",
    "    \n",
    "    if type(finDat[x][0]) == bool:\n",
    "\n",
    "        finDat = finDat.drop(x, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_h = pd.get_dummies(finDat.has_bandstructure_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "finDat= finDat.drop('has_bandstructure_2', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'total_magnetization_normalized_vol_2'"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finDat.columns[30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "finDat = finDat.drop('is_ordered_2',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "finDat= finDat.drop('is_magnetic_2', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "#finDat=finDat[finDat.synth_doped == False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('columns',finDat.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "finDat = finDat.drop('synth_doped', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "finDat = finDat.drop('no_crystal_temp_given_2', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'band_gap_2'"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finDat.columns[9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "finDat = finDat.drop('band_structure_2', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.int64'> 0\n",
      "<class 'numpy.int64'> 1\n",
      "<class 'numpy.float64'> 2\n",
      "<class 'numpy.float64'> 3\n",
      "<class 'numpy.float64'> 4\n",
      "<class 'numpy.int64'> 5\n",
      "<class 'numpy.float64'> 6\n",
      "<class 'numpy.float64'> 7\n",
      "<class 'numpy.float64'> 8\n",
      "<class 'numpy.float64'> 9\n",
      "<class 'numpy.float64'> 10\n",
      "<class 'float'> 11\n",
      "<class 'float'> 12\n",
      "<class 'numpy.float64'> 13\n",
      "<class 'numpy.float64'> 14\n",
      "<class 'numpy.float64'> 15\n",
      "<class 'numpy.float64'> 16\n",
      "<class 'numpy.float64'> 17\n",
      "<class 'numpy.float64'> 18\n",
      "<class 'numpy.float64'> 19\n",
      "<class 'numpy.float64'> 20\n",
      "<class 'numpy.int64'> 21\n",
      "<class 'numpy.int64'> 22\n",
      "<class 'numpy.float64'> 23\n",
      "<class 'numpy.float64'> 24\n",
      "<class 'numpy.int64'> 25\n",
      "<class 'numpy.int64'> 26\n",
      "<class 'numpy.float64'> 27\n",
      "<class 'numpy.float64'> 28\n",
      "<class 'numpy.int64'> 29\n",
      "<class 'numpy.float64'> 30\n",
      "<class 'numpy.float64'> 31\n",
      "<class 'numpy.int64'> 32\n",
      "<class 'numpy.int64'> 33\n",
      "<class 'numpy.int64'> 34\n",
      "<class 'numpy.int64'> 35\n",
      "<class 'numpy.int64'> 36\n",
      "<class 'numpy.int64'> 37\n",
      "<class 'numpy.int64'> 38\n",
      "<class 'numpy.int64'> 39\n",
      "<class 'numpy.int64'> 40\n",
      "<class 'numpy.int64'> 41\n",
      "<class 'numpy.int64'> 42\n",
      "<class 'numpy.int64'> 43\n",
      "<class 'numpy.float64'> 44\n",
      "<class 'numpy.float64'> 45\n",
      "<class 'numpy.float64'> 46\n",
      "<class 'numpy.float64'> 47\n",
      "<class 'numpy.float64'> 48\n",
      "<class 'numpy.float64'> 49\n",
      "<class 'numpy.float64'> 50\n",
      "<class 'numpy.float64'> 51\n",
      "<class 'numpy.float64'> 52\n",
      "<class 'numpy.float64'> 53\n",
      "<class 'numpy.float64'> 54\n",
      "<class 'numpy.float64'> 55\n",
      "<class 'numpy.float64'> 56\n",
      "<class 'numpy.float64'> 57\n",
      "<class 'numpy.float64'> 58\n",
      "<class 'numpy.float64'> 59\n",
      "<class 'numpy.float64'> 60\n",
      "<class 'numpy.float64'> 61\n",
      "<class 'numpy.float64'> 62\n",
      "<class 'numpy.float64'> 63\n",
      "<class 'numpy.float64'> 64\n",
      "<class 'numpy.float64'> 65\n",
      "<class 'numpy.float64'> 66\n",
      "<class 'numpy.float64'> 67\n",
      "<class 'numpy.float64'> 68\n",
      "<class 'numpy.float64'> 69\n",
      "<class 'numpy.float64'> 70\n",
      "<class 'numpy.float64'> 71\n",
      "<class 'numpy.float64'> 72\n",
      "<class 'numpy.float64'> 73\n",
      "<class 'numpy.float64'> 74\n",
      "<class 'numpy.float64'> 75\n",
      "<class 'numpy.float64'> 76\n",
      "<class 'numpy.float64'> 77\n",
      "<class 'numpy.float64'> 78\n",
      "<class 'numpy.float64'> 79\n",
      "<class 'numpy.float64'> 80\n",
      "<class 'numpy.float64'> 81\n",
      "<class 'numpy.float64'> 82\n",
      "<class 'numpy.float64'> 83\n",
      "<class 'numpy.float64'> 84\n",
      "<class 'numpy.float64'> 85\n",
      "<class 'numpy.float64'> 86\n",
      "<class 'numpy.float64'> 87\n",
      "<class 'numpy.float64'> 88\n",
      "<class 'numpy.float64'> 89\n",
      "<class 'numpy.float64'> 90\n",
      "<class 'numpy.float64'> 91\n",
      "<class 'numpy.float64'> 92\n",
      "<class 'numpy.float64'> 93\n",
      "<class 'numpy.float64'> 94\n",
      "<class 'numpy.float64'> 95\n",
      "<class 'numpy.float64'> 96\n",
      "<class 'numpy.float64'> 97\n",
      "<class 'numpy.float64'> 98\n",
      "<class 'numpy.float64'> 99\n",
      "<class 'numpy.float64'> 100\n",
      "<class 'numpy.float64'> 101\n",
      "<class 'numpy.float64'> 102\n",
      "<class 'numpy.float64'> 103\n",
      "<class 'numpy.float64'> 104\n",
      "<class 'numpy.float64'> 105\n",
      "<class 'numpy.float64'> 106\n",
      "<class 'numpy.float64'> 107\n",
      "<class 'numpy.float64'> 108\n",
      "<class 'numpy.float64'> 109\n",
      "<class 'numpy.float64'> 110\n",
      "<class 'numpy.float64'> 111\n",
      "<class 'numpy.float64'> 112\n",
      "<class 'numpy.float64'> 113\n",
      "<class 'numpy.float64'> 114\n",
      "<class 'numpy.float64'> 115\n",
      "<class 'numpy.float64'> 116\n",
      "<class 'numpy.float64'> 117\n",
      "<class 'numpy.float64'> 118\n",
      "<class 'numpy.float64'> 119\n",
      "<class 'numpy.float64'> 120\n",
      "<class 'numpy.float64'> 121\n",
      "<class 'numpy.float64'> 122\n",
      "<class 'numpy.float64'> 123\n",
      "<class 'numpy.float64'> 124\n",
      "<class 'numpy.float64'> 125\n",
      "<class 'numpy.float64'> 126\n",
      "<class 'numpy.float64'> 127\n",
      "<class 'numpy.float64'> 128\n",
      "<class 'numpy.float64'> 129\n",
      "<class 'numpy.float64'> 130\n",
      "<class 'numpy.float64'> 131\n",
      "<class 'numpy.float64'> 132\n",
      "<class 'numpy.float64'> 133\n",
      "<class 'numpy.float64'> 134\n",
      "<class 'numpy.float64'> 135\n",
      "<class 'numpy.float64'> 136\n",
      "<class 'numpy.float64'> 137\n",
      "<class 'numpy.float64'> 138\n",
      "<class 'numpy.float64'> 139\n",
      "<class 'numpy.float64'> 140\n",
      "<class 'numpy.float64'> 141\n",
      "<class 'numpy.float64'> 142\n",
      "<class 'numpy.float64'> 143\n",
      "<class 'numpy.float64'> 144\n",
      "<class 'numpy.float64'> 145\n",
      "<class 'numpy.float64'> 146\n"
     ]
    }
   ],
   "source": [
    "num = 0\n",
    "for x in finDat.columns:\n",
    "    print(str(type(finDat[x][0]))  + ' ' + str(num))\n",
    "    num = num+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "finDat= finDat.drop('index', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "finDat=finDat.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'save' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[158], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m np\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[43msave\u001b[49m\u001b[38;5;241m.\u001b[39mcolumns)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'save' is not defined"
     ]
    }
   ],
   "source": [
    "np.save('columns', save.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>formula_similarity</th>\n",
       "      <th>totreldiff</th>\n",
       "      <th>formula_frac</th>\n",
       "      <th>tc</th>\n",
       "      <th>num_elements_sc</th>\n",
       "      <th>lata_2</th>\n",
       "      <th>latb_2</th>\n",
       "      <th>latc_2</th>\n",
       "      <th>band_gap_2</th>\n",
       "      <th>density_2</th>\n",
       "      <th>...</th>\n",
       "      <th>Pu</th>\n",
       "      <th>Am</th>\n",
       "      <th>Cm</th>\n",
       "      <th>Bk</th>\n",
       "      <th>Cf</th>\n",
       "      <th>Es</th>\n",
       "      <th>Fm</th>\n",
       "      <th>Md</th>\n",
       "      <th>No</th>\n",
       "      <th>Lr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>0.006667</td>\n",
       "      <td>1.00</td>\n",
       "      <td>37.95</td>\n",
       "      <td>3</td>\n",
       "      <td>3.073680</td>\n",
       "      <td>3.073680</td>\n",
       "      <td>3.534350</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.637301</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.014667</td>\n",
       "      <td>1.00</td>\n",
       "      <td>34.70</td>\n",
       "      <td>3</td>\n",
       "      <td>3.073680</td>\n",
       "      <td>3.073680</td>\n",
       "      <td>3.534350</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.637301</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.013333</td>\n",
       "      <td>1.00</td>\n",
       "      <td>36.70</td>\n",
       "      <td>3</td>\n",
       "      <td>3.073680</td>\n",
       "      <td>3.073680</td>\n",
       "      <td>3.534350</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.637301</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>2.00</td>\n",
       "      <td>17.90</td>\n",
       "      <td>3</td>\n",
       "      <td>5.334611</td>\n",
       "      <td>5.334611</td>\n",
       "      <td>5.334611</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.694218</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>2.00</td>\n",
       "      <td>16.12</td>\n",
       "      <td>3</td>\n",
       "      <td>4.702381</td>\n",
       "      <td>4.702381</td>\n",
       "      <td>4.702381</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.778142</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1402</th>\n",
       "      <td>3</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>2.00</td>\n",
       "      <td>12.85</td>\n",
       "      <td>3</td>\n",
       "      <td>4.702381</td>\n",
       "      <td>4.702381</td>\n",
       "      <td>4.702381</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.778142</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1403</th>\n",
       "      <td>3</td>\n",
       "      <td>0.075000</td>\n",
       "      <td>2.00</td>\n",
       "      <td>14.55</td>\n",
       "      <td>3</td>\n",
       "      <td>4.702381</td>\n",
       "      <td>4.702381</td>\n",
       "      <td>4.702381</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.778142</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1404</th>\n",
       "      <td>3</td>\n",
       "      <td>0.150000</td>\n",
       "      <td>2.00</td>\n",
       "      <td>10.90</td>\n",
       "      <td>3</td>\n",
       "      <td>4.702381</td>\n",
       "      <td>4.702381</td>\n",
       "      <td>4.702381</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.778142</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1405</th>\n",
       "      <td>3</td>\n",
       "      <td>0.150000</td>\n",
       "      <td>2.00</td>\n",
       "      <td>13.20</td>\n",
       "      <td>3</td>\n",
       "      <td>4.702381</td>\n",
       "      <td>4.702381</td>\n",
       "      <td>4.702381</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.778142</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1406</th>\n",
       "      <td>2</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.08</td>\n",
       "      <td>16.10</td>\n",
       "      <td>2</td>\n",
       "      <td>4.702381</td>\n",
       "      <td>4.702381</td>\n",
       "      <td>4.702381</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.778142</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1407 rows × 146 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      formula_similarity  totreldiff  formula_frac     tc  num_elements_sc  \\\n",
       "0                      2    0.006667          1.00  37.95                3   \n",
       "1                      2    0.014667          1.00  34.70                3   \n",
       "2                      2    0.013333          1.00  36.70                3   \n",
       "3                      2    0.010000          2.00  17.90                3   \n",
       "4                      2    0.010000          2.00  16.12                3   \n",
       "...                  ...         ...           ...    ...              ...   \n",
       "1402                   3    0.100000          2.00  12.85                3   \n",
       "1403                   3    0.075000          2.00  14.55                3   \n",
       "1404                   3    0.150000          2.00  10.90                3   \n",
       "1405                   3    0.150000          2.00  13.20                3   \n",
       "1406                   2    0.038462          0.08  16.10                2   \n",
       "\n",
       "        lata_2    latb_2    latc_2  band_gap_2  density_2  ...   Pu   Am   Cm  \\\n",
       "0     3.073680  3.073680  3.534350         0.0   2.637301  ...  0.0  0.0  0.0   \n",
       "1     3.073680  3.073680  3.534350         0.0   2.637301  ...  0.0  0.0  0.0   \n",
       "2     3.073680  3.073680  3.534350         0.0   2.637301  ...  0.0  0.0  0.0   \n",
       "3     5.334611  5.334611  5.334611         0.0   8.694218  ...  0.0  0.0  0.0   \n",
       "4     4.702381  4.702381  4.702381         0.0   5.778142  ...  0.0  0.0  0.0   \n",
       "...        ...       ...       ...         ...        ...  ...  ...  ...  ...   \n",
       "1402  4.702381  4.702381  4.702381         0.0   5.778142  ...  0.0  0.0  0.0   \n",
       "1403  4.702381  4.702381  4.702381         0.0   5.778142  ...  0.0  0.0  0.0   \n",
       "1404  4.702381  4.702381  4.702381         0.0   5.778142  ...  0.0  0.0  0.0   \n",
       "1405  4.702381  4.702381  4.702381         0.0   5.778142  ...  0.0  0.0  0.0   \n",
       "1406  4.702381  4.702381  4.702381         0.0   5.778142  ...  0.0  0.0  0.0   \n",
       "\n",
       "       Bk   Cf   Es   Fm   Md   No   Lr  \n",
       "0     0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "1     0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "2     0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "3     0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "4     0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "...   ...  ...  ...  ...  ...  ...  ...  \n",
       "1402  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "1403  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "1404  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "1405  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "1406  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "\n",
       "[1407 rows x 146 columns]"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finDat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "row = pd.concat([finDat,elements], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "finDat = finDat.drop('doi_2', axis=1)\n",
    "finDat = finDat.drop('doi_bibtex_2', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_4 (InputLayer)        [(None, 143)]             0         \n",
      "                                                                 \n",
      " fc1_relu (Dense)            (None, 256)               36864     \n",
      "                                                                 \n",
      " fc2 (Dense)                 (None, 128)               32896     \n",
      "                                                                 \n",
      " fc3 (Dense)                 (None, 64)                8256      \n",
      "                                                                 \n",
      " fc4 (Dense)                 (None, 32)                2080      \n",
      "                                                                 \n",
      " relu (Dense)                (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 80,129\n",
      "Trainable params: 80,129\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "Inputs = Input(shape=(143))\n",
    "x = BatchNormalization()(Inputs)\n",
    "\n",
    "x = Dense(256, activation='relu', kernel_initializer='lecun_uniform', name='fc1_relu')(Inputs)\n",
    "y = Dense(128, activation='relu', kernel_initializer='lecun_uniform', name = 'fc2')(x)\n",
    "z = Dense(64, activation='relu', kernel_initializer='lecun_uniform', name = 'fc3')(y)\n",
    "a = Dense(32, activation='relu', kernel_initializer='lecun_uniform', name='fc4')(z)\n",
    "predictions = Dense(1, activation='relu', kernel_initializer='lecun_uniform', name = 'relu')(a)\n",
    "model = Model(inputs=Inputs, outputs=predictions)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anrunw/miniconda3/envs/tensorflow/lib/python3.10/site-packages/keras/optimizers/optimizer_v2/adam.py:117: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "adam = Adam(lr=0.0001)\n",
    "model.compile(optimizer=adam, loss='MSE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['tc'] not found in axis\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[173], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m X_train, X_test, y_train, y_test \u001b[38;5;241m=\u001b[39m train_test_split(\u001b[43mfinDat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtc\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m, finDat[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtc\u001b[39m\u001b[38;5;124m'\u001b[39m], test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/tensorflow/lib/python3.10/site-packages/pandas/util/_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[1;32m    326\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    327\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[1;32m    329\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[1;32m    330\u001b[0m     )\n\u001b[0;32m--> 331\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/tensorflow/lib/python3.10/site-packages/pandas/core/frame.py:5399\u001b[0m, in \u001b[0;36mDataFrame.drop\u001b[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[1;32m   5251\u001b[0m \u001b[38;5;129m@deprecate_nonkeyword_arguments\u001b[39m(version\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, allowed_args\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mself\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m   5252\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdrop\u001b[39m(  \u001b[38;5;66;03m# type: ignore[override]\u001b[39;00m\n\u001b[1;32m   5253\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   5260\u001b[0m     errors: IgnoreRaise \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraise\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   5261\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   5262\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   5263\u001b[0m \u001b[38;5;124;03m    Drop specified labels from rows or columns.\u001b[39;00m\n\u001b[1;32m   5264\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   5397\u001b[0m \u001b[38;5;124;03m            weight  1.0     0.8\u001b[39;00m\n\u001b[1;32m   5398\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 5399\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   5400\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5401\u001b[0m \u001b[43m        \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5402\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5403\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5404\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5405\u001b[0m \u001b[43m        \u001b[49m\u001b[43minplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5406\u001b[0m \u001b[43m        \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5407\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/tensorflow/lib/python3.10/site-packages/pandas/util/_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[1;32m    326\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    327\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[1;32m    329\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[1;32m    330\u001b[0m     )\n\u001b[0;32m--> 331\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/tensorflow/lib/python3.10/site-packages/pandas/core/generic.py:4505\u001b[0m, in \u001b[0;36mNDFrame.drop\u001b[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[1;32m   4503\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m axis, labels \u001b[38;5;129;01min\u001b[39;00m axes\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m   4504\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 4505\u001b[0m         obj \u001b[38;5;241m=\u001b[39m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_drop_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4507\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inplace:\n\u001b[1;32m   4508\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_inplace(obj)\n",
      "File \u001b[0;32m~/miniconda3/envs/tensorflow/lib/python3.10/site-packages/pandas/core/generic.py:4546\u001b[0m, in \u001b[0;36mNDFrame._drop_axis\u001b[0;34m(self, labels, axis, level, errors, only_slice)\u001b[0m\n\u001b[1;32m   4544\u001b[0m         new_axis \u001b[38;5;241m=\u001b[39m axis\u001b[38;5;241m.\u001b[39mdrop(labels, level\u001b[38;5;241m=\u001b[39mlevel, errors\u001b[38;5;241m=\u001b[39merrors)\n\u001b[1;32m   4545\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 4546\u001b[0m         new_axis \u001b[38;5;241m=\u001b[39m \u001b[43maxis\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4547\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m axis\u001b[38;5;241m.\u001b[39mget_indexer(new_axis)\n\u001b[1;32m   4549\u001b[0m \u001b[38;5;66;03m# Case for non-unique axis\u001b[39;00m\n\u001b[1;32m   4550\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/tensorflow/lib/python3.10/site-packages/pandas/core/indexes/base.py:6934\u001b[0m, in \u001b[0;36mIndex.drop\u001b[0;34m(self, labels, errors)\u001b[0m\n\u001b[1;32m   6932\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mask\u001b[38;5;241m.\u001b[39many():\n\u001b[1;32m   6933\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m errors \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m-> 6934\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(labels[mask])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not found in axis\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   6935\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m indexer[\u001b[38;5;241m~\u001b[39mmask]\n\u001b[1;32m   6936\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdelete(indexer)\n",
      "\u001b[0;31mKeyError\u001b[0m: \"['tc'] not found in axis\""
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(finDat.drop('tc',axis=1), finDat['tc'], test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['tc'] not found in axis\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[174], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mfinDat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtc\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/tensorflow/lib/python3.10/site-packages/pandas/util/_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[1;32m    326\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    327\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[1;32m    329\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[1;32m    330\u001b[0m     )\n\u001b[0;32m--> 331\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/tensorflow/lib/python3.10/site-packages/pandas/core/frame.py:5399\u001b[0m, in \u001b[0;36mDataFrame.drop\u001b[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[1;32m   5251\u001b[0m \u001b[38;5;129m@deprecate_nonkeyword_arguments\u001b[39m(version\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, allowed_args\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mself\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m   5252\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdrop\u001b[39m(  \u001b[38;5;66;03m# type: ignore[override]\u001b[39;00m\n\u001b[1;32m   5253\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   5260\u001b[0m     errors: IgnoreRaise \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraise\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   5261\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   5262\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   5263\u001b[0m \u001b[38;5;124;03m    Drop specified labels from rows or columns.\u001b[39;00m\n\u001b[1;32m   5264\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   5397\u001b[0m \u001b[38;5;124;03m            weight  1.0     0.8\u001b[39;00m\n\u001b[1;32m   5398\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 5399\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   5400\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5401\u001b[0m \u001b[43m        \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5402\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5403\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5404\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5405\u001b[0m \u001b[43m        \u001b[49m\u001b[43minplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5406\u001b[0m \u001b[43m        \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5407\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/tensorflow/lib/python3.10/site-packages/pandas/util/_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[1;32m    326\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    327\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[1;32m    329\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[1;32m    330\u001b[0m     )\n\u001b[0;32m--> 331\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/tensorflow/lib/python3.10/site-packages/pandas/core/generic.py:4505\u001b[0m, in \u001b[0;36mNDFrame.drop\u001b[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[1;32m   4503\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m axis, labels \u001b[38;5;129;01min\u001b[39;00m axes\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m   4504\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 4505\u001b[0m         obj \u001b[38;5;241m=\u001b[39m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_drop_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4507\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inplace:\n\u001b[1;32m   4508\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_inplace(obj)\n",
      "File \u001b[0;32m~/miniconda3/envs/tensorflow/lib/python3.10/site-packages/pandas/core/generic.py:4546\u001b[0m, in \u001b[0;36mNDFrame._drop_axis\u001b[0;34m(self, labels, axis, level, errors, only_slice)\u001b[0m\n\u001b[1;32m   4544\u001b[0m         new_axis \u001b[38;5;241m=\u001b[39m axis\u001b[38;5;241m.\u001b[39mdrop(labels, level\u001b[38;5;241m=\u001b[39mlevel, errors\u001b[38;5;241m=\u001b[39merrors)\n\u001b[1;32m   4545\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 4546\u001b[0m         new_axis \u001b[38;5;241m=\u001b[39m \u001b[43maxis\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4547\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m axis\u001b[38;5;241m.\u001b[39mget_indexer(new_axis)\n\u001b[1;32m   4549\u001b[0m \u001b[38;5;66;03m# Case for non-unique axis\u001b[39;00m\n\u001b[1;32m   4550\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/tensorflow/lib/python3.10/site-packages/pandas/core/indexes/base.py:6934\u001b[0m, in \u001b[0;36mIndex.drop\u001b[0;34m(self, labels, errors)\u001b[0m\n\u001b[1;32m   6932\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mask\u001b[38;5;241m.\u001b[39many():\n\u001b[1;32m   6933\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m errors \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m-> 6934\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(labels[mask])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not found in axis\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   6935\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m indexer[\u001b[38;5;241m~\u001b[39mmask]\n\u001b[1;32m   6936\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdelete(indexer)\n",
      "\u001b[0;31mKeyError\u001b[0m: \"['tc'] not found in axis\""
     ]
    }
   ],
   "source": [
    "finDat.drop('tc', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['tc'] not found in axis\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[175], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m finDat \u001b[38;5;241m=\u001b[39m \u001b[43mfinDat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtc\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/tensorflow/lib/python3.10/site-packages/pandas/util/_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[1;32m    326\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    327\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[1;32m    329\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[1;32m    330\u001b[0m     )\n\u001b[0;32m--> 331\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/tensorflow/lib/python3.10/site-packages/pandas/core/frame.py:5399\u001b[0m, in \u001b[0;36mDataFrame.drop\u001b[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[1;32m   5251\u001b[0m \u001b[38;5;129m@deprecate_nonkeyword_arguments\u001b[39m(version\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, allowed_args\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mself\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m   5252\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdrop\u001b[39m(  \u001b[38;5;66;03m# type: ignore[override]\u001b[39;00m\n\u001b[1;32m   5253\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   5260\u001b[0m     errors: IgnoreRaise \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraise\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   5261\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   5262\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   5263\u001b[0m \u001b[38;5;124;03m    Drop specified labels from rows or columns.\u001b[39;00m\n\u001b[1;32m   5264\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   5397\u001b[0m \u001b[38;5;124;03m            weight  1.0     0.8\u001b[39;00m\n\u001b[1;32m   5398\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 5399\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   5400\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5401\u001b[0m \u001b[43m        \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5402\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5403\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5404\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5405\u001b[0m \u001b[43m        \u001b[49m\u001b[43minplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5406\u001b[0m \u001b[43m        \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5407\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/tensorflow/lib/python3.10/site-packages/pandas/util/_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[1;32m    326\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    327\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[1;32m    329\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[1;32m    330\u001b[0m     )\n\u001b[0;32m--> 331\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/tensorflow/lib/python3.10/site-packages/pandas/core/generic.py:4505\u001b[0m, in \u001b[0;36mNDFrame.drop\u001b[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[1;32m   4503\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m axis, labels \u001b[38;5;129;01min\u001b[39;00m axes\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m   4504\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 4505\u001b[0m         obj \u001b[38;5;241m=\u001b[39m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_drop_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4507\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inplace:\n\u001b[1;32m   4508\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_inplace(obj)\n",
      "File \u001b[0;32m~/miniconda3/envs/tensorflow/lib/python3.10/site-packages/pandas/core/generic.py:4546\u001b[0m, in \u001b[0;36mNDFrame._drop_axis\u001b[0;34m(self, labels, axis, level, errors, only_slice)\u001b[0m\n\u001b[1;32m   4544\u001b[0m         new_axis \u001b[38;5;241m=\u001b[39m axis\u001b[38;5;241m.\u001b[39mdrop(labels, level\u001b[38;5;241m=\u001b[39mlevel, errors\u001b[38;5;241m=\u001b[39merrors)\n\u001b[1;32m   4545\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 4546\u001b[0m         new_axis \u001b[38;5;241m=\u001b[39m \u001b[43maxis\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4547\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m axis\u001b[38;5;241m.\u001b[39mget_indexer(new_axis)\n\u001b[1;32m   4549\u001b[0m \u001b[38;5;66;03m# Case for non-unique axis\u001b[39;00m\n\u001b[1;32m   4550\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/tensorflow/lib/python3.10/site-packages/pandas/core/indexes/base.py:6934\u001b[0m, in \u001b[0;36mIndex.drop\u001b[0;34m(self, labels, errors)\u001b[0m\n\u001b[1;32m   6932\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mask\u001b[38;5;241m.\u001b[39many():\n\u001b[1;32m   6933\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m errors \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m-> 6934\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(labels[mask])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not found in axis\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   6935\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m indexer[\u001b[38;5;241m~\u001b[39mmask]\n\u001b[1;32m   6936\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdelete(indexer)\n",
      "\u001b[0;31mKeyError\u001b[0m: \"['tc'] not found in axis\""
     ]
    }
   ],
   "source": [
    "finDat = finDat.drop('tc', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['doi_bibtex_2'] not found in axis\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[176], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mfinDat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdoi_bibtex_2\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/tensorflow/lib/python3.10/site-packages/pandas/util/_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[1;32m    326\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    327\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[1;32m    329\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[1;32m    330\u001b[0m     )\n\u001b[0;32m--> 331\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/tensorflow/lib/python3.10/site-packages/pandas/core/frame.py:5399\u001b[0m, in \u001b[0;36mDataFrame.drop\u001b[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[1;32m   5251\u001b[0m \u001b[38;5;129m@deprecate_nonkeyword_arguments\u001b[39m(version\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, allowed_args\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mself\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m   5252\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdrop\u001b[39m(  \u001b[38;5;66;03m# type: ignore[override]\u001b[39;00m\n\u001b[1;32m   5253\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   5260\u001b[0m     errors: IgnoreRaise \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraise\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   5261\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   5262\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   5263\u001b[0m \u001b[38;5;124;03m    Drop specified labels from rows or columns.\u001b[39;00m\n\u001b[1;32m   5264\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   5397\u001b[0m \u001b[38;5;124;03m            weight  1.0     0.8\u001b[39;00m\n\u001b[1;32m   5398\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 5399\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   5400\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5401\u001b[0m \u001b[43m        \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5402\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5403\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5404\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5405\u001b[0m \u001b[43m        \u001b[49m\u001b[43minplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5406\u001b[0m \u001b[43m        \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5407\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/tensorflow/lib/python3.10/site-packages/pandas/util/_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[1;32m    326\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    327\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[1;32m    329\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[1;32m    330\u001b[0m     )\n\u001b[0;32m--> 331\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/tensorflow/lib/python3.10/site-packages/pandas/core/generic.py:4505\u001b[0m, in \u001b[0;36mNDFrame.drop\u001b[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[1;32m   4503\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m axis, labels \u001b[38;5;129;01min\u001b[39;00m axes\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m   4504\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 4505\u001b[0m         obj \u001b[38;5;241m=\u001b[39m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_drop_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4507\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inplace:\n\u001b[1;32m   4508\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_inplace(obj)\n",
      "File \u001b[0;32m~/miniconda3/envs/tensorflow/lib/python3.10/site-packages/pandas/core/generic.py:4546\u001b[0m, in \u001b[0;36mNDFrame._drop_axis\u001b[0;34m(self, labels, axis, level, errors, only_slice)\u001b[0m\n\u001b[1;32m   4544\u001b[0m         new_axis \u001b[38;5;241m=\u001b[39m axis\u001b[38;5;241m.\u001b[39mdrop(labels, level\u001b[38;5;241m=\u001b[39mlevel, errors\u001b[38;5;241m=\u001b[39merrors)\n\u001b[1;32m   4545\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 4546\u001b[0m         new_axis \u001b[38;5;241m=\u001b[39m \u001b[43maxis\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4547\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m axis\u001b[38;5;241m.\u001b[39mget_indexer(new_axis)\n\u001b[1;32m   4549\u001b[0m \u001b[38;5;66;03m# Case for non-unique axis\u001b[39;00m\n\u001b[1;32m   4550\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/tensorflow/lib/python3.10/site-packages/pandas/core/indexes/base.py:6934\u001b[0m, in \u001b[0;36mIndex.drop\u001b[0;34m(self, labels, errors)\u001b[0m\n\u001b[1;32m   6932\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mask\u001b[38;5;241m.\u001b[39many():\n\u001b[1;32m   6933\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m errors \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m-> 6934\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(labels[mask])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not found in axis\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   6935\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m indexer[\u001b[38;5;241m~\u001b[39mmask]\n\u001b[1;32m   6936\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdelete(indexer)\n",
      "\u001b[0;31mKeyError\u001b[0m: \"['doi_bibtex_2'] not found in axis\""
     ]
    }
   ],
   "source": [
    "finDat.drop('doi_bibtex_2', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 95.4880 - val_loss: 132.3746\n",
      "Epoch 2/1000\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 95.4132 - val_loss: 131.1792\n",
      "Epoch 3/1000\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 95.2408 - val_loss: 131.2899\n",
      "Epoch 4/1000\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 95.0472 - val_loss: 131.1078\n",
      "Epoch 5/1000\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 94.9960 - val_loss: 130.8668\n",
      "Epoch 6/1000\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 95.0966 - val_loss: 132.1486\n",
      "Epoch 7/1000\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 95.2332 - val_loss: 131.1857\n",
      "Epoch 8/1000\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 95.2273 - val_loss: 131.9622\n",
      "Epoch 9/1000\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 95.0923 - val_loss: 130.8347\n",
      "Epoch 10/1000\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 94.9254 - val_loss: 130.9407\n",
      "Epoch 11/1000\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 94.8108 - val_loss: 131.2186\n",
      "Epoch 12/1000\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 94.8206 - val_loss: 130.8103\n",
      "Epoch 13/1000\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 94.8947 - val_loss: 131.6782\n",
      "Epoch 14/1000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 94.9330 - val_loss: 130.6059\n",
      "Epoch 15/1000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 94.8834 - val_loss: 131.3333\n",
      "Epoch 16/1000\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 94.8232 - val_loss: 130.4450\n",
      "Epoch 17/1000\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 94.7669 - val_loss: 131.0999\n",
      "Epoch 18/1000\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 94.6876 - val_loss: 130.6481\n",
      "Epoch 19/1000\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 94.5920 - val_loss: 130.7159\n",
      "Epoch 20/1000\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 94.5273 - val_loss: 130.6357\n",
      "Epoch 21/1000\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 94.4971 - val_loss: 130.3843\n",
      "Epoch 22/1000\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 94.4962 - val_loss: 130.9290\n",
      "Epoch 23/1000\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 94.4903 - val_loss: 130.4882\n",
      "Epoch 24/1000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 94.5184 - val_loss: 131.1609\n",
      "Epoch 25/1000\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 94.5167 - val_loss: 130.3105\n",
      "Epoch 26/1000\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 94.4836 - val_loss: 130.7782\n",
      "Epoch 27/1000\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 94.3879 - val_loss: 130.4118\n",
      "Epoch 28/1000\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 94.2880 - val_loss: 130.5907\n",
      "Epoch 29/1000\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 94.2426 - val_loss: 130.6758\n",
      "Epoch 30/1000\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 94.2235 - val_loss: 130.1990\n",
      "Epoch 31/1000\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 94.2498 - val_loss: 130.8751\n",
      "Epoch 32/1000\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 94.3125 - val_loss: 130.0914\n",
      "Epoch 33/1000\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 94.3774 - val_loss: 131.2285\n",
      "Epoch 34/1000\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 94.4316 - val_loss: 130.2113\n",
      "Epoch 35/1000\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 94.3325 - val_loss: 130.8949\n",
      "Epoch 36/1000\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 94.1742 - val_loss: 130.1939\n",
      "Epoch 37/1000\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 94.0425 - val_loss: 130.2573\n",
      "Epoch 38/1000\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 93.9532 - val_loss: 130.2188\n",
      "Epoch 39/1000\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 93.9503 - val_loss: 129.8105\n",
      "Epoch 40/1000\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 94.0282 - val_loss: 130.7835\n",
      "Epoch 41/1000\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 94.1052 - val_loss: 129.9730\n",
      "Epoch 42/1000\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 94.1501 - val_loss: 130.9576\n",
      "Epoch 43/1000\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 94.1526 - val_loss: 129.8414\n",
      "Epoch 44/1000\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 94.0090 - val_loss: 130.2285\n",
      "Epoch 45/1000\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 93.8437 - val_loss: 129.7938\n",
      "Epoch 46/1000\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 93.7318 - val_loss: 129.8446\n",
      "Epoch 47/1000\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 93.6893 - val_loss: 130.2532\n",
      "Epoch 48/1000\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 93.7312 - val_loss: 129.7590\n",
      "Epoch 49/1000\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 93.8334 - val_loss: 130.6211\n",
      "Epoch 50/1000\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 93.8848 - val_loss: 129.5393\n",
      "Epoch 51/1000\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 93.8876 - val_loss: 130.2818\n",
      "Epoch 52/1000\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 93.7352 - val_loss: 129.5572\n",
      "Epoch 53/1000\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 93.5579 - val_loss: 129.7227\n",
      "Epoch 54/1000\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 93.4870 - val_loss: 129.9213\n",
      "Epoch 55/1000\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 93.5106 - val_loss: 129.3878\n",
      "Epoch 56/1000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 93.5777 - val_loss: 130.2119\n",
      "Epoch 57/1000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 93.6296 - val_loss: 129.3723\n",
      "Epoch 58/1000\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 93.5728 - val_loss: 130.0677\n",
      "Epoch 59/1000\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 93.4808 - val_loss: 129.3839\n",
      "Epoch 60/1000\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 93.3699 - val_loss: 129.5766\n",
      "Epoch 61/1000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 93.2876 - val_loss: 129.4197\n",
      "Epoch 62/1000\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 93.2512 - val_loss: 129.1991\n",
      "Epoch 63/1000\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 93.2547 - val_loss: 129.7006\n",
      "Epoch 64/1000\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 93.3009 - val_loss: 129.0921\n",
      "Epoch 65/1000\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 93.3809 - val_loss: 130.0164\n",
      "Epoch 66/1000\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 93.4334 - val_loss: 129.1127\n",
      "Epoch 67/1000\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 93.4100 - val_loss: 130.1538\n",
      "Epoch 68/1000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 93.3049 - val_loss: 129.3788\n",
      "Epoch 69/1000\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 93.1829 - val_loss: 129.6570\n",
      "Epoch 70/1000\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 93.0573 - val_loss: 129.4601\n",
      "Epoch 71/1000\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 93.0071 - val_loss: 129.1523\n",
      "Epoch 72/1000\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 93.0276 - val_loss: 129.4938\n",
      "Epoch 73/1000\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 93.0475 - val_loss: 128.8144\n",
      "Epoch 74/1000\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 93.0700 - val_loss: 129.6832\n",
      "Epoch 75/1000\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 93.0745 - val_loss: 128.9305\n",
      "Epoch 76/1000\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 92.9912 - val_loss: 129.2280\n",
      "Epoch 77/1000\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 92.8790 - val_loss: 128.9343\n",
      "Epoch 78/1000\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 92.8253 - val_loss: 129.0010\n",
      "Epoch 79/1000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 92.8009 - val_loss: 129.1369\n",
      "Epoch 80/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 21ms/step - loss: 92.7777 - val_loss: 128.8104\n",
      "Epoch 81/1000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 92.7816 - val_loss: 129.2677\n",
      "Epoch 82/1000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 92.8040 - val_loss: 128.6893\n",
      "Epoch 83/1000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 92.8113 - val_loss: 129.3350\n",
      "Epoch 84/1000\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 92.8058 - val_loss: 128.6012\n",
      "Epoch 85/1000\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 92.8249 - val_loss: 129.5304\n",
      "Epoch 86/1000\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 92.8413 - val_loss: 128.7423\n",
      "Epoch 87/1000\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 92.8016 - val_loss: 129.2849\n",
      "Epoch 88/1000\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 92.6702 - val_loss: 128.6646\n",
      "Epoch 89/1000\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 92.5585 - val_loss: 128.7034\n",
      "Epoch 90/1000\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 92.5082 - val_loss: 128.9463\n",
      "Epoch 91/1000\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 92.4997 - val_loss: 128.5581\n",
      "Epoch 92/1000\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 92.5242 - val_loss: 129.1598\n",
      "Epoch 93/1000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 92.5365 - val_loss: 128.5422\n",
      "Epoch 94/1000\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 92.5173 - val_loss: 128.9918\n",
      "Epoch 95/1000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 92.4648 - val_loss: 128.2794\n",
      "Epoch 96/1000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 92.4383 - val_loss: 128.7737\n",
      "Epoch 97/1000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 92.4113 - val_loss: 128.3035\n",
      "Epoch 98/1000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 92.3684 - val_loss: 128.8121\n",
      "Epoch 99/1000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 92.3154 - val_loss: 128.3686\n",
      "Epoch 100/1000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 92.2727 - val_loss: 128.6704\n",
      "Epoch 101/1000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 92.2547 - val_loss: 128.2621\n",
      "Epoch 102/1000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 92.2419 - val_loss: 128.7486\n",
      "Epoch 103/1000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 92.2156 - val_loss: 128.2649\n",
      "Epoch 104/1000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 92.2007 - val_loss: 128.7829\n",
      "Epoch 105/1000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 92.2070 - val_loss: 128.1331\n",
      "Epoch 106/1000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 92.2391 - val_loss: 128.8111\n",
      "Epoch 107/1000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 92.2073 - val_loss: 128.1667\n",
      "Epoch 108/1000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 92.1254 - val_loss: 128.6436\n",
      "Epoch 109/1000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 92.0553 - val_loss: 128.2215\n",
      "Epoch 110/1000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 92.0028 - val_loss: 128.3284\n",
      "Epoch 111/1000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 91.9617 - val_loss: 128.0202\n",
      "Epoch 112/1000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 91.9284 - val_loss: 128.1921\n",
      "Epoch 113/1000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 91.8958 - val_loss: 128.1251\n",
      "Epoch 114/1000\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 91.8691 - val_loss: 128.2585\n",
      "Epoch 115/1000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 91.8416 - val_loss: 127.9880\n",
      "Epoch 116/1000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 91.8165 - val_loss: 128.0767\n",
      "Epoch 117/1000\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 91.8016 - val_loss: 127.7950\n",
      "Epoch 118/1000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 91.8164 - val_loss: 128.6801\n",
      "Epoch 119/1000\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 91.9125 - val_loss: 128.0046\n",
      "Epoch 120/1000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 92.0967 - val_loss: 129.5746\n",
      "Epoch 121/1000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 92.2871 - val_loss: 128.1250\n",
      "Epoch 122/1000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 92.2808 - val_loss: 129.1435\n",
      "Epoch 123/1000\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 92.0402 - val_loss: 127.9236\n",
      "Epoch 124/1000\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 91.7329 - val_loss: 127.9756\n",
      "Epoch 125/1000\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 91.6102 - val_loss: 128.5570\n",
      "Epoch 126/1000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 91.7399 - val_loss: 127.9124\n",
      "Epoch 127/1000\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 91.9463 - val_loss: 128.9517\n",
      "Epoch 128/1000\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 91.9280 - val_loss: 127.6654\n",
      "Epoch 129/1000\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 91.7826 - val_loss: 128.2386\n",
      "Epoch 130/1000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 91.5779 - val_loss: 127.7583\n",
      "Epoch 131/1000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 91.4655 - val_loss: 127.8331\n",
      "Epoch 132/1000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 91.4367 - val_loss: 128.1694\n",
      "Epoch 133/1000\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 91.4479 - val_loss: 127.7154\n",
      "Epoch 134/1000\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 91.4652 - val_loss: 128.2766\n",
      "Epoch 135/1000\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 91.4907 - val_loss: 127.5483\n",
      "Epoch 136/1000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 91.5079 - val_loss: 128.3367\n",
      "Epoch 137/1000\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 91.5079 - val_loss: 127.5062\n",
      "Epoch 138/1000\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 91.4539 - val_loss: 128.1446\n",
      "Epoch 139/1000\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 91.3683 - val_loss: 127.5851\n",
      "Epoch 140/1000\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 91.2847 - val_loss: 127.8222\n",
      "Epoch 141/1000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 91.2032 - val_loss: 127.7169\n",
      "Epoch 142/1000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 91.1572 - val_loss: 127.4589\n",
      "Epoch 143/1000\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 91.1608 - val_loss: 127.8363\n",
      "Epoch 144/1000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 91.2119 - val_loss: 127.2422\n",
      "Epoch 145/1000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 91.2826 - val_loss: 128.1071\n",
      "Epoch 146/1000\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 91.3231 - val_loss: 127.2132\n",
      "Epoch 147/1000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 91.2765 - val_loss: 127.8462\n",
      "Epoch 148/1000\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 91.1774 - val_loss: 127.1790\n",
      "Epoch 149/1000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 91.0649 - val_loss: 127.5644\n",
      "Epoch 150/1000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 90.9865 - val_loss: 127.4871\n",
      "Epoch 151/1000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 90.9370 - val_loss: 127.4034\n",
      "Epoch 152/1000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 90.9394 - val_loss: 127.8691\n",
      "Epoch 153/1000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 90.9852 - val_loss: 127.2471\n",
      "Epoch 154/1000\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 91.0130 - val_loss: 127.8527\n",
      "Epoch 155/1000\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 91.0171 - val_loss: 127.0822\n",
      "Epoch 156/1000\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 90.9896 - val_loss: 127.7895\n",
      "Epoch 157/1000\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 90.9411 - val_loss: 127.1962\n",
      "Epoch 158/1000\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 90.8342 - val_loss: 127.4413\n",
      "Epoch 159/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 22ms/step - loss: 90.7550 - val_loss: 127.2262\n",
      "Epoch 160/1000\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 90.7094 - val_loss: 127.1186\n",
      "Epoch 161/1000\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 90.6968 - val_loss: 127.3755\n",
      "Epoch 162/1000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 90.7082 - val_loss: 126.9976\n",
      "Epoch 163/1000\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 90.7708 - val_loss: 127.9154\n",
      "Epoch 164/1000\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 90.8576 - val_loss: 127.0491\n",
      "Epoch 165/1000\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 90.8826 - val_loss: 127.8727\n",
      "Epoch 166/1000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 90.8503 - val_loss: 126.8482\n",
      "Epoch 167/1000\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 90.7625 - val_loss: 127.4741\n",
      "Epoch 168/1000\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 90.6481 - val_loss: 126.9178\n",
      "Epoch 169/1000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 90.5476 - val_loss: 127.1021\n",
      "Epoch 170/1000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 90.4776 - val_loss: 127.0745\n",
      "Epoch 171/1000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 90.4512 - val_loss: 126.8300\n",
      "Epoch 172/1000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 90.4594 - val_loss: 127.4126\n",
      "Epoch 173/1000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 90.5243 - val_loss: 126.8590\n",
      "Epoch 174/1000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 90.5897 - val_loss: 127.9067\n",
      "Epoch 175/1000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 90.6651 - val_loss: 126.8912\n",
      "Epoch 176/1000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 90.7262 - val_loss: 127.8071\n",
      "Epoch 177/1000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 90.6139 - val_loss: 126.8616\n",
      "Epoch 178/1000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 90.4509 - val_loss: 127.2213\n",
      "Epoch 179/1000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 90.2951 - val_loss: 127.0149\n",
      "Epoch 180/1000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 90.2364 - val_loss: 126.7548\n",
      "Epoch 181/1000\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 90.2416 - val_loss: 127.1930\n",
      "Epoch 182/1000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 90.3094 - val_loss: 126.6283\n",
      "Epoch 183/1000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 90.3803 - val_loss: 127.5476\n",
      "Epoch 184/1000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 90.4263 - val_loss: 126.6257\n",
      "Epoch 185/1000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 90.4341 - val_loss: 127.4552\n",
      "Epoch 186/1000\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 90.3188 - val_loss: 126.6927\n",
      "Epoch 187/1000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 90.1593 - val_loss: 126.8675\n",
      "Epoch 188/1000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 90.0649 - val_loss: 126.9566\n",
      "Epoch 189/1000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 90.0523 - val_loss: 126.5043\n",
      "Epoch 190/1000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 90.1225 - val_loss: 127.3149\n",
      "Epoch 191/1000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 90.2361 - val_loss: 126.5390\n",
      "Epoch 192/1000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 90.2802 - val_loss: 127.4440\n",
      "Epoch 193/1000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 90.2170 - val_loss: 126.4842\n",
      "Epoch 194/1000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 90.0953 - val_loss: 126.8863\n",
      "Epoch 195/1000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 89.9592 - val_loss: 126.5503\n",
      "Epoch 196/1000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 89.8732 - val_loss: 126.5106\n",
      "Epoch 197/1000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 89.8602 - val_loss: 126.9338\n",
      "Epoch 198/1000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 89.9075 - val_loss: 126.4309\n",
      "Epoch 199/1000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 89.9454 - val_loss: 127.2063\n",
      "Epoch 200/1000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 89.9832 - val_loss: 126.4096\n",
      "Epoch 201/1000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 89.9733 - val_loss: 127.0661\n",
      "Epoch 202/1000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 89.8841 - val_loss: 126.4278\n",
      "Epoch 203/1000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 89.7835 - val_loss: 126.8391\n",
      "Epoch 204/1000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 89.6915 - val_loss: 126.7892\n",
      "Epoch 205/1000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 89.6538 - val_loss: 126.6343\n",
      "Epoch 206/1000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 89.6430 - val_loss: 126.8614\n",
      "Epoch 207/1000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 89.6517 - val_loss: 126.4101\n",
      "Epoch 208/1000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 89.6644 - val_loss: 127.0391\n",
      "Epoch 209/1000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 89.6908 - val_loss: 126.4611\n",
      "Epoch 210/1000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 89.6629 - val_loss: 127.0000\n",
      "Epoch 211/1000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 89.6186 - val_loss: 126.3656\n",
      "Epoch 212/1000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 89.5779 - val_loss: 126.6614\n",
      "Epoch 213/1000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 89.5293 - val_loss: 126.2223\n",
      "Epoch 214/1000\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 89.4660 - val_loss: 126.3667\n",
      "Epoch 215/1000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 89.4191 - val_loss: 126.4462\n",
      "Epoch 216/1000\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 89.3969 - val_loss: 126.3431\n",
      "Epoch 217/1000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 89.4004 - val_loss: 126.7316\n",
      "Epoch 218/1000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 89.4140 - val_loss: 126.2300\n",
      "Epoch 219/1000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 89.4333 - val_loss: 126.9794\n",
      "Epoch 220/1000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 89.4551 - val_loss: 126.3468\n",
      "Epoch 221/1000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 89.4915 - val_loss: 127.1818\n",
      "Epoch 222/1000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 89.4863 - val_loss: 126.1405\n",
      "Epoch 223/1000\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 89.4214 - val_loss: 126.5464\n",
      "Epoch 224/1000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 89.3220 - val_loss: 125.9160\n",
      "Epoch 225/1000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 89.2743 - val_loss: 126.5150\n",
      "Epoch 226/1000\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 89.2210 - val_loss: 126.4382\n",
      "Epoch 227/1000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 89.1649 - val_loss: 126.6901\n",
      "Epoch 228/1000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 89.1302 - val_loss: 126.2875\n",
      "Epoch 229/1000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 89.0955 - val_loss: 126.2350\n",
      "Epoch 230/1000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 89.0752 - val_loss: 126.0789\n",
      "Epoch 231/1000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 89.0607 - val_loss: 126.5213\n",
      "Epoch 232/1000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 89.0506 - val_loss: 126.2316\n",
      "Epoch 233/1000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 89.0609 - val_loss: 126.7459\n",
      "Epoch 234/1000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 89.0942 - val_loss: 126.0103\n",
      "Epoch 235/1000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 89.1996 - val_loss: 126.9581\n",
      "Epoch 236/1000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 89.2561 - val_loss: 126.0077\n",
      "Epoch 237/1000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 89.2939 - val_loss: 127.2103\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 238/1000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 89.2674 - val_loss: 126.2200\n",
      "Epoch 239/1000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 89.1112 - val_loss: 126.6626\n",
      "Epoch 240/1000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 88.9515 - val_loss: 126.1592\n",
      "Epoch 241/1000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 88.8430 - val_loss: 126.1580\n",
      "Epoch 242/1000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 88.8240 - val_loss: 126.6399\n",
      "Epoch 243/1000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 88.8638 - val_loss: 126.1613\n",
      "Epoch 244/1000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 88.9185 - val_loss: 126.9034\n",
      "Epoch 245/1000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 88.9879 - val_loss: 125.8045\n",
      "Epoch 246/1000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 88.9847 - val_loss: 126.4942\n",
      "Epoch 247/1000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 88.9285 - val_loss: 125.6975\n",
      "Epoch 248/1000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 88.8295 - val_loss: 126.3147\n",
      "Epoch 249/1000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 88.7297 - val_loss: 126.0856\n",
      "Epoch 250/1000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 88.6509 - val_loss: 126.1031\n",
      "Epoch 251/1000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 88.6179 - val_loss: 126.3808\n",
      "Epoch 252/1000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 88.6368 - val_loss: 125.8720\n",
      "Epoch 253/1000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 88.7119 - val_loss: 126.7371\n",
      "Epoch 254/1000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 88.7935 - val_loss: 125.8987\n",
      "Epoch 255/1000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 88.7853 - val_loss: 126.6893\n",
      "Epoch 256/1000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 88.7271 - val_loss: 125.6420\n",
      "Epoch 257/1000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 88.6568 - val_loss: 126.2596\n",
      "Epoch 258/1000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 88.5705 - val_loss: 125.7800\n",
      "Epoch 259/1000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 88.4750 - val_loss: 126.1146\n",
      "Epoch 260/1000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 88.4076 - val_loss: 126.0080\n",
      "Epoch 261/1000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 88.3777 - val_loss: 125.9365\n",
      "Epoch 262/1000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 88.3551 - val_loss: 125.8998\n",
      "Epoch 263/1000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 88.3389 - val_loss: 125.8406\n",
      "Epoch 264/1000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 88.3145 - val_loss: 126.0029\n",
      "Epoch 265/1000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 88.3060 - val_loss: 125.7849\n",
      "Epoch 266/1000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 88.2908 - val_loss: 126.1464\n",
      "Epoch 267/1000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 88.2980 - val_loss: 125.7427\n",
      "Epoch 268/1000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 88.3311 - val_loss: 126.4246\n",
      "Epoch 269/1000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 88.3560 - val_loss: 125.7542\n",
      "Epoch 270/1000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 88.3687 - val_loss: 126.4811\n",
      "Epoch 271/1000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 88.3909 - val_loss: 125.5367\n",
      "Epoch 272/1000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 88.4118 - val_loss: 126.3384\n",
      "Epoch 273/1000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 88.3599 - val_loss: 125.6020\n",
      "Epoch 274/1000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 88.3005 - val_loss: 126.4421\n",
      "Epoch 275/1000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 88.2101 - val_loss: 125.9321\n",
      "Epoch 276/1000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 88.1222 - val_loss: 126.1519\n",
      "Epoch 277/1000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 88.0671 - val_loss: 125.7950\n",
      "Epoch 278/1000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 88.0252 - val_loss: 125.8863\n",
      "Epoch 279/1000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 87.9985 - val_loss: 125.9482\n",
      "Epoch 280/1000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 87.9696 - val_loss: 125.8550\n",
      "Epoch 281/1000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 87.9480 - val_loss: 125.7754\n",
      "Epoch 282/1000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 87.9385 - val_loss: 125.5315\n",
      "Epoch 283/1000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 87.9149 - val_loss: 125.6910\n",
      "Epoch 284/1000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 87.8932 - val_loss: 125.7814\n",
      "Epoch 285/1000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 87.8758 - val_loss: 126.0227\n",
      "Epoch 286/1000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 87.8655 - val_loss: 125.5218\n",
      "Epoch 287/1000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 87.8924 - val_loss: 126.1635\n",
      "Epoch 288/1000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 88.0188 - val_loss: 125.4399\n",
      "Epoch 289/1000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 88.2028 - val_loss: 127.1186\n",
      "Epoch 290/1000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 88.4103 - val_loss: 125.8225\n",
      "Epoch 291/1000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 88.4885 - val_loss: 127.2171\n",
      "Epoch 292/1000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 88.3118 - val_loss: 125.6099\n",
      "Epoch 293/1000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 87.9877 - val_loss: 126.0674\n",
      "Epoch 294/1000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 87.7463 - val_loss: 125.8900\n",
      "Epoch 295/1000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 87.6657 - val_loss: 125.7816\n",
      "Epoch 296/1000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 87.7198 - val_loss: 126.4494\n",
      "Epoch 297/1000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 87.8344 - val_loss: 125.4686\n",
      "Epoch 298/1000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 87.9514 - val_loss: 126.3432\n",
      "Epoch 299/1000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 87.9677 - val_loss: 125.1809\n",
      "Epoch 300/1000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 87.8724 - val_loss: 126.0403\n",
      "Epoch 301/1000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 87.7268 - val_loss: 125.3950\n",
      "Epoch 302/1000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 87.5894 - val_loss: 125.7164\n",
      "Epoch 303/1000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 87.4995 - val_loss: 125.6407\n",
      "Epoch 304/1000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 87.4716 - val_loss: 125.5572\n",
      "Epoch 305/1000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 87.4783 - val_loss: 126.0367\n",
      "Epoch 306/1000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 87.4900 - val_loss: 125.5561\n",
      "Epoch 307/1000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 87.5335 - val_loss: 126.1906\n",
      "Epoch 308/1000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 87.5718 - val_loss: 125.4073\n",
      "Epoch 309/1000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 87.5831 - val_loss: 126.0969\n",
      "Epoch 310/1000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 87.5537 - val_loss: 125.3278\n",
      "Epoch 311/1000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 87.4922 - val_loss: 125.8755\n",
      "Epoch 312/1000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 87.4067 - val_loss: 125.4191\n",
      "Epoch 313/1000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 87.3248 - val_loss: 125.7920\n",
      "Epoch 314/1000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 87.2834 - val_loss: 125.7327\n",
      "Epoch 315/1000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 87.2538 - val_loss: 125.6550\n",
      "Epoch 316/1000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 87.2279 - val_loss: 125.7742\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 317/1000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 87.2282 - val_loss: 125.2743\n",
      "Epoch 318/1000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 87.2576 - val_loss: 125.9016\n",
      "Epoch 319/1000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 87.3155 - val_loss: 125.2735\n",
      "Epoch 320/1000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 87.3947 - val_loss: 126.3037\n",
      "Epoch 321/1000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 87.4430 - val_loss: 125.3001\n",
      "Epoch 322/1000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 87.3958 - val_loss: 126.1312\n",
      "Epoch 323/1000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 87.2776 - val_loss: 125.4799\n",
      "Epoch 324/1000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 87.1485 - val_loss: 125.9354\n",
      "Epoch 325/1000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 87.0821 - val_loss: 125.5978\n",
      "Epoch 326/1000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 87.0286 - val_loss: 125.5648\n",
      "Epoch 327/1000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 86.9888 - val_loss: 125.3707\n",
      "Epoch 328/1000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 86.9661 - val_loss: 125.3362\n",
      "Epoch 329/1000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 86.9455 - val_loss: 125.3741\n",
      "Epoch 330/1000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 86.9224 - val_loss: 125.3185\n",
      "Epoch 331/1000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 86.9041 - val_loss: 125.6933\n",
      "Epoch 332/1000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 86.9182 - val_loss: 125.3359\n",
      "Epoch 333/1000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 86.9679 - val_loss: 126.1034\n",
      "Epoch 334/1000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 87.0673 - val_loss: 125.1352\n",
      "Epoch 335/1000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 87.1959 - val_loss: 126.4591\n",
      "Epoch 336/1000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 87.2595 - val_loss: 125.1684\n",
      "Epoch 337/1000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 87.2417 - val_loss: 126.4283\n",
      "Epoch 338/1000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 87.1136 - val_loss: 125.2112\n",
      "Epoch 339/1000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 86.9348 - val_loss: 125.6559\n",
      "Epoch 340/1000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 86.7870 - val_loss: 125.1683\n",
      "Epoch 341/1000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 86.7189 - val_loss: 125.1725\n",
      "Epoch 342/1000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 86.6972 - val_loss: 125.6022\n",
      "Epoch 343/1000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 86.7162 - val_loss: 125.2215\n",
      "Epoch 344/1000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 86.7713 - val_loss: 126.0935\n",
      "Epoch 345/1000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 86.8413 - val_loss: 125.2381\n",
      "Epoch 346/1000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 86.9105 - val_loss: 126.2461\n",
      "Epoch 347/1000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 86.8795 - val_loss: 125.2214\n",
      "Epoch 348/1000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 86.7788 - val_loss: 125.8194\n",
      "Epoch 349/1000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 86.6411 - val_loss: 125.2335\n",
      "Epoch 350/1000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 86.5504 - val_loss: 125.3031\n",
      "Epoch 351/1000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 86.5102 - val_loss: 125.3010\n",
      "Epoch 352/1000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 86.4942 - val_loss: 125.1038\n",
      "Epoch 353/1000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 86.4866 - val_loss: 125.4720\n",
      "Epoch 354/1000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 86.5046 - val_loss: 124.9808\n",
      "Epoch 355/1000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 86.5508 - val_loss: 125.9528\n",
      "Epoch 356/1000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 86.6499 - val_loss: 125.1455\n",
      "Epoch 357/1000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 86.7401 - val_loss: 126.2807\n",
      "Epoch 358/1000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 86.6969 - val_loss: 125.1790\n",
      "Epoch 359/1000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 86.5609 - val_loss: 125.5646\n",
      "Epoch 360/1000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 86.4262 - val_loss: 124.9159\n",
      "Epoch 361/1000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 86.3285 - val_loss: 125.1130\n",
      "Epoch 362/1000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 86.2778 - val_loss: 125.2144\n",
      "Epoch 363/1000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 86.2487 - val_loss: 125.1945\n",
      "Epoch 364/1000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 86.2450 - val_loss: 125.5112\n",
      "Epoch 365/1000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 86.2557 - val_loss: 124.9248\n",
      "Epoch 366/1000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 86.2740 - val_loss: 125.5026\n",
      "Epoch 367/1000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 86.3013 - val_loss: 124.8859\n",
      "Epoch 368/1000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 86.3545 - val_loss: 125.9854\n",
      "Epoch 369/1000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 86.3952 - val_loss: 125.1589\n",
      "Epoch 370/1000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 86.4017 - val_loss: 125.9455\n",
      "Epoch 371/1000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 86.3028 - val_loss: 124.9925\n",
      "Epoch 372/1000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 86.2073 - val_loss: 125.4259\n",
      "Epoch 373/1000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 86.1160 - val_loss: 125.0758\n",
      "Epoch 374/1000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 86.0411 - val_loss: 125.3137\n",
      "Epoch 375/1000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 86.0105 - val_loss: 125.2918\n",
      "Epoch 376/1000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 85.9900 - val_loss: 124.8879\n",
      "Epoch 377/1000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 85.9893 - val_loss: 125.3045\n",
      "Epoch 378/1000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 86.0254 - val_loss: 124.7422\n",
      "Epoch 379/1000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 86.0884 - val_loss: 125.8234\n",
      "Epoch 380/1000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 86.1595 - val_loss: 124.9164\n",
      "Epoch 381/1000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 86.2051 - val_loss: 126.0717\n",
      "Epoch 382/1000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 86.2273 - val_loss: 124.8876\n",
      "Epoch 383/1000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 86.1858 - val_loss: 125.6625\n",
      "Epoch 384/1000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 86.0539 - val_loss: 124.9035\n",
      "Epoch 385/1000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 85.8941 - val_loss: 125.2748\n",
      "Epoch 386/1000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 85.7947 - val_loss: 125.2413\n",
      "Epoch 387/1000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 85.7728 - val_loss: 124.9454\n",
      "Epoch 388/1000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 85.7939 - val_loss: 125.5165\n",
      "Epoch 389/1000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 85.8828 - val_loss: 124.8306\n",
      "Epoch 390/1000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 86.0039 - val_loss: 126.0044\n",
      "Epoch 391/1000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 86.0431 - val_loss: 124.9657\n",
      "Epoch 392/1000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 86.0166 - val_loss: 125.8886\n",
      "Epoch 393/1000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 85.9134 - val_loss: 124.8244\n",
      "Epoch 394/1000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 85.7669 - val_loss: 125.3384\n",
      "Epoch 395/1000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 85.6457 - val_loss: 125.0152\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 396/1000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 85.5864 - val_loss: 124.9984\n",
      "Epoch 397/1000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 85.5611 - val_loss: 125.1807\n",
      "Epoch 398/1000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 85.5586 - val_loss: 124.7980\n",
      "Epoch 399/1000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 85.5943 - val_loss: 125.5579\n",
      "Epoch 400/1000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 85.6755 - val_loss: 124.7183\n",
      "Epoch 401/1000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 85.7806 - val_loss: 125.8702\n",
      "Epoch 402/1000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 85.8234 - val_loss: 124.6778\n",
      "Epoch 403/1000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 85.7289 - val_loss: 125.5638\n",
      "Epoch 404/1000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 85.6520 - val_loss: 124.7920\n",
      "Epoch 405/1000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 85.5562 - val_loss: 125.2421\n",
      "Epoch 406/1000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 85.4467 - val_loss: 124.9137\n",
      "Epoch 407/1000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 85.3832 - val_loss: 124.9150\n",
      "Epoch 408/1000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 85.3435 - val_loss: 125.2282\n",
      "Epoch 409/1000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 85.3559 - val_loss: 124.7768\n",
      "Epoch 410/1000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 85.4090 - val_loss: 125.5378\n",
      "Epoch 411/1000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 85.4317 - val_loss: 124.8895\n",
      "Epoch 412/1000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 85.4168 - val_loss: 125.6633\n",
      "Epoch 413/1000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 85.4050 - val_loss: 124.9326\n",
      "Epoch 414/1000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 85.4040 - val_loss: 125.5672\n",
      "Epoch 415/1000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 85.3909 - val_loss: 124.6942\n",
      "Epoch 416/1000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 85.3861 - val_loss: 125.4997\n",
      "Epoch 417/1000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 85.3487 - val_loss: 124.6937\n",
      "Epoch 418/1000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 85.3112 - val_loss: 125.3793\n",
      "Epoch 419/1000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 85.2710 - val_loss: 124.5732\n",
      "Epoch 420/1000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 85.2515 - val_loss: 125.3028\n",
      "Epoch 421/1000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 85.2287 - val_loss: 124.7050\n",
      "Epoch 422/1000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 85.1794 - val_loss: 125.3219\n",
      "Epoch 423/1000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 85.1129 - val_loss: 124.7524\n",
      "Epoch 424/1000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 85.0564 - val_loss: 124.8152\n",
      "Epoch 425/1000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 85.0130 - val_loss: 124.6090\n",
      "Epoch 426/1000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 84.9926 - val_loss: 124.7268\n",
      "Epoch 427/1000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 84.9574 - val_loss: 124.9864\n",
      "Epoch 428/1000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 84.9462 - val_loss: 124.9389\n",
      "Epoch 429/1000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 84.9217 - val_loss: 125.0722\n",
      "Epoch 430/1000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 84.9074 - val_loss: 124.7411\n",
      "Epoch 431/1000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 84.8884 - val_loss: 124.8374\n",
      "Epoch 432/1000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 84.8909 - val_loss: 124.3709\n",
      "Epoch 433/1000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 84.9454 - val_loss: 125.4380\n",
      "Epoch 434/1000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 85.0750 - val_loss: 124.8227\n",
      "Epoch 435/1000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 85.2788 - val_loss: 126.6168\n",
      "Epoch 436/1000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 85.4403 - val_loss: 124.8448\n",
      "Epoch 437/1000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 85.4749 - val_loss: 125.8451\n",
      "Epoch 438/1000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 85.2133 - val_loss: 124.4011\n",
      "Epoch 439/1000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 84.9360 - val_loss: 124.8985\n",
      "Epoch 440/1000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 84.7373 - val_loss: 125.1455\n",
      "Epoch 441/1000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 84.7261 - val_loss: 124.7702\n",
      "Epoch 442/1000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 84.8335 - val_loss: 125.8595\n",
      "Epoch 443/1000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 85.0280 - val_loss: 124.3948\n",
      "Epoch 444/1000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 85.1385 - val_loss: 125.6920\n",
      "Epoch 445/1000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 85.0638 - val_loss: 124.2411\n",
      "Epoch 446/1000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 84.8922 - val_loss: 125.1603\n",
      "Epoch 447/1000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 84.6741 - val_loss: 124.7349\n",
      "Epoch 448/1000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 84.5655 - val_loss: 124.6906\n",
      "Epoch 449/1000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 84.5415 - val_loss: 124.8374\n",
      "Epoch 450/1000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 84.5683 - val_loss: 124.2786\n",
      "Epoch 451/1000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 84.6269 - val_loss: 125.1616\n",
      "Epoch 452/1000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 84.6661 - val_loss: 124.3032\n",
      "Epoch 453/1000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 84.7136 - val_loss: 125.5194\n",
      "Epoch 454/1000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 84.7391 - val_loss: 124.3602\n",
      "Epoch 455/1000\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 84.7148 - val_loss: 125.2997\n",
      "Epoch 456/1000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 84.5929 - val_loss: 124.4601\n",
      "Epoch 457/1000\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 84.4670 - val_loss: 124.9497\n",
      "Epoch 458/1000\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 84.4057 - val_loss: 124.7105\n",
      "Epoch 459/1000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 84.3536 - val_loss: 124.8111\n",
      "Epoch 460/1000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 84.3196 - val_loss: 124.9105\n",
      "Epoch 461/1000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 84.3085 - val_loss: 124.4637\n",
      "Epoch 462/1000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 84.3264 - val_loss: 125.0341\n",
      "Epoch 463/1000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 84.3873 - val_loss: 124.3262\n",
      "Epoch 464/1000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 84.4590 - val_loss: 125.5366\n",
      "Epoch 465/1000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 84.5329 - val_loss: 124.4658\n",
      "Epoch 466/1000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 84.5979 - val_loss: 125.6538\n",
      "Epoch 467/1000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 84.5597 - val_loss: 124.3342\n",
      "Epoch 468/1000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 84.4910 - val_loss: 125.1712\n",
      "Epoch 469/1000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 84.3292 - val_loss: 124.3591\n",
      "Epoch 470/1000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 84.2082 - val_loss: 124.8133\n",
      "Epoch 471/1000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 84.1089 - val_loss: 124.7637\n",
      "Epoch 472/1000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 84.0757 - val_loss: 124.5277\n",
      "Epoch 473/1000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 84.0906 - val_loss: 125.0961\n",
      "Epoch 474/1000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 84.1587 - val_loss: 124.3149\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 475/1000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 84.2589 - val_loss: 125.5073\n",
      "Epoch 476/1000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 84.3409 - val_loss: 124.3488\n",
      "Epoch 477/1000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 84.3459 - val_loss: 125.4507\n",
      "Epoch 478/1000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 84.2917 - val_loss: 124.2030\n",
      "Epoch 479/1000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 84.2112 - val_loss: 124.8969\n",
      "Epoch 480/1000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 84.0643 - val_loss: 124.2067\n",
      "Epoch 481/1000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 83.9727 - val_loss: 124.5491\n",
      "Epoch 482/1000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 83.9067 - val_loss: 124.5120\n",
      "Epoch 483/1000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 83.8706 - val_loss: 124.4474\n",
      "Epoch 484/1000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 83.8528 - val_loss: 124.7898\n",
      "Epoch 485/1000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 83.8715 - val_loss: 124.2197\n",
      "Epoch 486/1000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 83.9121 - val_loss: 124.9014\n",
      "Epoch 487/1000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 83.9585 - val_loss: 124.1039\n",
      "Epoch 488/1000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 83.9856 - val_loss: 125.1953\n",
      "Epoch 489/1000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 84.0011 - val_loss: 124.2664\n",
      "Epoch 490/1000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 84.0135 - val_loss: 125.1873\n",
      "Epoch 491/1000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 83.9864 - val_loss: 124.0722\n",
      "Epoch 492/1000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 83.9366 - val_loss: 124.9871\n",
      "Epoch 493/1000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 83.8522 - val_loss: 124.3410\n",
      "Epoch 494/1000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 83.7430 - val_loss: 124.6986\n",
      "Epoch 495/1000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 83.6639 - val_loss: 124.5787\n",
      "Epoch 496/1000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 83.6316 - val_loss: 124.3633\n",
      "Epoch 497/1000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 83.6434 - val_loss: 124.8806\n",
      "Epoch 498/1000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 83.6765 - val_loss: 124.1800\n",
      "Epoch 499/1000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 83.7631 - val_loss: 125.2545\n",
      "Epoch 500/1000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 83.8822 - val_loss: 124.0128\n",
      "Epoch 501/1000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 83.9013 - val_loss: 125.1904\n",
      "Epoch 502/1000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 83.8448 - val_loss: 124.0831\n",
      "Epoch 503/1000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 83.7423 - val_loss: 124.8297\n",
      "Epoch 504/1000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 83.5947 - val_loss: 124.2627\n",
      "Epoch 505/1000\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 83.4858 - val_loss: 124.4791\n",
      "Epoch 506/1000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 83.4347 - val_loss: 124.5876\n",
      "Epoch 507/1000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 83.4391 - val_loss: 124.0185\n",
      "Epoch 508/1000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 83.5275 - val_loss: 125.0218\n",
      "Epoch 509/1000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 83.6867 - val_loss: 123.9487\n",
      "Epoch 510/1000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 83.8332 - val_loss: 125.4349\n",
      "Epoch 511/1000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 83.8179 - val_loss: 123.9237\n",
      "Epoch 512/1000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 83.6622 - val_loss: 124.7386\n",
      "Epoch 513/1000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 83.4543 - val_loss: 123.9287\n",
      "Epoch 514/1000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 83.3173 - val_loss: 123.9842\n",
      "Epoch 515/1000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 83.2742 - val_loss: 124.4066\n",
      "Epoch 516/1000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 83.3006 - val_loss: 123.9923\n",
      "Epoch 517/1000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 83.4449 - val_loss: 125.2253\n",
      "Epoch 518/1000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 83.5746 - val_loss: 123.9511\n",
      "Epoch 519/1000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 83.5827 - val_loss: 125.0235\n",
      "Epoch 520/1000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 83.4796 - val_loss: 123.7996\n",
      "Epoch 521/1000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 83.3369 - val_loss: 124.3522\n",
      "Epoch 522/1000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 83.2194 - val_loss: 123.9299\n",
      "Epoch 523/1000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 83.1230 - val_loss: 124.0034\n",
      "Epoch 524/1000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 83.0999 - val_loss: 124.3130\n",
      "Epoch 525/1000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 83.1264 - val_loss: 123.8314\n",
      "Epoch 526/1000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 83.1951 - val_loss: 124.7745\n",
      "Epoch 527/1000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 83.2840 - val_loss: 123.8587\n",
      "Epoch 528/1000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 83.3659 - val_loss: 125.1523\n",
      "Epoch 529/1000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 83.4182 - val_loss: 123.7966\n",
      "Epoch 530/1000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 83.3986 - val_loss: 124.8545\n",
      "Epoch 531/1000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 83.2627 - val_loss: 123.6003\n",
      "Epoch 532/1000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 83.1310 - val_loss: 124.0806\n",
      "Epoch 533/1000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 82.9806 - val_loss: 124.0285\n",
      "Epoch 534/1000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 82.9232 - val_loss: 123.8515\n",
      "Epoch 535/1000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 82.9701 - val_loss: 124.6465\n",
      "Epoch 536/1000\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 83.0704 - val_loss: 123.5061\n",
      "Epoch 537/1000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 83.1855 - val_loss: 124.6736\n",
      "Epoch 538/1000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 83.1758 - val_loss: 123.5941\n",
      "Epoch 539/1000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 83.0896 - val_loss: 124.6402\n",
      "Epoch 540/1000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 83.0004 - val_loss: 123.9415\n",
      "Epoch 541/1000\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 82.8980 - val_loss: 124.2449\n",
      "Epoch 542/1000\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 82.8228 - val_loss: 123.6832\n",
      "Epoch 543/1000\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 82.7813 - val_loss: 123.7784\n",
      "Epoch 544/1000\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 82.7498 - val_loss: 123.7140\n",
      "Epoch 545/1000\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 82.7343 - val_loss: 124.0757\n",
      "Epoch 546/1000\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 82.7231 - val_loss: 123.8151\n",
      "Epoch 547/1000\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 82.7188 - val_loss: 124.2957\n",
      "Epoch 548/1000\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 82.7511 - val_loss: 123.6815\n",
      "Epoch 549/1000\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 82.7988 - val_loss: 124.6293\n",
      "Epoch 550/1000\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 82.8843 - val_loss: 123.8022\n",
      "Epoch 551/1000\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 82.9564 - val_loss: 124.9125\n",
      "Epoch 552/1000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 83.0161 - val_loss: 123.5878\n",
      "Epoch 553/1000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 82.9704 - val_loss: 124.5913\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 554/1000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 82.8663 - val_loss: 123.5699\n",
      "Epoch 555/1000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 82.7171 - val_loss: 124.1816\n",
      "Epoch 556/1000\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 82.6174 - val_loss: 123.6677\n",
      "Epoch 557/1000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 82.5463 - val_loss: 123.7965\n",
      "Epoch 558/1000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 82.5046 - val_loss: 123.8871\n",
      "Epoch 559/1000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 82.4919 - val_loss: 123.7063\n",
      "Epoch 560/1000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 82.5172 - val_loss: 124.4777\n",
      "Epoch 561/1000\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 82.6093 - val_loss: 123.6804\n",
      "Epoch 562/1000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 82.7056 - val_loss: 124.7854\n",
      "Epoch 563/1000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 82.8125 - val_loss: 123.6002\n",
      "Epoch 564/1000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 82.8899 - val_loss: 125.0058\n",
      "Epoch 565/1000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 82.8902 - val_loss: 123.7010\n",
      "Epoch 566/1000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 82.7857 - val_loss: 124.6413\n",
      "Epoch 567/1000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 82.6152 - val_loss: 123.5453\n",
      "Epoch 568/1000\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 82.4540 - val_loss: 123.7982\n",
      "Epoch 569/1000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 82.3290 - val_loss: 123.6154\n",
      "Epoch 570/1000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 82.2919 - val_loss: 123.3791\n",
      "Epoch 571/1000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 82.3269 - val_loss: 124.1978\n",
      "Epoch 572/1000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 82.4494 - val_loss: 123.5190\n",
      "Epoch 573/1000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 82.5631 - val_loss: 124.9807\n",
      "Epoch 574/1000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 82.7312 - val_loss: 123.7125\n",
      "Epoch 575/1000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 82.6794 - val_loss: 124.8330\n",
      "Epoch 576/1000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 82.5726 - val_loss: 123.6047\n",
      "Epoch 577/1000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 82.3809 - val_loss: 124.2985\n",
      "Epoch 578/1000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 82.2488 - val_loss: 123.6749\n",
      "Epoch 579/1000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 82.1564 - val_loss: 123.7447\n",
      "Epoch 580/1000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 82.1113 - val_loss: 124.0607\n",
      "Epoch 581/1000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 82.1237 - val_loss: 123.6868\n",
      "Epoch 582/1000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 82.2125 - val_loss: 124.6553\n",
      "Epoch 583/1000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 82.3573 - val_loss: 123.5215\n",
      "Epoch 584/1000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 82.4398 - val_loss: 124.6456\n",
      "Epoch 585/1000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 82.4880 - val_loss: 123.4029\n",
      "Epoch 586/1000\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 82.4271 - val_loss: 124.5959\n",
      "Epoch 587/1000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 82.3217 - val_loss: 123.6265\n",
      "Epoch 588/1000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 82.1721 - val_loss: 124.2719\n",
      "Epoch 589/1000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 82.0530 - val_loss: 123.7449\n",
      "Epoch 590/1000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 81.9691 - val_loss: 123.9365\n",
      "Epoch 591/1000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 81.9180 - val_loss: 123.9293\n",
      "Epoch 592/1000\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 81.9015 - val_loss: 123.7901\n",
      "Epoch 593/1000\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 81.8963 - val_loss: 124.0782\n",
      "Epoch 594/1000\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 81.9041 - val_loss: 123.6163\n",
      "Epoch 595/1000\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 81.9239 - val_loss: 124.2998\n",
      "Epoch 596/1000\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 81.9750 - val_loss: 123.6566\n",
      "Epoch 597/1000\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 82.0623 - val_loss: 124.8581\n",
      "Epoch 598/1000\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 82.1619 - val_loss: 123.6974\n",
      "Epoch 599/1000\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 82.2315 - val_loss: 124.8663\n",
      "Epoch 600/1000\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 82.2072 - val_loss: 123.4862\n",
      "Epoch 601/1000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 82.1043 - val_loss: 124.4675\n",
      "Epoch 602/1000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 81.9496 - val_loss: 123.6610\n",
      "Epoch 603/1000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 81.8166 - val_loss: 123.9453\n",
      "Epoch 604/1000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 81.7266 - val_loss: 123.7198\n",
      "Epoch 605/1000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 81.6945 - val_loss: 123.5418\n",
      "Epoch 606/1000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 81.6977 - val_loss: 123.9726\n",
      "Epoch 607/1000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 81.7480 - val_loss: 123.4343\n",
      "Epoch 608/1000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 81.8392 - val_loss: 124.5662\n",
      "Epoch 609/1000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 81.9582 - val_loss: 123.5478\n",
      "Epoch 610/1000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 82.0714 - val_loss: 124.9513\n",
      "Epoch 611/1000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 82.0525 - val_loss: 123.5678\n",
      "Epoch 612/1000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 81.8988 - val_loss: 124.2996\n",
      "Epoch 613/1000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 81.7016 - val_loss: 123.5993\n",
      "Epoch 614/1000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 81.5838 - val_loss: 123.6134\n",
      "Epoch 615/1000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 81.5471 - val_loss: 123.9171\n",
      "Epoch 616/1000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 81.5842 - val_loss: 123.4714\n",
      "Epoch 617/1000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 81.7322 - val_loss: 124.8321\n",
      "Epoch 618/1000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 81.9499 - val_loss: 123.5841\n",
      "Epoch 619/1000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 82.1230 - val_loss: 125.1391\n",
      "Epoch 620/1000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 82.0885 - val_loss: 123.3981\n",
      "Epoch 621/1000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 81.8396 - val_loss: 124.1581\n",
      "Epoch 622/1000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 81.5479 - val_loss: 123.7134\n",
      "Epoch 623/1000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 81.4297 - val_loss: 123.5300\n",
      "Epoch 624/1000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 81.5068 - val_loss: 124.4677\n",
      "Epoch 625/1000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 81.6629 - val_loss: 123.6301\n",
      "Epoch 626/1000\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 81.8402 - val_loss: 125.0485\n",
      "Epoch 627/1000\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 81.8884 - val_loss: 123.4977\n",
      "Epoch 628/1000\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 81.7935 - val_loss: 124.5156\n",
      "Epoch 629/1000\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 81.6174 - val_loss: 123.4758\n",
      "Epoch 630/1000\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 81.4347 - val_loss: 123.8697\n",
      "Epoch 631/1000\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 81.3225 - val_loss: 123.8636\n",
      "Epoch 632/1000\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 81.3025 - val_loss: 123.4283\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 633/1000\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 81.3821 - val_loss: 124.3042\n",
      "Epoch 634/1000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 81.5066 - val_loss: 123.4294\n",
      "Epoch 635/1000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 81.6106 - val_loss: 124.6862\n",
      "Epoch 636/1000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 81.6092 - val_loss: 123.6321\n",
      "Epoch 637/1000\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 81.4805 - val_loss: 124.4667\n",
      "Epoch 638/1000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 81.3539 - val_loss: 123.6920\n",
      "Epoch 639/1000\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 81.2486 - val_loss: 123.8407\n",
      "Epoch 640/1000\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 81.1682 - val_loss: 123.8220\n",
      "Epoch 641/1000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 81.1702 - val_loss: 123.5196\n",
      "Epoch 642/1000\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 81.2066 - val_loss: 124.0641\n",
      "Epoch 643/1000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 81.2468 - val_loss: 123.2958\n",
      "Epoch 644/1000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 81.2980 - val_loss: 124.3166\n",
      "Epoch 645/1000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 81.3681 - val_loss: 123.4207\n",
      "Epoch 646/1000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 81.3699 - val_loss: 124.4626\n",
      "Epoch 647/1000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 81.3303 - val_loss: 123.4727\n",
      "Epoch 648/1000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 81.2242 - val_loss: 124.1320\n",
      "Epoch 649/1000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 81.1416 - val_loss: 123.4993\n",
      "Epoch 650/1000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 81.0933 - val_loss: 123.9199\n",
      "Epoch 651/1000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 81.0489 - val_loss: 123.6511\n",
      "Epoch 652/1000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 80.9898 - val_loss: 123.6810\n",
      "Epoch 653/1000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 80.9735 - val_loss: 123.7068\n",
      "Epoch 654/1000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 80.9612 - val_loss: 123.4351\n",
      "Epoch 655/1000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 80.9747 - val_loss: 124.0014\n",
      "Epoch 656/1000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 81.0140 - val_loss: 123.4211\n",
      "Epoch 657/1000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 81.1013 - val_loss: 124.4578\n",
      "Epoch 658/1000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 81.2142 - val_loss: 123.3831\n",
      "Epoch 659/1000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 81.2946 - val_loss: 124.5205\n",
      "Epoch 660/1000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 81.2928 - val_loss: 123.4024\n",
      "Epoch 661/1000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 81.2265 - val_loss: 124.3627\n",
      "Epoch 662/1000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 81.0599 - val_loss: 123.7366\n",
      "Epoch 663/1000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 80.8892 - val_loss: 123.9543\n",
      "Epoch 664/1000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 80.8232 - val_loss: 123.9654\n",
      "Epoch 665/1000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 80.8272 - val_loss: 123.3171\n",
      "Epoch 666/1000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 80.9142 - val_loss: 124.2237\n",
      "Epoch 667/1000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 81.0412 - val_loss: 123.2953\n",
      "Epoch 668/1000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 81.1733 - val_loss: 124.7886\n",
      "Epoch 669/1000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 81.2451 - val_loss: 123.4682\n",
      "Epoch 670/1000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 81.1639 - val_loss: 124.4152\n",
      "Epoch 671/1000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 81.0013 - val_loss: 123.3114\n",
      "Epoch 672/1000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 80.8249 - val_loss: 123.6636\n",
      "Epoch 673/1000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 80.6993 - val_loss: 123.7669\n",
      "Epoch 674/1000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 80.7050 - val_loss: 123.4012\n",
      "Epoch 675/1000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 80.8277 - val_loss: 124.6191\n",
      "Epoch 676/1000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 81.0337 - val_loss: 123.3322\n",
      "Epoch 677/1000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 81.2061 - val_loss: 124.8458\n",
      "Epoch 678/1000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 81.2149 - val_loss: 123.3165\n",
      "Epoch 679/1000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 81.0405 - val_loss: 124.3821\n",
      "Epoch 680/1000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 80.8109 - val_loss: 123.5296\n",
      "Epoch 681/1000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 80.6536 - val_loss: 123.6217\n",
      "Epoch 682/1000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 80.5814 - val_loss: 123.9468\n",
      "Epoch 683/1000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 80.6370 - val_loss: 123.4512\n",
      "Epoch 684/1000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 80.7773 - val_loss: 124.7455\n",
      "Epoch 685/1000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 80.9399 - val_loss: 123.4320\n",
      "Epoch 686/1000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 80.9604 - val_loss: 124.3847\n",
      "Epoch 687/1000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 80.8633 - val_loss: 123.2192\n",
      "Epoch 688/1000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 80.6673 - val_loss: 123.8034\n",
      "Epoch 689/1000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 80.5122 - val_loss: 123.6900\n",
      "Epoch 690/1000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 80.4556 - val_loss: 123.5808\n",
      "Epoch 691/1000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 80.4811 - val_loss: 124.1494\n",
      "Epoch 692/1000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 80.5548 - val_loss: 123.3749\n",
      "Epoch 693/1000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 80.6866 - val_loss: 124.5173\n",
      "Epoch 694/1000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 80.7303 - val_loss: 123.4716\n",
      "Epoch 695/1000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 80.7341 - val_loss: 124.5382\n",
      "Epoch 696/1000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 80.6289 - val_loss: 123.6156\n",
      "Epoch 697/1000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 80.4926 - val_loss: 123.8428\n",
      "Epoch 698/1000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 80.3668 - val_loss: 123.6535\n",
      "Epoch 699/1000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 80.3528 - val_loss: 123.1659\n",
      "Epoch 700/1000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 80.4545 - val_loss: 124.4153\n",
      "Epoch 701/1000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 80.6309 - val_loss: 123.3854\n",
      "Epoch 702/1000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 80.7251 - val_loss: 124.7158\n",
      "Epoch 703/1000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 80.6769 - val_loss: 123.3601\n",
      "Epoch 704/1000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 80.4407 - val_loss: 123.6875\n",
      "Epoch 705/1000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 80.2853 - val_loss: 123.5522\n",
      "Epoch 706/1000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 80.2547 - val_loss: 123.4568\n",
      "Epoch 707/1000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 80.2972 - val_loss: 124.3613\n",
      "Epoch 708/1000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 80.4145 - val_loss: 123.4682\n",
      "Epoch 709/1000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 80.5431 - val_loss: 124.7514\n",
      "Epoch 710/1000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 80.6290 - val_loss: 123.3521\n",
      "Epoch 711/1000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 80.5024 - val_loss: 124.2203\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 712/1000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 80.3393 - val_loss: 123.5105\n",
      "Epoch 713/1000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 80.2101 - val_loss: 123.6231\n",
      "Epoch 714/1000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 80.1437 - val_loss: 123.8223\n",
      "Epoch 715/1000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 80.1574 - val_loss: 123.3430\n",
      "Epoch 716/1000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 80.2532 - val_loss: 124.4353\n",
      "Epoch 717/1000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 80.3688 - val_loss: 123.3459\n",
      "Epoch 718/1000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 80.4417 - val_loss: 124.5249\n",
      "Epoch 719/1000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 80.4056 - val_loss: 123.3749\n",
      "Epoch 720/1000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 80.3021 - val_loss: 124.0762\n",
      "Epoch 721/1000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 80.1601 - val_loss: 123.3717\n",
      "Epoch 722/1000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 80.0765 - val_loss: 123.5692\n",
      "Epoch 723/1000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 80.0219 - val_loss: 123.6075\n",
      "Epoch 724/1000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 80.0107 - val_loss: 123.4918\n",
      "Epoch 725/1000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 80.0137 - val_loss: 123.9996\n",
      "Epoch 726/1000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 80.0448 - val_loss: 123.3569\n",
      "Epoch 727/1000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 80.1072 - val_loss: 124.2236\n",
      "Epoch 728/1000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 80.1644 - val_loss: 123.3737\n",
      "Epoch 729/1000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 80.2075 - val_loss: 124.4687\n",
      "Epoch 730/1000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 80.2037 - val_loss: 123.4116\n",
      "Epoch 731/1000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 80.1441 - val_loss: 124.1134\n",
      "Epoch 732/1000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 80.0405 - val_loss: 123.3356\n",
      "Epoch 733/1000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 79.9720 - val_loss: 123.7563\n",
      "Epoch 734/1000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 79.8965 - val_loss: 123.7791\n",
      "Epoch 735/1000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 79.8647 - val_loss: 123.5711\n",
      "Epoch 736/1000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 79.8752 - val_loss: 124.1345\n",
      "Epoch 737/1000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 79.9909 - val_loss: 123.2495\n",
      "Epoch 738/1000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 80.1337 - val_loss: 124.4804\n",
      "Epoch 739/1000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 80.2224 - val_loss: 123.3047\n",
      "Epoch 740/1000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 80.2130 - val_loss: 124.5053\n",
      "Epoch 741/1000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 80.1143 - val_loss: 123.3911\n",
      "Epoch 742/1000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 79.9842 - val_loss: 124.0046\n",
      "Epoch 743/1000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 79.8505 - val_loss: 123.3692\n",
      "Epoch 744/1000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 79.7719 - val_loss: 123.5698\n",
      "Epoch 745/1000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 79.7389 - val_loss: 123.7732\n",
      "Epoch 746/1000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 79.7337 - val_loss: 123.5887\n",
      "Epoch 747/1000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 79.7545 - val_loss: 124.1692\n",
      "Epoch 748/1000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 79.7984 - val_loss: 123.4368\n",
      "Epoch 749/1000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 79.8582 - val_loss: 124.3803\n",
      "Epoch 750/1000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 79.9418 - val_loss: 123.2672\n",
      "Epoch 751/1000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 79.9234 - val_loss: 124.1521\n",
      "Epoch 752/1000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 79.8343 - val_loss: 123.3988\n",
      "Epoch 753/1000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 79.7199 - val_loss: 123.7540\n",
      "Epoch 754/1000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 79.6443 - val_loss: 123.5452\n",
      "Epoch 755/1000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 79.6134 - val_loss: 123.3585\n",
      "Epoch 756/1000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 79.6182 - val_loss: 123.8185\n",
      "Epoch 757/1000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 79.6469 - val_loss: 123.2482\n",
      "Epoch 758/1000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 79.7243 - val_loss: 124.3089\n",
      "Epoch 759/1000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 79.8284 - val_loss: 123.2762\n",
      "Epoch 760/1000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 79.8602 - val_loss: 124.3079\n",
      "Epoch 761/1000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 79.8218 - val_loss: 123.2361\n",
      "Epoch 762/1000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 79.7758 - val_loss: 124.1478\n",
      "Epoch 763/1000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 79.6787 - val_loss: 123.3718\n",
      "Epoch 764/1000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 79.5984 - val_loss: 123.8381\n",
      "Epoch 765/1000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 79.5062 - val_loss: 123.6170\n",
      "Epoch 766/1000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 79.4571 - val_loss: 123.5291\n",
      "Epoch 767/1000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 79.4395 - val_loss: 123.6448\n",
      "Epoch 768/1000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 79.4457 - val_loss: 123.2436\n",
      "Epoch 769/1000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 79.5137 - val_loss: 124.2601\n",
      "Epoch 770/1000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 79.6276 - val_loss: 123.4358\n",
      "Epoch 771/1000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 79.7684 - val_loss: 124.7234\n",
      "Epoch 772/1000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 79.8025 - val_loss: 123.4372\n",
      "Epoch 773/1000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 79.7320 - val_loss: 124.2934\n",
      "Epoch 774/1000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 79.5482 - val_loss: 123.4391\n",
      "Epoch 775/1000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 79.4103 - val_loss: 123.6417\n",
      "Epoch 776/1000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 79.3299 - val_loss: 123.5229\n",
      "Epoch 777/1000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 79.3131 - val_loss: 123.3488\n",
      "Epoch 778/1000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 79.3251 - val_loss: 123.7929\n",
      "Epoch 779/1000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 79.3478 - val_loss: 123.2096\n",
      "Epoch 780/1000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 79.3967 - val_loss: 124.0757\n",
      "Epoch 781/1000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 79.4512 - val_loss: 123.2879\n",
      "Epoch 782/1000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 79.4922 - val_loss: 124.3381\n",
      "Epoch 783/1000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 79.4726 - val_loss: 123.4190\n",
      "Epoch 784/1000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 79.4775 - val_loss: 124.3151\n",
      "Epoch 785/1000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 79.4288 - val_loss: 123.3355\n",
      "Epoch 786/1000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 79.4202 - val_loss: 124.1440\n",
      "Epoch 787/1000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 79.3360 - val_loss: 123.3629\n",
      "Epoch 788/1000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 79.2506 - val_loss: 123.7432\n",
      "Epoch 789/1000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 79.1856 - val_loss: 123.4414\n",
      "Epoch 790/1000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 79.1424 - val_loss: 123.5881\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 791/1000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 79.1213 - val_loss: 123.6256\n",
      "Epoch 792/1000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 79.1116 - val_loss: 123.4627\n",
      "Epoch 793/1000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 79.0997 - val_loss: 123.7349\n",
      "Epoch 794/1000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 79.1064 - val_loss: 123.3196\n",
      "Epoch 795/1000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 79.1474 - val_loss: 124.0535\n",
      "Epoch 796/1000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 79.2129 - val_loss: 123.3570\n",
      "Epoch 797/1000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 79.2894 - val_loss: 124.4642\n",
      "Epoch 798/1000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 79.3525 - val_loss: 123.5404\n",
      "Epoch 799/1000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 79.3876 - val_loss: 124.4709\n",
      "Epoch 800/1000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 79.2897 - val_loss: 123.4538\n",
      "Epoch 801/1000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 79.1653 - val_loss: 124.0852\n",
      "Epoch 802/1000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 79.0696 - val_loss: 123.5592\n",
      "Epoch 803/1000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 79.0078 - val_loss: 123.8336\n",
      "Epoch 804/1000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 78.9593 - val_loss: 123.5760\n",
      "Epoch 805/1000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 78.9484 - val_loss: 123.7168\n",
      "Epoch 806/1000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 78.9408 - val_loss: 123.3651\n",
      "Epoch 807/1000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 78.9635 - val_loss: 123.9749\n",
      "Epoch 808/1000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 79.0531 - val_loss: 123.2956\n",
      "Epoch 809/1000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 79.1630 - val_loss: 124.6995\n",
      "Epoch 810/1000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 79.3424 - val_loss: 123.4085\n",
      "Epoch 811/1000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 79.4154 - val_loss: 124.8146\n",
      "Epoch 812/1000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 79.3980 - val_loss: 123.4988\n",
      "Epoch 813/1000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 79.2730 - val_loss: 124.3456\n",
      "Epoch 814/1000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 79.0521 - val_loss: 123.5940\n",
      "Epoch 815/1000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 78.8744 - val_loss: 123.6946\n",
      "Epoch 816/1000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 78.8131 - val_loss: 124.0315\n",
      "Epoch 817/1000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 78.8705 - val_loss: 123.3957\n",
      "Epoch 818/1000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 79.0437 - val_loss: 124.6787\n",
      "Epoch 819/1000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 79.1950 - val_loss: 123.3769\n",
      "Epoch 820/1000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 79.2171 - val_loss: 124.4704\n",
      "Epoch 821/1000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 79.1072 - val_loss: 123.3602\n",
      "Epoch 822/1000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 78.9643 - val_loss: 123.9406\n",
      "Epoch 823/1000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 78.8256 - val_loss: 123.5393\n",
      "Epoch 824/1000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 78.7459 - val_loss: 123.7934\n",
      "Epoch 825/1000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 78.7070 - val_loss: 124.0391\n",
      "Epoch 826/1000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 78.7237 - val_loss: 123.4360\n",
      "Epoch 827/1000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 78.8136 - val_loss: 124.3443\n",
      "Epoch 828/1000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 78.9236 - val_loss: 123.4198\n",
      "Epoch 829/1000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 79.0607 - val_loss: 124.9312\n",
      "Epoch 830/1000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 79.0697 - val_loss: 123.6975\n",
      "Epoch 831/1000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 78.9754 - val_loss: 124.5012\n",
      "Epoch 832/1000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 78.7751 - val_loss: 123.6366\n",
      "Epoch 833/1000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 78.6475 - val_loss: 123.6189\n",
      "Epoch 834/1000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 78.6073 - val_loss: 123.9255\n",
      "Epoch 835/1000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 78.6403 - val_loss: 123.5403\n",
      "Epoch 836/1000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 78.7136 - val_loss: 124.4900\n",
      "Epoch 837/1000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 78.8227 - val_loss: 123.5126\n",
      "Epoch 838/1000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 78.9139 - val_loss: 124.6722\n",
      "Epoch 839/1000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 78.9131 - val_loss: 123.4562\n",
      "Epoch 840/1000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 78.8490 - val_loss: 124.3378\n",
      "Epoch 841/1000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 78.7159 - val_loss: 123.5042\n",
      "Epoch 842/1000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 78.6283 - val_loss: 123.9369\n",
      "Epoch 843/1000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 78.5342 - val_loss: 123.7095\n",
      "Epoch 844/1000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 78.4752 - val_loss: 123.5729\n",
      "Epoch 845/1000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 78.4782 - val_loss: 123.9817\n",
      "Epoch 846/1000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 78.5518 - val_loss: 123.2734\n",
      "Epoch 847/1000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 78.6095 - val_loss: 124.1628\n",
      "Epoch 848/1000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 78.6593 - val_loss: 123.2446\n",
      "Epoch 849/1000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 78.6940 - val_loss: 124.3700\n",
      "Epoch 850/1000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 78.6800 - val_loss: 123.4723\n",
      "Epoch 851/1000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 78.6240 - val_loss: 124.2449\n",
      "Epoch 852/1000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 78.5272 - val_loss: 123.6110\n",
      "Epoch 853/1000\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 78.4495 - val_loss: 123.9343\n",
      "Epoch 854/1000\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 78.3801 - val_loss: 123.6938\n",
      "Epoch 855/1000\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 78.3392 - val_loss: 123.6947\n",
      "Epoch 856/1000\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 78.3230 - val_loss: 123.9564\n",
      "Epoch 857/1000\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 78.3675 - val_loss: 123.4338\n",
      "Epoch 858/1000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 78.4537 - val_loss: 124.3425\n",
      "Epoch 859/1000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 78.5831 - val_loss: 123.4404\n",
      "Epoch 860/1000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 78.7253 - val_loss: 124.9746\n",
      "Epoch 861/1000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 78.8215 - val_loss: 123.6637\n",
      "Epoch 862/1000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 78.7583 - val_loss: 124.6575\n",
      "Epoch 863/1000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 78.6297 - val_loss: 123.4537\n",
      "Epoch 864/1000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 78.4844 - val_loss: 123.9464\n",
      "Epoch 865/1000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 78.3296 - val_loss: 123.6072\n",
      "Epoch 866/1000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 78.2330 - val_loss: 123.8393\n",
      "Epoch 867/1000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 78.2078 - val_loss: 124.1247\n",
      "Epoch 868/1000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 78.2320 - val_loss: 123.4843\n",
      "Epoch 869/1000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 78.3235 - val_loss: 124.4455\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 870/1000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 78.5073 - val_loss: 123.4225\n",
      "Epoch 871/1000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 78.6273 - val_loss: 125.0065\n",
      "Epoch 872/1000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 78.6929 - val_loss: 123.7937\n",
      "Epoch 873/1000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 78.6298 - val_loss: 124.7377\n",
      "Epoch 874/1000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 78.4745 - val_loss: 123.5082\n",
      "Epoch 875/1000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 78.2829 - val_loss: 123.7865\n",
      "Epoch 876/1000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 78.1382 - val_loss: 123.7625\n",
      "Epoch 877/1000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 78.0848 - val_loss: 123.6943\n",
      "Epoch 878/1000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 78.1442 - val_loss: 124.5839\n",
      "Epoch 879/1000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 78.2940 - val_loss: 123.5107\n",
      "Epoch 880/1000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 78.4238 - val_loss: 124.6476\n",
      "Epoch 881/1000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 78.5014 - val_loss: 123.3577\n",
      "Epoch 882/1000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 78.4810 - val_loss: 124.5580\n",
      "Epoch 883/1000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 78.3845 - val_loss: 123.6363\n",
      "Epoch 884/1000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 78.2008 - val_loss: 124.1740\n",
      "Epoch 885/1000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 78.0626 - val_loss: 123.6762\n",
      "Epoch 886/1000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 77.9839 - val_loss: 123.4808\n",
      "Epoch 887/1000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 77.9763 - val_loss: 123.9402\n",
      "Epoch 888/1000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 78.0528 - val_loss: 123.4711\n",
      "Epoch 889/1000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 78.2230 - val_loss: 124.8116\n",
      "Epoch 890/1000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 78.3901 - val_loss: 123.6395\n",
      "Epoch 891/1000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 78.4252 - val_loss: 124.8233\n",
      "Epoch 892/1000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 78.3401 - val_loss: 123.5471\n",
      "Epoch 893/1000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 78.1381 - val_loss: 124.0348\n",
      "Epoch 894/1000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 77.9520 - val_loss: 123.7351\n",
      "Epoch 895/1000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 77.8753 - val_loss: 123.4974\n",
      "Epoch 896/1000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 77.9492 - val_loss: 124.4385\n",
      "Epoch 897/1000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 78.1657 - val_loss: 123.4476\n",
      "Epoch 898/1000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 78.2887 - val_loss: 124.8922\n",
      "Epoch 899/1000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 78.3929 - val_loss: 123.5450\n",
      "Epoch 900/1000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 78.2337 - val_loss: 124.4880\n",
      "Epoch 901/1000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 78.0640 - val_loss: 123.5791\n",
      "Epoch 902/1000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 77.9078 - val_loss: 123.8167\n",
      "Epoch 903/1000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 77.8029 - val_loss: 123.6822\n",
      "Epoch 904/1000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 77.7778 - val_loss: 123.5168\n",
      "Epoch 905/1000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 77.7889 - val_loss: 124.0625\n",
      "Epoch 906/1000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 77.8449 - val_loss: 123.5583\n",
      "Epoch 907/1000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 77.9028 - val_loss: 124.3869\n",
      "Epoch 908/1000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 77.9442 - val_loss: 123.5231\n",
      "Epoch 909/1000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 77.9316 - val_loss: 124.2947\n",
      "Epoch 910/1000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 77.9197 - val_loss: 123.4944\n",
      "Epoch 911/1000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 77.8635 - val_loss: 124.1646\n",
      "Epoch 912/1000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 77.7937 - val_loss: 123.7027\n",
      "Epoch 913/1000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 77.7311 - val_loss: 124.0443\n",
      "Epoch 914/1000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 77.6891 - val_loss: 123.7594\n",
      "Epoch 915/1000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 77.6681 - val_loss: 123.8535\n",
      "Epoch 916/1000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 77.6348 - val_loss: 123.7790\n",
      "Epoch 917/1000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 77.6255 - val_loss: 123.8451\n",
      "Epoch 918/1000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 77.6063 - val_loss: 123.7578\n",
      "Epoch 919/1000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 77.6011 - val_loss: 123.8418\n",
      "Epoch 920/1000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 77.5880 - val_loss: 123.6338\n",
      "Epoch 921/1000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 77.6045 - val_loss: 124.1779\n",
      "Epoch 922/1000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 77.6632 - val_loss: 123.6310\n",
      "Epoch 923/1000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 77.7553 - val_loss: 124.7230\n",
      "Epoch 924/1000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 77.8589 - val_loss: 123.6750\n",
      "Epoch 925/1000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 77.9584 - val_loss: 124.9248\n",
      "Epoch 926/1000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 78.0280 - val_loss: 123.6072\n",
      "Epoch 927/1000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 77.9864 - val_loss: 124.6652\n",
      "Epoch 928/1000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 77.8446 - val_loss: 123.6082\n",
      "Epoch 929/1000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 77.7114 - val_loss: 124.0993\n",
      "Epoch 930/1000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 77.5519 - val_loss: 123.7603\n",
      "Epoch 931/1000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 77.4745 - val_loss: 123.6587\n",
      "Epoch 932/1000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 77.4970 - val_loss: 124.2705\n",
      "Epoch 933/1000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 77.5874 - val_loss: 123.5738\n",
      "Epoch 934/1000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 77.7149 - val_loss: 124.6455\n",
      "Epoch 935/1000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 77.7887 - val_loss: 123.5022\n",
      "Epoch 936/1000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 77.7973 - val_loss: 124.5653\n",
      "Epoch 937/1000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 77.7307 - val_loss: 123.5561\n",
      "Epoch 938/1000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 77.6623 - val_loss: 124.2754\n",
      "Epoch 939/1000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 77.5262 - val_loss: 123.7122\n",
      "Epoch 940/1000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 77.4309 - val_loss: 124.0264\n",
      "Epoch 941/1000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 77.3790 - val_loss: 123.8515\n",
      "Epoch 942/1000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 77.3636 - val_loss: 123.6647\n",
      "Epoch 943/1000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 77.3526 - val_loss: 123.7949\n",
      "Epoch 944/1000\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 77.3554 - val_loss: 123.4696\n",
      "Epoch 945/1000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 77.3994 - val_loss: 124.2100\n",
      "Epoch 946/1000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 77.4496 - val_loss: 123.6562\n",
      "Epoch 947/1000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 77.5423 - val_loss: 124.6821\n",
      "Epoch 948/1000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 77.6075 - val_loss: 123.7034\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 949/1000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 77.6059 - val_loss: 124.5095\n",
      "Epoch 950/1000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 77.5648 - val_loss: 123.5574\n",
      "Epoch 951/1000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 77.4261 - val_loss: 123.9656\n",
      "Epoch 952/1000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 77.3150 - val_loss: 123.5983\n",
      "Epoch 953/1000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 77.2483 - val_loss: 123.6353\n",
      "Epoch 954/1000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 77.2299 - val_loss: 124.0299\n",
      "Epoch 955/1000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 77.2702 - val_loss: 123.6166\n",
      "Epoch 956/1000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 77.3889 - val_loss: 124.7513\n",
      "Epoch 957/1000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 77.5544 - val_loss: 123.7260\n",
      "Epoch 958/1000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 77.7273 - val_loss: 124.9539\n",
      "Epoch 959/1000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 77.7662 - val_loss: 123.5887\n",
      "Epoch 960/1000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 77.6859 - val_loss: 124.5139\n",
      "Epoch 961/1000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 77.4807 - val_loss: 123.6507\n",
      "Epoch 962/1000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 77.2603 - val_loss: 123.9580\n",
      "Epoch 963/1000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 77.1354 - val_loss: 124.0481\n",
      "Epoch 964/1000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 77.1395 - val_loss: 123.6222\n",
      "Epoch 965/1000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 77.2468 - val_loss: 124.5422\n",
      "Epoch 966/1000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 77.3903 - val_loss: 123.5774\n",
      "Epoch 967/1000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 77.5159 - val_loss: 124.9671\n",
      "Epoch 968/1000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 77.5744 - val_loss: 123.8027\n",
      "Epoch 969/1000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 77.4990 - val_loss: 124.6811\n",
      "Epoch 970/1000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 77.3344 - val_loss: 123.6296\n",
      "Epoch 971/1000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 77.1541 - val_loss: 123.7794\n",
      "Epoch 972/1000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 77.0438 - val_loss: 123.8394\n",
      "Epoch 973/1000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 77.0295 - val_loss: 123.6183\n",
      "Epoch 974/1000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 77.0823 - val_loss: 124.3925\n",
      "Epoch 975/1000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 77.1919 - val_loss: 123.7019\n",
      "Epoch 976/1000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 77.3183 - val_loss: 124.7167\n",
      "Epoch 977/1000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 77.3801 - val_loss: 123.6393\n",
      "Epoch 978/1000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 77.3135 - val_loss: 124.3149\n",
      "Epoch 979/1000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 77.1620 - val_loss: 123.6376\n",
      "Epoch 980/1000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 77.0143 - val_loss: 123.8902\n",
      "Epoch 981/1000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 76.9475 - val_loss: 124.0525\n",
      "Epoch 982/1000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 76.9474 - val_loss: 123.7083\n",
      "Epoch 983/1000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 77.0109 - val_loss: 124.4821\n",
      "Epoch 984/1000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 77.1579 - val_loss: 123.5478\n",
      "Epoch 985/1000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 77.2950 - val_loss: 124.7794\n",
      "Epoch 986/1000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 77.3693 - val_loss: 123.6132\n",
      "Epoch 987/1000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 77.2794 - val_loss: 124.6359\n",
      "Epoch 988/1000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 77.1304 - val_loss: 123.7927\n",
      "Epoch 989/1000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 76.9623 - val_loss: 123.9500\n",
      "Epoch 990/1000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 76.8691 - val_loss: 123.8556\n",
      "Epoch 991/1000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 76.8536 - val_loss: 123.5542\n",
      "Epoch 992/1000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 76.9084 - val_loss: 124.3188\n",
      "Epoch 993/1000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 77.0177 - val_loss: 123.6736\n",
      "Epoch 994/1000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 77.1159 - val_loss: 124.8748\n",
      "Epoch 995/1000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 77.1815 - val_loss: 123.8112\n",
      "Epoch 996/1000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 77.1266 - val_loss: 124.6346\n",
      "Epoch 997/1000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 77.0179 - val_loss: 123.6929\n",
      "Epoch 998/1000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 76.9061 - val_loss: 124.0333\n",
      "Epoch 999/1000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 76.8042 - val_loss: 123.7511\n",
      "Epoch 1000/1000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 76.7474 - val_loss: 123.6613\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(np.asarray(X_train).astype('float32'), np.asarray(y_train).astype('float32'), batch_size = 1024, epochs = 1000, \n",
    "                    validation_split = 0.2, shuffle = True, callbacks = None,\n",
    "                    use_multiprocessing=True, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learningCurve(history):\n",
    "    plt.figure(figsize=(10,8))\n",
    "    plt.plot(history.history['loss'], linewidth=1)\n",
    "    plt.plot(history.history['val_loss'], linewidth=1)\n",
    "    plt.title('Model Loss over Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['training sample loss','validation sample loss'])\n",
    "    plt.savefig('Learning_curve')\n",
    "    plt.show\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "learningCurve(history)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['tc'] not found in axis\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[185], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m save \u001b[38;5;241m=\u001b[39m \u001b[43mfinDat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtc\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/tensorflow/lib/python3.10/site-packages/pandas/util/_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[1;32m    326\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    327\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[1;32m    329\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[1;32m    330\u001b[0m     )\n\u001b[0;32m--> 331\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/tensorflow/lib/python3.10/site-packages/pandas/core/frame.py:5399\u001b[0m, in \u001b[0;36mDataFrame.drop\u001b[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[1;32m   5251\u001b[0m \u001b[38;5;129m@deprecate_nonkeyword_arguments\u001b[39m(version\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, allowed_args\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mself\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m   5252\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdrop\u001b[39m(  \u001b[38;5;66;03m# type: ignore[override]\u001b[39;00m\n\u001b[1;32m   5253\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   5260\u001b[0m     errors: IgnoreRaise \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraise\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   5261\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   5262\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   5263\u001b[0m \u001b[38;5;124;03m    Drop specified labels from rows or columns.\u001b[39;00m\n\u001b[1;32m   5264\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   5397\u001b[0m \u001b[38;5;124;03m            weight  1.0     0.8\u001b[39;00m\n\u001b[1;32m   5398\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 5399\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   5400\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5401\u001b[0m \u001b[43m        \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5402\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5403\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5404\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5405\u001b[0m \u001b[43m        \u001b[49m\u001b[43minplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5406\u001b[0m \u001b[43m        \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5407\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/tensorflow/lib/python3.10/site-packages/pandas/util/_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[1;32m    326\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    327\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[1;32m    329\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[1;32m    330\u001b[0m     )\n\u001b[0;32m--> 331\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/tensorflow/lib/python3.10/site-packages/pandas/core/generic.py:4505\u001b[0m, in \u001b[0;36mNDFrame.drop\u001b[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[1;32m   4503\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m axis, labels \u001b[38;5;129;01min\u001b[39;00m axes\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m   4504\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 4505\u001b[0m         obj \u001b[38;5;241m=\u001b[39m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_drop_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4507\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inplace:\n\u001b[1;32m   4508\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_inplace(obj)\n",
      "File \u001b[0;32m~/miniconda3/envs/tensorflow/lib/python3.10/site-packages/pandas/core/generic.py:4546\u001b[0m, in \u001b[0;36mNDFrame._drop_axis\u001b[0;34m(self, labels, axis, level, errors, only_slice)\u001b[0m\n\u001b[1;32m   4544\u001b[0m         new_axis \u001b[38;5;241m=\u001b[39m axis\u001b[38;5;241m.\u001b[39mdrop(labels, level\u001b[38;5;241m=\u001b[39mlevel, errors\u001b[38;5;241m=\u001b[39merrors)\n\u001b[1;32m   4545\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 4546\u001b[0m         new_axis \u001b[38;5;241m=\u001b[39m \u001b[43maxis\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4547\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m axis\u001b[38;5;241m.\u001b[39mget_indexer(new_axis)\n\u001b[1;32m   4549\u001b[0m \u001b[38;5;66;03m# Case for non-unique axis\u001b[39;00m\n\u001b[1;32m   4550\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/tensorflow/lib/python3.10/site-packages/pandas/core/indexes/base.py:6934\u001b[0m, in \u001b[0;36mIndex.drop\u001b[0;34m(self, labels, errors)\u001b[0m\n\u001b[1;32m   6932\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mask\u001b[38;5;241m.\u001b[39many():\n\u001b[1;32m   6933\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m errors \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m-> 6934\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(labels[mask])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not found in axis\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   6935\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m indexer[\u001b[38;5;241m~\u001b[39mmask]\n\u001b[1;32m   6936\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdelete(indexer)\n",
      "\u001b[0;31mKeyError\u001b[0m: \"['tc'] not found in axis\""
     ]
    }
   ],
   "source": [
    "save = finDat.drop('tc', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>formula_similarity</th>\n",
       "      <th>totreldiff</th>\n",
       "      <th>formula_frac</th>\n",
       "      <th>num_elements_sc</th>\n",
       "      <th>lata_2</th>\n",
       "      <th>latb_2</th>\n",
       "      <th>latc_2</th>\n",
       "      <th>band_gap_2</th>\n",
       "      <th>density_2</th>\n",
       "      <th>e_above_hull_2</th>\n",
       "      <th>...</th>\n",
       "      <th>Pu</th>\n",
       "      <th>Am</th>\n",
       "      <th>Cm</th>\n",
       "      <th>Bk</th>\n",
       "      <th>Cf</th>\n",
       "      <th>Es</th>\n",
       "      <th>Fm</th>\n",
       "      <th>Md</th>\n",
       "      <th>No</th>\n",
       "      <th>Lr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>0.006667</td>\n",
       "      <td>1.00</td>\n",
       "      <td>3</td>\n",
       "      <td>3.073680</td>\n",
       "      <td>3.073680</td>\n",
       "      <td>3.534350</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.637301</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.014667</td>\n",
       "      <td>1.00</td>\n",
       "      <td>3</td>\n",
       "      <td>3.073680</td>\n",
       "      <td>3.073680</td>\n",
       "      <td>3.534350</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.637301</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.013333</td>\n",
       "      <td>1.00</td>\n",
       "      <td>3</td>\n",
       "      <td>3.073680</td>\n",
       "      <td>3.073680</td>\n",
       "      <td>3.534350</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.637301</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>2.00</td>\n",
       "      <td>3</td>\n",
       "      <td>5.334611</td>\n",
       "      <td>5.334611</td>\n",
       "      <td>5.334611</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.694218</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>2.00</td>\n",
       "      <td>3</td>\n",
       "      <td>4.702381</td>\n",
       "      <td>4.702381</td>\n",
       "      <td>4.702381</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.778142</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1402</th>\n",
       "      <td>3</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>2.00</td>\n",
       "      <td>3</td>\n",
       "      <td>4.702381</td>\n",
       "      <td>4.702381</td>\n",
       "      <td>4.702381</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.778142</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1403</th>\n",
       "      <td>3</td>\n",
       "      <td>0.075000</td>\n",
       "      <td>2.00</td>\n",
       "      <td>3</td>\n",
       "      <td>4.702381</td>\n",
       "      <td>4.702381</td>\n",
       "      <td>4.702381</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.778142</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1404</th>\n",
       "      <td>3</td>\n",
       "      <td>0.150000</td>\n",
       "      <td>2.00</td>\n",
       "      <td>3</td>\n",
       "      <td>4.702381</td>\n",
       "      <td>4.702381</td>\n",
       "      <td>4.702381</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.778142</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1405</th>\n",
       "      <td>3</td>\n",
       "      <td>0.150000</td>\n",
       "      <td>2.00</td>\n",
       "      <td>3</td>\n",
       "      <td>4.702381</td>\n",
       "      <td>4.702381</td>\n",
       "      <td>4.702381</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.778142</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1406</th>\n",
       "      <td>2</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.08</td>\n",
       "      <td>2</td>\n",
       "      <td>4.702381</td>\n",
       "      <td>4.702381</td>\n",
       "      <td>4.702381</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.778142</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1407 rows × 143 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      formula_similarity  totreldiff  formula_frac  num_elements_sc    lata_2  \\\n",
       "0                      2    0.006667          1.00                3  3.073680   \n",
       "1                      2    0.014667          1.00                3  3.073680   \n",
       "2                      2    0.013333          1.00                3  3.073680   \n",
       "3                      2    0.010000          2.00                3  5.334611   \n",
       "4                      2    0.010000          2.00                3  4.702381   \n",
       "...                  ...         ...           ...              ...       ...   \n",
       "1402                   3    0.100000          2.00                3  4.702381   \n",
       "1403                   3    0.075000          2.00                3  4.702381   \n",
       "1404                   3    0.150000          2.00                3  4.702381   \n",
       "1405                   3    0.150000          2.00                3  4.702381   \n",
       "1406                   2    0.038462          0.08                2  4.702381   \n",
       "\n",
       "        latb_2    latc_2  band_gap_2  density_2  e_above_hull_2  ...   Pu  \\\n",
       "0     3.073680  3.534350         0.0   2.637301             0.0  ...  0.0   \n",
       "1     3.073680  3.534350         0.0   2.637301             0.0  ...  0.0   \n",
       "2     3.073680  3.534350         0.0   2.637301             0.0  ...  0.0   \n",
       "3     5.334611  5.334611         0.0   8.694218             0.0  ...  0.0   \n",
       "4     4.702381  4.702381         0.0   5.778142             0.0  ...  0.0   \n",
       "...        ...       ...         ...        ...             ...  ...  ...   \n",
       "1402  4.702381  4.702381         0.0   5.778142             0.0  ...  0.0   \n",
       "1403  4.702381  4.702381         0.0   5.778142             0.0  ...  0.0   \n",
       "1404  4.702381  4.702381         0.0   5.778142             0.0  ...  0.0   \n",
       "1405  4.702381  4.702381         0.0   5.778142             0.0  ...  0.0   \n",
       "1406  4.702381  4.702381         0.0   5.778142             0.0  ...  0.0   \n",
       "\n",
       "       Am   Cm   Bk   Cf   Es   Fm   Md   No   Lr  \n",
       "0     0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "1     0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "2     0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "3     0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "4     0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "...   ...  ...  ...  ...  ...  ...  ...  ...  ...  \n",
       "1402  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "1403  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "1404  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "1405  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "1406  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "\n",
       "[1407 rows x 143 columns]"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finDat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('finDat',np.asarray(finDat.astype('float32')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('tc', tc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights('modelF.h5')\n",
    "model_json = model.to_json()\n",
    "with open(\"modelF.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('columns', finDat.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>formula_similarity</th>\n",
       "      <th>totreldiff</th>\n",
       "      <th>formula_frac</th>\n",
       "      <th>num_elements_sc</th>\n",
       "      <th>lata_2</th>\n",
       "      <th>latb_2</th>\n",
       "      <th>latc_2</th>\n",
       "      <th>band_gap_2</th>\n",
       "      <th>density_2</th>\n",
       "      <th>e_above_hull_2</th>\n",
       "      <th>...</th>\n",
       "      <th>Pu</th>\n",
       "      <th>Am</th>\n",
       "      <th>Cm</th>\n",
       "      <th>Bk</th>\n",
       "      <th>Cf</th>\n",
       "      <th>Es</th>\n",
       "      <th>Fm</th>\n",
       "      <th>Md</th>\n",
       "      <th>No</th>\n",
       "      <th>Lr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>0.006667</td>\n",
       "      <td>1.00</td>\n",
       "      <td>3</td>\n",
       "      <td>3.073680</td>\n",
       "      <td>3.073680</td>\n",
       "      <td>3.534350</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.637301</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.014667</td>\n",
       "      <td>1.00</td>\n",
       "      <td>3</td>\n",
       "      <td>3.073680</td>\n",
       "      <td>3.073680</td>\n",
       "      <td>3.534350</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.637301</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.013333</td>\n",
       "      <td>1.00</td>\n",
       "      <td>3</td>\n",
       "      <td>3.073680</td>\n",
       "      <td>3.073680</td>\n",
       "      <td>3.534350</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.637301</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>2.00</td>\n",
       "      <td>3</td>\n",
       "      <td>5.334611</td>\n",
       "      <td>5.334611</td>\n",
       "      <td>5.334611</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.694218</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>2.00</td>\n",
       "      <td>3</td>\n",
       "      <td>4.702381</td>\n",
       "      <td>4.702381</td>\n",
       "      <td>4.702381</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.778142</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1402</th>\n",
       "      <td>3</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>2.00</td>\n",
       "      <td>3</td>\n",
       "      <td>4.702381</td>\n",
       "      <td>4.702381</td>\n",
       "      <td>4.702381</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.778142</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1403</th>\n",
       "      <td>3</td>\n",
       "      <td>0.075000</td>\n",
       "      <td>2.00</td>\n",
       "      <td>3</td>\n",
       "      <td>4.702381</td>\n",
       "      <td>4.702381</td>\n",
       "      <td>4.702381</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.778142</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1404</th>\n",
       "      <td>3</td>\n",
       "      <td>0.150000</td>\n",
       "      <td>2.00</td>\n",
       "      <td>3</td>\n",
       "      <td>4.702381</td>\n",
       "      <td>4.702381</td>\n",
       "      <td>4.702381</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.778142</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1405</th>\n",
       "      <td>3</td>\n",
       "      <td>0.150000</td>\n",
       "      <td>2.00</td>\n",
       "      <td>3</td>\n",
       "      <td>4.702381</td>\n",
       "      <td>4.702381</td>\n",
       "      <td>4.702381</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.778142</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1406</th>\n",
       "      <td>2</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.08</td>\n",
       "      <td>2</td>\n",
       "      <td>4.702381</td>\n",
       "      <td>4.702381</td>\n",
       "      <td>4.702381</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.778142</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1407 rows × 143 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      formula_similarity  totreldiff  formula_frac  num_elements_sc    lata_2  \\\n",
       "0                      2    0.006667          1.00                3  3.073680   \n",
       "1                      2    0.014667          1.00                3  3.073680   \n",
       "2                      2    0.013333          1.00                3  3.073680   \n",
       "3                      2    0.010000          2.00                3  5.334611   \n",
       "4                      2    0.010000          2.00                3  4.702381   \n",
       "...                  ...         ...           ...              ...       ...   \n",
       "1402                   3    0.100000          2.00                3  4.702381   \n",
       "1403                   3    0.075000          2.00                3  4.702381   \n",
       "1404                   3    0.150000          2.00                3  4.702381   \n",
       "1405                   3    0.150000          2.00                3  4.702381   \n",
       "1406                   2    0.038462          0.08                2  4.702381   \n",
       "\n",
       "        latb_2    latc_2  band_gap_2  density_2  e_above_hull_2  ...   Pu  \\\n",
       "0     3.073680  3.534350         0.0   2.637301             0.0  ...  0.0   \n",
       "1     3.073680  3.534350         0.0   2.637301             0.0  ...  0.0   \n",
       "2     3.073680  3.534350         0.0   2.637301             0.0  ...  0.0   \n",
       "3     5.334611  5.334611         0.0   8.694218             0.0  ...  0.0   \n",
       "4     4.702381  4.702381         0.0   5.778142             0.0  ...  0.0   \n",
       "...        ...       ...         ...        ...             ...  ...  ...   \n",
       "1402  4.702381  4.702381         0.0   5.778142             0.0  ...  0.0   \n",
       "1403  4.702381  4.702381         0.0   5.778142             0.0  ...  0.0   \n",
       "1404  4.702381  4.702381         0.0   5.778142             0.0  ...  0.0   \n",
       "1405  4.702381  4.702381         0.0   5.778142             0.0  ...  0.0   \n",
       "1406  4.702381  4.702381         0.0   5.778142             0.0  ...  0.0   \n",
       "\n",
       "       Am   Cm   Bk   Cf   Es   Fm   Md   No   Lr  \n",
       "0     0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "1     0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "2     0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "3     0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "4     0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "...   ...  ...  ...  ...  ...  ...  ...  ...  ...  \n",
       "1402  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "1403  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "1404  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "1405  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "1406  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "\n",
       "[1407 rows x 143 columns]"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finDat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "df5fa5efdf1c60a896ccc8bc52bcd2fc69320846d08f7bc80e2522b0c75b6345"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
